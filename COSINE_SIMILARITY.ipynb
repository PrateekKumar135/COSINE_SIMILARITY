{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA9ix0mgEaTWZHdeZIB/hP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85d658b3e8164a0f9ec2bdf5bb10ed4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f272b20f37b44349f50846bfb68dc10",
              "IPY_MODEL_f98ad67994a743918c24af432bd1d4ad",
              "IPY_MODEL_092bc3fc4fb0440fabef9d3fd129272f"
            ],
            "layout": "IPY_MODEL_316d343aa5224077bd9d767a48b58f96"
          }
        },
        "8f272b20f37b44349f50846bfb68dc10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4cf4d8dad244249108a1d5282b3ba0",
            "placeholder": "​",
            "style": "IPY_MODEL_4e2084c73e1e4af5a9766f6349ab6c18",
            "value": "README.md: 100%"
          }
        },
        "f98ad67994a743918c24af432bd1d4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71b80aac01644da2afa01a13c891c56d",
            "max": 2297,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dffcb9c5d894f49b30df6106de3b4dd",
            "value": 2297
          }
        },
        "092bc3fc4fb0440fabef9d3fd129272f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20721f1a2ad348eba0e96d082787d601",
            "placeholder": "​",
            "style": "IPY_MODEL_2e7b97496b084d28a8e79c90bf638a65",
            "value": " 2.30k/2.30k [00:00&lt;00:00, 173kB/s]"
          }
        },
        "316d343aa5224077bd9d767a48b58f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4cf4d8dad244249108a1d5282b3ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2084c73e1e4af5a9766f6349ab6c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71b80aac01644da2afa01a13c891c56d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dffcb9c5d894f49b30df6106de3b4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20721f1a2ad348eba0e96d082787d601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e7b97496b084d28a8e79c90bf638a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee17aa810d424d4287ba8e4b77de605b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4355ea73f4b8444dbaac7a5276e16bd6",
              "IPY_MODEL_c318a4e9c2d44b22b1d800a68da7456d",
              "IPY_MODEL_32d6ba92dccd4752aa5e866df924c1b6"
            ],
            "layout": "IPY_MODEL_486d87855c634509a43bdab010853d8d"
          }
        },
        "4355ea73f4b8444dbaac7a5276e16bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a9365a45c14cda92c8f4fcd26fee9f",
            "placeholder": "​",
            "style": "IPY_MODEL_868db91f0acd4c0bb1b0cc98a32d21d6",
            "value": "train.txt.gz: 100%"
          }
        },
        "c318a4e9c2d44b22b1d800a68da7456d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d5570f7b424166bbeae30b0a492e38",
            "max": 85467066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_311bf99a88a642cca06e3ce85559a8c5",
            "value": 85467066
          }
        },
        "32d6ba92dccd4752aa5e866df924c1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c60552b0fcf449caeed6849a6b07b55",
            "placeholder": "​",
            "style": "IPY_MODEL_182ee4ad4bdc45cc90e176732dd0bcc6",
            "value": " 85.5M/85.5M [00:57&lt;00:00, 1.12MB/s]"
          }
        },
        "486d87855c634509a43bdab010853d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a9365a45c14cda92c8f4fcd26fee9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868db91f0acd4c0bb1b0cc98a32d21d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d5570f7b424166bbeae30b0a492e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311bf99a88a642cca06e3ce85559a8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c60552b0fcf449caeed6849a6b07b55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182ee4ad4bdc45cc90e176732dd0bcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8930c6f9dfb842c9a0184411b47219ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa641a3075de44fb863132ba4cad7448",
              "IPY_MODEL_b9c81000961a40f4a132bfea446c42c9",
              "IPY_MODEL_00de95eb38344634913fea90ec5fa3ea"
            ],
            "layout": "IPY_MODEL_f4d0d0f4df704c8cb79f40cc94f15804"
          }
        },
        "aa641a3075de44fb863132ba4cad7448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b468022f23433ba35b034b90def555",
            "placeholder": "​",
            "style": "IPY_MODEL_7e4b89d7d8b74f05b283d47c95f6f74d",
            "value": "test.txt.gz: 100%"
          }
        },
        "b9c81000961a40f4a132bfea446c42c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61445e8d92c747c0926ad85687997c49",
            "max": 9488687,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f4c4e7cc6d9426682a308e6fb17ad02",
            "value": 9488687
          }
        },
        "00de95eb38344634913fea90ec5fa3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef759869b4844490a456e90a3926e8d3",
            "placeholder": "​",
            "style": "IPY_MODEL_36656d1ab45c400f88ca35217d98942b",
            "value": " 9.49M/9.49M [00:08&lt;00:00, 1.06MB/s]"
          }
        },
        "f4d0d0f4df704c8cb79f40cc94f15804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b468022f23433ba35b034b90def555": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4b89d7d8b74f05b283d47c95f6f74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61445e8d92c747c0926ad85687997c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f4c4e7cc6d9426682a308e6fb17ad02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef759869b4844490a456e90a3926e8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36656d1ab45c400f88ca35217d98942b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "692b4a3583624d72ba8c758f39a0c61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32d00563c27b4cd8a259133df9e5417a",
              "IPY_MODEL_0cfdd64e0a84484fb1f7a3a186b06e52",
              "IPY_MODEL_18ee7b58a28e478cb6f91c9f1f3e161d"
            ],
            "layout": "IPY_MODEL_83e3aa563e0e4725b07e651140307860"
          }
        },
        "32d00563c27b4cd8a259133df9e5417a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3ada42899e443d9174ba071accf7b3",
            "placeholder": "​",
            "style": "IPY_MODEL_13289743691a465886ed0b17f60fa5b3",
            "value": "Generating train split: 100%"
          }
        },
        "0cfdd64e0a84484fb1f7a3a186b06e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0000a09360a442fbae5dc1a90dfe5eff",
            "max": 1534699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5828c89520aa4c38ae0f68d535e441a6",
            "value": 1534699
          }
        },
        "18ee7b58a28e478cb6f91c9f1f3e161d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2053ce08922846229efaf71d016201c1",
            "placeholder": "​",
            "style": "IPY_MODEL_65beac3c8548451b920072097ca28a02",
            "value": " 1534699/1534699 [00:05&lt;00:00, 307519.51 examples/s]"
          }
        },
        "83e3aa563e0e4725b07e651140307860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3ada42899e443d9174ba071accf7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13289743691a465886ed0b17f60fa5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0000a09360a442fbae5dc1a90dfe5eff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5828c89520aa4c38ae0f68d535e441a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2053ce08922846229efaf71d016201c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65beac3c8548451b920072097ca28a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68dafd4e21ea41c6996c8bb250ecff12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f13822d7fca44b0bb86e2508735c592",
              "IPY_MODEL_ade7a059d33b48daa576c55bbde1cf76",
              "IPY_MODEL_48ec2d022e164e96b3e491451294d5ca"
            ],
            "layout": "IPY_MODEL_c83beccbbf5b41b780050acab7842e66"
          }
        },
        "3f13822d7fca44b0bb86e2508735c592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c3bd4ae806b419791d8672530706a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_aacbb9a46b4b4e1ebadfa47529fb87be",
            "value": "Generating test split: 100%"
          }
        },
        "ade7a059d33b48daa576c55bbde1cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c08871f2ae25421c88feec5f0f800bd1",
            "max": 170522,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a2e36ca980d4a29b7688fbb13b6962d",
            "value": 170522
          }
        },
        "48ec2d022e164e96b3e491451294d5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b19609bbbf4944eeb95c8197998dbba8",
            "placeholder": "​",
            "style": "IPY_MODEL_8071844e3596423391e1a0d94e032d9d",
            "value": " 170522/170522 [00:00&lt;00:00, 291705.92 examples/s]"
          }
        },
        "c83beccbbf5b41b780050acab7842e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3bd4ae806b419791d8672530706a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aacbb9a46b4b4e1ebadfa47529fb87be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c08871f2ae25421c88feec5f0f800bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2e36ca980d4a29b7688fbb13b6962d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b19609bbbf4944eeb95c8197998dbba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8071844e3596423391e1a0d94e032d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrateekKumar135/COSINE_SIMILARITY/blob/main/COSINE_SIMILARITY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING LIBRARIES"
      ],
      "metadata": {
        "id": "9TYniuRj8xGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14Tr4ZhI8Ekb",
        "outputId": "3f57b84c-8199-4f97-ab6a-45863f9de5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tensorflow tensorflow-text nltk gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING LIBRARIES"
      ],
      "metadata": {
        "id": "D5CAv9ML83hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text  # Required for tokenization\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "Il8JRudh8FeU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET LOAD"
      ],
      "metadata": {
        "id": "gXljnNyO9DK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"agentlans/high-quality-english-sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "85d658b3e8164a0f9ec2bdf5bb10ed4e",
            "8f272b20f37b44349f50846bfb68dc10",
            "f98ad67994a743918c24af432bd1d4ad",
            "092bc3fc4fb0440fabef9d3fd129272f",
            "316d343aa5224077bd9d767a48b58f96",
            "7a4cf4d8dad244249108a1d5282b3ba0",
            "4e2084c73e1e4af5a9766f6349ab6c18",
            "71b80aac01644da2afa01a13c891c56d",
            "6dffcb9c5d894f49b30df6106de3b4dd",
            "20721f1a2ad348eba0e96d082787d601",
            "2e7b97496b084d28a8e79c90bf638a65",
            "ee17aa810d424d4287ba8e4b77de605b",
            "4355ea73f4b8444dbaac7a5276e16bd6",
            "c318a4e9c2d44b22b1d800a68da7456d",
            "32d6ba92dccd4752aa5e866df924c1b6",
            "486d87855c634509a43bdab010853d8d",
            "e2a9365a45c14cda92c8f4fcd26fee9f",
            "868db91f0acd4c0bb1b0cc98a32d21d6",
            "17d5570f7b424166bbeae30b0a492e38",
            "311bf99a88a642cca06e3ce85559a8c5",
            "5c60552b0fcf449caeed6849a6b07b55",
            "182ee4ad4bdc45cc90e176732dd0bcc6",
            "8930c6f9dfb842c9a0184411b47219ca",
            "aa641a3075de44fb863132ba4cad7448",
            "b9c81000961a40f4a132bfea446c42c9",
            "00de95eb38344634913fea90ec5fa3ea",
            "f4d0d0f4df704c8cb79f40cc94f15804",
            "a7b468022f23433ba35b034b90def555",
            "7e4b89d7d8b74f05b283d47c95f6f74d",
            "61445e8d92c747c0926ad85687997c49",
            "2f4c4e7cc6d9426682a308e6fb17ad02",
            "ef759869b4844490a456e90a3926e8d3",
            "36656d1ab45c400f88ca35217d98942b",
            "692b4a3583624d72ba8c758f39a0c61b",
            "32d00563c27b4cd8a259133df9e5417a",
            "0cfdd64e0a84484fb1f7a3a186b06e52",
            "18ee7b58a28e478cb6f91c9f1f3e161d",
            "83e3aa563e0e4725b07e651140307860",
            "fb3ada42899e443d9174ba071accf7b3",
            "13289743691a465886ed0b17f60fa5b3",
            "0000a09360a442fbae5dc1a90dfe5eff",
            "5828c89520aa4c38ae0f68d535e441a6",
            "2053ce08922846229efaf71d016201c1",
            "65beac3c8548451b920072097ca28a02",
            "68dafd4e21ea41c6996c8bb250ecff12",
            "3f13822d7fca44b0bb86e2508735c592",
            "ade7a059d33b48daa576c55bbde1cf76",
            "48ec2d022e164e96b3e491451294d5ca",
            "c83beccbbf5b41b780050acab7842e66",
            "9c3bd4ae806b419791d8672530706a1e",
            "aacbb9a46b4b4e1ebadfa47529fb87be",
            "c08871f2ae25421c88feec5f0f800bd1",
            "2a2e36ca980d4a29b7688fbb13b6962d",
            "b19609bbbf4944eeb95c8197998dbba8",
            "8071844e3596423391e1a0d94e032d9d"
          ]
        },
        "id": "fsgwGwEF8Fg7",
        "outputId": "c4503378-df0e-4650-fca4-915a704f3d29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/2.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85d658b3e8164a0f9ec2bdf5bb10ed4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.txt.gz:   0%|          | 0.00/85.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee17aa810d424d4287ba8e4b77de605b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.txt.gz:   0%|          | 0.00/9.49M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8930c6f9dfb842c9a0184411b47219ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1534699 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "692b4a3583624d72ba8c758f39a0c61b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/170522 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68dafd4e21ea41c6996c8bb250ecff12"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZFVGm948Fjp",
        "outputId": "64a4fb58-86d6-4d85-e3a0-16990ceb1128"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1534699\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 170522\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRACT TEXT DATA**"
      ],
      "metadata": {
        "id": "z1fsq6_h9ZGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data\n",
        "sentences = ds[\"train\"][\"text\"][:10000]\n",
        "sentences[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYtxbqp88Fmi",
        "outputId": "c454a632-154d-496f-d1cf-bad38aee7a56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soon we dropped into a living forest, where cold-tolerant evergreens and boreal animals still evoke the Canadian heritage of an ecosystem pushed south by glaciers 20,000 years ago.',\n",
              " 'Annual population growth rate (2011 est., CIA World Factbook): 1.284%.',\n",
              " 'This has led to the recent banning of Neonics in the EU, however the US and Canada are still using this chemical pesticide.',\n",
              " \"In addition, these colors weren't confined to a province but rather irregularly scattered across various regions over all of China.\",\n",
              " 'A family member or a support person may stay with a patient during recovery.',\n",
              " 'By the time emigrants got to what is now Nevada, they had been on the trail long enough that their supplies were down to just the basics.',\n",
              " \"Advances in valve technology allow air cannons to operate with lower ... generation, coal, metal and .... installed on a cement plant's primary crusher.\",\n",
              " 'It stretched 363 miles across New York State from the Hudson River in the east to Lake Erie in the west.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Sentences into TensorFlow Tensors**"
      ],
      "metadata": {
        "id": "bgDDi2Ze9jcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow tensors\n",
        "sentence_tensors = tf.constant(sentences)"
      ],
      "metadata": {
        "id": "v9AeoUuR8Ful"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Word Tokenization**"
      ],
      "metadata": {
        "id": "cNyL_ocw9rLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use UnicodeScriptTokenizer for word tokenization\n",
        "word_tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "word_tokens = word_tokenizer.tokenize(sentence_tensors)"
      ],
      "metadata": {
        "id": "x3cytcNl9ptx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Tokenized Words to List**"
      ],
      "metadata": {
        "id": "LsxW4w9C9xVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized words to list\n",
        "word_tokens_list = [tokens.numpy().tolist() for tokens in word_tokens]\n",
        "word_tokens_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrTiGJtx9pwj",
        "outputId": "0335ae4f-e922-4a38-a8e5-7a0bd5332606"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'Soon',\n",
              "  b'we',\n",
              "  b'dropped',\n",
              "  b'into',\n",
              "  b'a',\n",
              "  b'living',\n",
              "  b'forest',\n",
              "  b',',\n",
              "  b'where',\n",
              "  b'cold',\n",
              "  b'-',\n",
              "  b'tolerant',\n",
              "  b'evergreens',\n",
              "  b'and',\n",
              "  b'boreal',\n",
              "  b'animals',\n",
              "  b'still',\n",
              "  b'evoke',\n",
              "  b'the',\n",
              "  b'Canadian',\n",
              "  b'heritage',\n",
              "  b'of',\n",
              "  b'an',\n",
              "  b'ecosystem',\n",
              "  b'pushed',\n",
              "  b'south',\n",
              "  b'by',\n",
              "  b'glaciers',\n",
              "  b'20,000',\n",
              "  b'years',\n",
              "  b'ago',\n",
              "  b'.'],\n",
              " [b'Annual',\n",
              "  b'population',\n",
              "  b'growth',\n",
              "  b'rate',\n",
              "  b'(2011',\n",
              "  b'est',\n",
              "  b'.,',\n",
              "  b'CIA',\n",
              "  b'World',\n",
              "  b'Factbook',\n",
              "  b'):1.284%.'],\n",
              " [b'This',\n",
              "  b'has',\n",
              "  b'led',\n",
              "  b'to',\n",
              "  b'the',\n",
              "  b'recent',\n",
              "  b'banning',\n",
              "  b'of',\n",
              "  b'Neonics',\n",
              "  b'in',\n",
              "  b'the',\n",
              "  b'EU',\n",
              "  b',',\n",
              "  b'however',\n",
              "  b'the',\n",
              "  b'US',\n",
              "  b'and',\n",
              "  b'Canada',\n",
              "  b'are',\n",
              "  b'still',\n",
              "  b'using',\n",
              "  b'this',\n",
              "  b'chemical',\n",
              "  b'pesticide',\n",
              "  b'.'],\n",
              " [b'In',\n",
              "  b'addition',\n",
              "  b',',\n",
              "  b'these',\n",
              "  b'colors',\n",
              "  b'weren',\n",
              "  b\"'\",\n",
              "  b't',\n",
              "  b'confined',\n",
              "  b'to',\n",
              "  b'a',\n",
              "  b'province',\n",
              "  b'but',\n",
              "  b'rather',\n",
              "  b'irregularly',\n",
              "  b'scattered',\n",
              "  b'across',\n",
              "  b'various',\n",
              "  b'regions',\n",
              "  b'over',\n",
              "  b'all',\n",
              "  b'of',\n",
              "  b'China',\n",
              "  b'.'],\n",
              " [b'A',\n",
              "  b'family',\n",
              "  b'member',\n",
              "  b'or',\n",
              "  b'a',\n",
              "  b'support',\n",
              "  b'person',\n",
              "  b'may',\n",
              "  b'stay',\n",
              "  b'with',\n",
              "  b'a',\n",
              "  b'patient',\n",
              "  b'during',\n",
              "  b'recovery',\n",
              "  b'.']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decode Byte Tokens into Strings**"
      ],
      "metadata": {
        "id": "YL_7kGNZ976q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode byte tokens to strings\n",
        "decoded_tokens = [[token.decode(\"utf-8\") for token in sentence] for sentence in word_tokens_list]"
      ],
      "metadata": {
        "id": "qwrP15wG9p1S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Required NLTK Resources**"
      ],
      "metadata": {
        "id": "1rCRRP3Q-BgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords list if not already downloaded\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOqbrxTM9p4Y",
        "outputId": "b91ed371-17eb-476c-beb7-5cebcc65bf79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a Text Cleaning Function**"
      ],
      "metadata": {
        "id": "_hxeP6cV-H58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply stopword removal\n",
        "def remove_stopwords(tokens):\n",
        "    return [word.lower() for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Function to apply regex processing\n",
        "def apply_regex(tokens):\n",
        "    return [re.sub(r\"[^a-zA-Z]\", \"\", word).strip() for word in tokens if re.sub(r\"[^a-zA-Z]\", \"\", word).strip()]  # Keep only letters\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def apply_lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Apply processing in sequence\n",
        "processed_sentences = []\n",
        "for sentence in decoded_tokens:\n",
        "    stopword_removed = remove_stopwords(sentence)  # Step 1: Remove stopwords\n",
        "    regex_applied = apply_regex(stopword_removed)  # Step 2: Apply regex cleaning\n",
        "    lemmatized = apply_lemmatization(regex_applied)  # Step 3: Apply lemmatization\n",
        "\n",
        "    processed_sentences.append({\n",
        "        \"After Stopword Removal\": stopword_removed,\n",
        "        \"After Regex\": regex_applied,\n",
        "        \"After Lemmatization\": lemmatized\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "for i, process in enumerate(processed_sentences):\n",
        "    print(f\"🔹 Sentence {i+1}\")\n",
        "    print(f\"    After Stopword Removal: {process['After Stopword Removal']}\")\n",
        "    print(f\"    After Regex: {process['After Regex']}\")\n",
        "    print(f\"    After Lemmatization: {process['After Lemmatization']}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU-FROoa9p7K",
        "outputId": "4ef88203-0edc-4df5-eec6-3cb43341a410"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "🔹 Sentence 9001\n",
            "    After Stopword Removal: ['birthplace', 'america', \"'\", 'experimental', '-', 'theater', 'scene', 'fostered', 'writer', 'sam', 'shepard', ',', 'director', 'jerzy', 'grotowski', ',', 'composer', 'elizabeth', 'swados', '.']\n",
            "    After Regex: ['birthplace', 'america', 'experimental', 'theater', 'scene', 'fostered', 'writer', 'sam', 'shepard', 'director', 'jerzy', 'grotowski', 'composer', 'elizabeth', 'swados']\n",
            "    After Lemmatization: ['birthplace', 'america', 'experimental', 'theater', 'scene', 'fostered', 'writer', 'sam', 'shepard', 'director', 'jerzy', 'grotowski', 'composer', 'elizabeth', 'swados']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9002\n",
            "    After Stopword Removal: ['lenin', '(', 'basing', 'position', 'developed', 'earlier', 'kautsky', ',', 'principal', 'theoretician', 'spd', ')', 'insisted', 'socialist', 'consciousness', 'develop', 'spontaneously', 'within', 'working', 'class', ',', 'brought', 'workers', \"'\", 'movement', '.']\n",
            "    After Regex: ['lenin', 'basing', 'position', 'developed', 'earlier', 'kautsky', 'principal', 'theoretician', 'spd', 'insisted', 'socialist', 'consciousness', 'develop', 'spontaneously', 'within', 'working', 'class', 'brought', 'workers', 'movement']\n",
            "    After Lemmatization: ['lenin', 'basing', 'position', 'developed', 'earlier', 'kautsky', 'principal', 'theoretician', 'spd', 'insisted', 'socialist', 'consciousness', 'develop', 'spontaneously', 'within', 'working', 'class', 'brought', 'worker', 'movement']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9003\n",
            "    After Stopword Removal: ['child', 'banker', 'parent', 'may', 'well', 'math', ',', 'might', 'suffer', 'chemistry', '.']\n",
            "    After Regex: ['child', 'banker', 'parent', 'may', 'well', 'math', 'might', 'suffer', 'chemistry']\n",
            "    After Lemmatization: ['child', 'banker', 'parent', 'may', 'well', 'math', 'might', 'suffer', 'chemistry']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9004\n",
            "    After Stopword Removal: ['remarkable', 'bibliography', ',', 'took', 'sixteen', 'years', 'compile', ',', 'heralded', 'mormon', 'history', 'association', 'well', 'historians', 'generally', 'valuable', 'tool', 'yet', 'appear', 'students', 'lds', 'history', '.']\n",
            "    After Regex: ['remarkable', 'bibliography', 'took', 'sixteen', 'years', 'compile', 'heralded', 'mormon', 'history', 'association', 'well', 'historians', 'generally', 'valuable', 'tool', 'yet', 'appear', 'students', 'lds', 'history']\n",
            "    After Lemmatization: ['remarkable', 'bibliography', 'took', 'sixteen', 'year', 'compile', 'heralded', 'mormon', 'history', 'association', 'well', 'historian', 'generally', 'valuable', 'tool', 'yet', 'appear', 'student', 'lds', 'history']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9005\n",
            "    After Stopword Removal: ['indiana', 'machine', 'builder', 'helped', 'boat', 'company', 'simplify', 'speed', 'process', 'producing', 'tooling', 'used', 'make', 'fibreglass', 'boat', 'mould', '.']\n",
            "    After Regex: ['indiana', 'machine', 'builder', 'helped', 'boat', 'company', 'simplify', 'speed', 'process', 'producing', 'tooling', 'used', 'make', 'fibreglass', 'boat', 'mould']\n",
            "    After Lemmatization: ['indiana', 'machine', 'builder', 'helped', 'boat', 'company', 'simplify', 'speed', 'process', 'producing', 'tooling', 'used', 'make', 'fibreglass', 'boat', 'mould']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9006\n",
            "    After Stopword Removal: ['average', 'amount', 'borrowed', 'going', '5%', 'per', 'year', ',', 'accrued', 'interest', 'adds', '15%', 'principal', 'debt', 'levels', '.']\n",
            "    After Regex: ['average', 'amount', 'borrowed', 'going', 'per', 'year', 'accrued', 'interest', 'adds', 'principal', 'debt', 'levels']\n",
            "    After Lemmatization: ['average', 'amount', 'borrowed', 'going', 'per', 'year', 'accrued', 'interest', 'add', 'principal', 'debt', 'level']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9007\n",
            "    After Stopword Removal: ['probably', 'culpable', 'ignorance', 'error', 'solomon', 'laments', ',', 'pleasures', ',', 'many', 'amusements', 'court', ',', 'blinded', 'eyes', 'cast', 'mist', ',', 'could', 'attain', 'true', 'wisdom', 'designed', '.']\n",
            "    After Regex: ['probably', 'culpable', 'ignorance', 'error', 'solomon', 'laments', 'pleasures', 'many', 'amusements', 'court', 'blinded', 'eyes', 'cast', 'mist', 'could', 'attain', 'true', 'wisdom', 'designed']\n",
            "    After Lemmatization: ['probably', 'culpable', 'ignorance', 'error', 'solomon', 'lament', 'pleasure', 'many', 'amusement', 'court', 'blinded', 'eye', 'cast', 'mist', 'could', 'attain', 'true', 'wisdom', 'designed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9008\n",
            "    After Stopword Removal: ['chemicals', 'turn', 'steam', 'vapor', 'use', '.']\n",
            "    After Regex: ['chemicals', 'turn', 'steam', 'vapor', 'use']\n",
            "    After Lemmatization: ['chemical', 'turn', 'steam', 'vapor', 'use']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9009\n",
            "    After Stopword Removal: ['result', 'equation', 'describing', 'matter', 'antimatter', 'terms', 'quantum', 'fields', '.']\n",
            "    After Regex: ['result', 'equation', 'describing', 'matter', 'antimatter', 'terms', 'quantum', 'fields']\n",
            "    After Lemmatization: ['result', 'equation', 'describing', 'matter', 'antimatter', 'term', 'quantum', 'field']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9010\n",
            "    After Stopword Removal: ['wit', ':', 'study', 'published', 'last', 'year', 'agency', 'healthcare', 'research', 'quality', ',', 'part', 'u', '.', '.', 'department', 'health', 'human', 'services', ',', 'found', 'hospitalizations', 'eating', 'disorders', 'kids', '12', 'younger', 'jumped', '119', 'percent', '1999', '2006.']\n",
            "    After Regex: ['wit', 'study', 'published', 'last', 'year', 'agency', 'healthcare', 'research', 'quality', 'part', 'u', 'department', 'health', 'human', 'services', 'found', 'hospitalizations', 'eating', 'disorders', 'kids', 'younger', 'jumped', 'percent']\n",
            "    After Lemmatization: ['wit', 'study', 'published', 'last', 'year', 'agency', 'healthcare', 'research', 'quality', 'part', 'u', 'department', 'health', 'human', 'service', 'found', 'hospitalization', 'eating', 'disorder', 'kid', 'younger', 'jumped', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9011\n",
            "    After Stopword Removal: ['samaritan', 'theology', ',', 'messiah', 'thought', 'prophet', 'woman', 'well', 'led', 'faith', 'lord', \"'\", 'prophetic', 'ability', 'know', 'life', '.']\n",
            "    After Regex: ['samaritan', 'theology', 'messiah', 'thought', 'prophet', 'woman', 'well', 'led', 'faith', 'lord', 'prophetic', 'ability', 'know', 'life']\n",
            "    After Lemmatization: ['samaritan', 'theology', 'messiah', 'thought', 'prophet', 'woman', 'well', 'led', 'faith', 'lord', 'prophetic', 'ability', 'know', 'life']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9012\n",
            "    After Stopword Removal: ['common', 'reason', 'discontinuation', 'aes', '.']\n",
            "    After Regex: ['common', 'reason', 'discontinuation', 'aes']\n",
            "    After Lemmatization: ['common', 'reason', 'discontinuation', 'aes']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9013\n",
            "    After Stopword Removal: ['especially', 'knowledge', '/', 'intellects', '.']\n",
            "    After Regex: ['especially', 'knowledge', 'intellects']\n",
            "    After Lemmatization: ['especially', 'knowledge', 'intellect']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9014\n",
            "    After Stopword Removal: ['upanishads', 'prove', 'conviction', 'ancient', 'hindu', 'texts', 'support', 'monotheism', '.']\n",
            "    After Regex: ['upanishads', 'prove', 'conviction', 'ancient', 'hindu', 'texts', 'support', 'monotheism']\n",
            "    After Lemmatization: ['upanishad', 'prove', 'conviction', 'ancient', 'hindu', 'text', 'support', 'monotheism']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9015\n",
            "    After Stopword Removal: ['may', 'sprayed', 'skin', 'surface', 'form', 'minute', 'crystals', 'handheld', 'sprayer', ',', 'small', 'vacuum', 'simultaneously', 'removes', 'dead', 'loosened', 'skin', 'cells', 'used', 'chips', '.']\n",
            "    After Regex: ['may', 'sprayed', 'skin', 'surface', 'form', 'minute', 'crystals', 'handheld', 'sprayer', 'small', 'vacuum', 'simultaneously', 'removes', 'dead', 'loosened', 'skin', 'cells', 'used', 'chips']\n",
            "    After Lemmatization: ['may', 'sprayed', 'skin', 'surface', 'form', 'minute', 'crystal', 'handheld', 'sprayer', 'small', 'vacuum', 'simultaneously', 'remove', 'dead', 'loosened', 'skin', 'cell', 'used', 'chip']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9016\n",
            "    After Stopword Removal: ['police', 'lobbed', 'teargas', 'shells', 'used', 'batons', 'disperse', 'protesters', 'turned', 'violent', 'resorted', 'heavy', 'stone', 'pelting', '.']\n",
            "    After Regex: ['police', 'lobbed', 'teargas', 'shells', 'used', 'batons', 'disperse', 'protesters', 'turned', 'violent', 'resorted', 'heavy', 'stone', 'pelting']\n",
            "    After Lemmatization: ['police', 'lobbed', 'teargas', 'shell', 'used', 'baton', 'disperse', 'protester', 'turned', 'violent', 'resorted', 'heavy', 'stone', 'pelting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9017\n",
            "    After Stopword Removal: ['example', 'slowing', 'cyclic', 'speed', 'marker', ',', 'also', 'reduce', 'rate', 'fire', '.']\n",
            "    After Regex: ['example', 'slowing', 'cyclic', 'speed', 'marker', 'also', 'reduce', 'rate', 'fire']\n",
            "    After Lemmatization: ['example', 'slowing', 'cyclic', 'speed', 'marker', 'also', 'reduce', 'rate', 'fire']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9018\n",
            "    After Stopword Removal: ['addition', ',', 'found', 'part', 'lower', 'market', 'stood', 'east', 'redan', 'early', '1750', 'extended', 'redan', '1786', 'removed', '1800.']\n",
            "    After Regex: ['addition', 'found', 'part', 'lower', 'market', 'stood', 'east', 'redan', 'early', 'extended', 'redan', 'removed']\n",
            "    After Lemmatization: ['addition', 'found', 'part', 'lower', 'market', 'stood', 'east', 'redan', 'early', 'extended', 'redan', 'removed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9019\n",
            "    After Stopword Removal: ['small', 'business', 'loans', 'ideal', 'way', 'enhance', 'growth', 'small', 'business', '.']\n",
            "    After Regex: ['small', 'business', 'loans', 'ideal', 'way', 'enhance', 'growth', 'small', 'business']\n",
            "    After Lemmatization: ['small', 'business', 'loan', 'ideal', 'way', 'enhance', 'growth', 'small', 'business']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9020\n",
            "    After Stopword Removal: ['appropriate', 'adopt', 'resolution', 'today', 'ask', 'governments', 'europe', 'osce', 'individually', 'crack', 'anti', '-', 'semitism', ',', 'speak', ',', 'act', 'many', 'governments', 'europe', ',', 'many', 'parts', 'political', 'left', 'europe', 'elsewhere', 'well', 'right', 'done', '.']\n",
            "    After Regex: ['appropriate', 'adopt', 'resolution', 'today', 'ask', 'governments', 'europe', 'osce', 'individually', 'crack', 'anti', 'semitism', 'speak', 'act', 'many', 'governments', 'europe', 'many', 'parts', 'political', 'left', 'europe', 'elsewhere', 'well', 'right', 'done']\n",
            "    After Lemmatization: ['appropriate', 'adopt', 'resolution', 'today', 'ask', 'government', 'europe', 'osce', 'individually', 'crack', 'anti', 'semitism', 'speak', 'act', 'many', 'government', 'europe', 'many', 'part', 'political', 'left', 'europe', 'elsewhere', 'well', 'right', 'done']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9021\n",
            "    After Stopword Removal: ['systems', 'require', '-', 'plumbing', 'water', 'drain', 'lines', '.']\n",
            "    After Regex: ['systems', 'require', 'plumbing', 'water', 'drain', 'lines']\n",
            "    After Lemmatization: ['system', 'require', 'plumbing', 'water', 'drain', 'line']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9022\n",
            "    After Stopword Removal: ['lombard', 'offers', 'insights', 'solve', 'problems', 'beset', 'car', ';', 'anything', ',', 'book', 'suggests', 'insights', 'run', 'opposite', 'direction', '—', 'problems', 'car', 'reflect', 'fragility', 'contradictions', 'international', 'system', '.']\n",
            "    After Regex: ['lombard', 'offers', 'insights', 'solve', 'problems', 'beset', 'car', 'anything', 'book', 'suggests', 'insights', 'run', 'opposite', 'direction', 'problems', 'car', 'reflect', 'fragility', 'contradictions', 'international', 'system']\n",
            "    After Lemmatization: ['lombard', 'offer', 'insight', 'solve', 'problem', 'beset', 'car', 'anything', 'book', 'suggests', 'insight', 'run', 'opposite', 'direction', 'problem', 'car', 'reflect', 'fragility', 'contradiction', 'international', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9023\n",
            "    After Stopword Removal: ['turn', 'research', 'project', 'software', 'project', 'create', 'something', 'others', 'readily', 'reuse', '.']\n",
            "    After Regex: ['turn', 'research', 'project', 'software', 'project', 'create', 'something', 'others', 'readily', 'reuse']\n",
            "    After Lemmatization: ['turn', 'research', 'project', 'software', 'project', 'create', 'something', 'others', 'readily', 'reuse']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9024\n",
            "    After Stopword Removal: ['american', 'association', 'bovine', 'practitioners', '(', 'aabp', ')', 'created', 'document', 'help', 'bovine', 'veterinarians', 'create', 'effective', 'antimicrobial', 'stewardship', 'programs', 'dairy', 'beef', 'producers', '.']\n",
            "    After Regex: ['american', 'association', 'bovine', 'practitioners', 'aabp', 'created', 'document', 'help', 'bovine', 'veterinarians', 'create', 'effective', 'antimicrobial', 'stewardship', 'programs', 'dairy', 'beef', 'producers']\n",
            "    After Lemmatization: ['american', 'association', 'bovine', 'practitioner', 'aabp', 'created', 'document', 'help', 'bovine', 'veterinarian', 'create', 'effective', 'antimicrobial', 'stewardship', 'program', 'dairy', 'beef', 'producer']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9025\n",
            "    After Stopword Removal: ['bangladesh', 'sri', 'lanka', 'might', 'closer', 'india', 'terms', 'standards', 'governance', ',', 'though', 'may', 'saying', 'much', 'given', 'prevalent', '.']\n",
            "    After Regex: ['bangladesh', 'sri', 'lanka', 'might', 'closer', 'india', 'terms', 'standards', 'governance', 'though', 'may', 'saying', 'much', 'given', 'prevalent']\n",
            "    After Lemmatization: ['bangladesh', 'sri', 'lanka', 'might', 'closer', 'india', 'term', 'standard', 'governance', 'though', 'may', 'saying', 'much', 'given', 'prevalent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9026\n",
            "    After Stopword Removal: [',', 'would', 'expect', 'anything', 'grow', 'staphylococcus', 'gram', 'positive', ',', 'emb', 'used', 'identify', 'gram', 'negative', 'bacteria', '.']\n",
            "    After Regex: ['would', 'expect', 'anything', 'grow', 'staphylococcus', 'gram', 'positive', 'emb', 'used', 'identify', 'gram', 'negative', 'bacteria']\n",
            "    After Lemmatization: ['would', 'expect', 'anything', 'grow', 'staphylococcus', 'gram', 'positive', 'emb', 'used', 'identify', 'gram', 'negative', 'bacteria']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9027\n",
            "    After Stopword Removal: ['services', 'assist', 'become', 'lifelong', 'learners', ',', 'able', 'contribute', 'function', 'multicultural', 'globally', 'competitive', 'world', '.']\n",
            "    After Regex: ['services', 'assist', 'become', 'lifelong', 'learners', 'able', 'contribute', 'function', 'multicultural', 'globally', 'competitive', 'world']\n",
            "    After Lemmatization: ['service', 'assist', 'become', 'lifelong', 'learner', 'able', 'contribute', 'function', 'multicultural', 'globally', 'competitive', 'world']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9028\n",
            "    After Stopword Removal: ['least', '35%', 'registered', 'boats', 'u', '.', '.', 'uninsured', '.']\n",
            "    After Regex: ['least', 'registered', 'boats', 'u', 'uninsured']\n",
            "    After Lemmatization: ['least', 'registered', 'boat', 'u', 'uninsured']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9029\n",
            "    After Stopword Removal: ['eventually', 'companions', 'killed', 'efforts', 'evangelize', 'tribe', '.']\n",
            "    After Regex: ['eventually', 'companions', 'killed', 'efforts', 'evangelize', 'tribe']\n",
            "    After Lemmatization: ['eventually', 'companion', 'killed', 'effort', 'evangelize', 'tribe']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9030\n",
            "    After Stopword Removal: ['report', 'begins', 'brief', 'illustration', 'context', 'large', '-', 'scale', 'land', 'acquisition', 'emphasising', 'importance', 'finding', 'methods', 'encouraging', 'transparency', 'accountability', 'deals', 'considered', '.']\n",
            "    After Regex: ['report', 'begins', 'brief', 'illustration', 'context', 'large', 'scale', 'land', 'acquisition', 'emphasising', 'importance', 'finding', 'methods', 'encouraging', 'transparency', 'accountability', 'deals', 'considered']\n",
            "    After Lemmatization: ['report', 'begin', 'brief', 'illustration', 'context', 'large', 'scale', 'land', 'acquisition', 'emphasising', 'importance', 'finding', 'method', 'encouraging', 'transparency', 'accountability', 'deal', 'considered']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9031\n",
            "    After Stopword Removal: ['henry', 'viii', 'catholic', 'everything', 'except', 'catholic', '.']\n",
            "    After Regex: ['henry', 'viii', 'catholic', 'everything', 'except', 'catholic']\n",
            "    After Lemmatization: ['henry', 'viii', 'catholic', 'everything', 'except', 'catholic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9032\n",
            "    After Stopword Removal: ['ginseng', 'reduce', 'fatigue', '?']\n",
            "    After Regex: ['ginseng', 'reduce', 'fatigue']\n",
            "    After Lemmatization: ['ginseng', 'reduce', 'fatigue']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9033\n",
            "    After Stopword Removal: ['impact', 'thailands', 'hivcontrol', 'programme', 'indicated', 'decline', 'sexually', 'transmitted', 'diseases', '.']\n",
            "    After Regex: ['impact', 'thailands', 'hivcontrol', 'programme', 'indicated', 'decline', 'sexually', 'transmitted', 'diseases']\n",
            "    After Lemmatization: ['impact', 'thailand', 'hivcontrol', 'programme', 'indicated', 'decline', 'sexually', 'transmitted', 'disease']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9034\n",
            "    After Stopword Removal: ['look', 'ibm', 'power', 'system', 'capabilities', 'amount', 'memory', 'single', 'system', ',', 'knew', 'support', 'scale', '-', 'scale', '-', 'configurations', '.']\n",
            "    After Regex: ['look', 'ibm', 'power', 'system', 'capabilities', 'amount', 'memory', 'single', 'system', 'knew', 'support', 'scale', 'scale', 'configurations']\n",
            "    After Lemmatization: ['look', 'ibm', 'power', 'system', 'capability', 'amount', 'memory', 'single', 'system', 'knew', 'support', 'scale', 'scale', 'configuration']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9035\n",
            "    After Stopword Removal: ['breathless', 'years', ',', 'labour', 'implemented', 'remarkably', 'radical', 'programme', ',', 'creating', 'nhs', ',', 'nationalising', 'coal', ',', 'steel', ',', 'rail', ',', 'road', 'transport', 'electricity', ',', 'initiating', 'mighty', 'house', '-', 'building', 'programme', '.']\n",
            "    After Regex: ['breathless', 'years', 'labour', 'implemented', 'remarkably', 'radical', 'programme', 'creating', 'nhs', 'nationalising', 'coal', 'steel', 'rail', 'road', 'transport', 'electricity', 'initiating', 'mighty', 'house', 'building', 'programme']\n",
            "    After Lemmatization: ['breathless', 'year', 'labour', 'implemented', 'remarkably', 'radical', 'programme', 'creating', 'nh', 'nationalising', 'coal', 'steel', 'rail', 'road', 'transport', 'electricity', 'initiating', 'mighty', 'house', 'building', 'programme']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9036\n",
            "    After Stopword Removal: ['according', 'invention', ',', 'patient', 'control', 'manifested', 'bringing', 'external', 'magnet', 'proximity', 'implanted', 'reed', 'switch', 'associated', 'pacemaker', '.']\n",
            "    After Regex: ['according', 'invention', 'patient', 'control', 'manifested', 'bringing', 'external', 'magnet', 'proximity', 'implanted', 'reed', 'switch', 'associated', 'pacemaker']\n",
            "    After Lemmatization: ['according', 'invention', 'patient', 'control', 'manifested', 'bringing', 'external', 'magnet', 'proximity', 'implanted', 'reed', 'switch', 'associated', 'pacemaker']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9037\n",
            "    After Stopword Removal: ['great', 'museums', 'manila', 'include', 'casa', 'manila', ',', 'light', 'sound', 'museum', ',', 'national', 'museum', 'philippines', ',', 'museo', 'pambata', 'metropolitan', 'museum', 'manila', '.']\n",
            "    After Regex: ['great', 'museums', 'manila', 'include', 'casa', 'manila', 'light', 'sound', 'museum', 'national', 'museum', 'philippines', 'museo', 'pambata', 'metropolitan', 'museum', 'manila']\n",
            "    After Lemmatization: ['great', 'museum', 'manila', 'include', 'casa', 'manila', 'light', 'sound', 'museum', 'national', 'museum', 'philippine', 'museo', 'pambata', 'metropolitan', 'museum', 'manila']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9038\n",
            "    After Stopword Removal: ['know', 'saddam', 'structures', 'deep', 'baghdad', '.']\n",
            "    After Regex: ['know', 'saddam', 'structures', 'deep', 'baghdad']\n",
            "    After Lemmatization: ['know', 'saddam', 'structure', 'deep', 'baghdad']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9039\n",
            "    After Stopword Removal: ['1747,', 'became', 'vestryman', 'bruton', 'parish', 'church', ',', '1748,', 'became', 'williamsburg', \"'\", 'representative', 'house', 'burgesses', ',', '1749,', 'justice', 'peace', '.']\n",
            "    After Regex: ['became', 'vestryman', 'bruton', 'parish', 'church', 'became', 'williamsburg', 'representative', 'house', 'burgesses', 'justice', 'peace']\n",
            "    After Lemmatization: ['became', 'vestryman', 'bruton', 'parish', 'church', 'became', 'williamsburg', 'representative', 'house', 'burgess', 'justice', 'peace']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9040\n",
            "    After Stopword Removal: ['explosion', 'mobile', 'technology', 'recent', 'years', ',', 'cell', 'phone', 'plans', 'improved', 'coverage', ',', 'price', ',', 'capabilities', '.']\n",
            "    After Regex: ['explosion', 'mobile', 'technology', 'recent', 'years', 'cell', 'phone', 'plans', 'improved', 'coverage', 'price', 'capabilities']\n",
            "    After Lemmatization: ['explosion', 'mobile', 'technology', 'recent', 'year', 'cell', 'phone', 'plan', 'improved', 'coverage', 'price', 'capability']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9041\n",
            "    After Stopword Removal: ['first', 'step', 'patent', 'suit', 'construe', 'claims', 'order', 'give', 'meaning', 'determine', 'scope', '(', 'whirlpool', 'corp', 'v', 'camco', 'inc', ',2000', 'scc', '67', 'para', '43[', 'whirlpool', ']).']\n",
            "    After Regex: ['first', 'step', 'patent', 'suit', 'construe', 'claims', 'order', 'give', 'meaning', 'determine', 'scope', 'whirlpool', 'corp', 'v', 'camco', 'inc', 'scc', 'para', 'whirlpool']\n",
            "    After Lemmatization: ['first', 'step', 'patent', 'suit', 'construe', 'claim', 'order', 'give', 'meaning', 'determine', 'scope', 'whirlpool', 'corp', 'v', 'camco', 'inc', 'scc', 'para', 'whirlpool']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9042\n",
            "    After Stopword Removal: ['vaginal', ',', 'cervical', ',', 'pelvic', 'infection', 'may', 'interfere', '&', 'c', '.']\n",
            "    After Regex: ['vaginal', 'cervical', 'pelvic', 'infection', 'may', 'interfere', 'c']\n",
            "    After Lemmatization: ['vaginal', 'cervical', 'pelvic', 'infection', 'may', 'interfere', 'c']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9043\n",
            "    After Stopword Removal: ['plan', 'attend', 'upcoming', 'workshop', 'landscaping', 'water', 'quality', 'focus', 'rain', 'gardens', '.']\n",
            "    After Regex: ['plan', 'attend', 'upcoming', 'workshop', 'landscaping', 'water', 'quality', 'focus', 'rain', 'gardens']\n",
            "    After Lemmatization: ['plan', 'attend', 'upcoming', 'workshop', 'landscaping', 'water', 'quality', 'focus', 'rain', 'garden']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9044\n",
            "    After Stopword Removal: ['earliest', 'known', 'charles', 'rex', '1699.']\n",
            "    After Regex: ['earliest', 'known', 'charles', 'rex']\n",
            "    After Lemmatization: ['earliest', 'known', 'charles', 'rex']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9045\n",
            "    After Stopword Removal: ['example', ':', 'tutoring', ';', 'serving', 'meals', 'homeless', 'shelter', ';', 'working', 'elderly', 'nursing', 'home', ',', 'etc', '.']\n",
            "    After Regex: ['example', 'tutoring', 'serving', 'meals', 'homeless', 'shelter', 'working', 'elderly', 'nursing', 'home', 'etc']\n",
            "    After Lemmatization: ['example', 'tutoring', 'serving', 'meal', 'homeless', 'shelter', 'working', 'elderly', 'nursing', 'home', 'etc']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9046\n",
            "    After Stopword Removal: ['since', 'beginning', 'humankind', ',', 'hate', 'around', ',', 'since', ',', 'good', 'people', 'faiths', 'backgrounds', 'worked', 'combat', '.']\n",
            "    After Regex: ['since', 'beginning', 'humankind', 'hate', 'around', 'since', 'good', 'people', 'faiths', 'backgrounds', 'worked', 'combat']\n",
            "    After Lemmatization: ['since', 'beginning', 'humankind', 'hate', 'around', 'since', 'good', 'people', 'faith', 'background', 'worked', 'combat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9047\n",
            "    After Stopword Removal: ['different', 'ways', 'doctors', 'treat', 'atrial', 'fibrillation', '.']\n",
            "    After Regex: ['different', 'ways', 'doctors', 'treat', 'atrial', 'fibrillation']\n",
            "    After Lemmatization: ['different', 'way', 'doctor', 'treat', 'atrial', 'fibrillation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9048\n",
            "    After Stopword Removal: ['going', 'supervise', 'students', ':', 'throughout', 'day', ',', 'school', 'starts', ',', 'recess', ',', 'lunch', 'homework', 'centre', 'time', '?']\n",
            "    After Regex: ['going', 'supervise', 'students', 'throughout', 'day', 'school', 'starts', 'recess', 'lunch', 'homework', 'centre', 'time']\n",
            "    After Lemmatization: ['going', 'supervise', 'student', 'throughout', 'day', 'school', 'start', 'recess', 'lunch', 'homework', 'centre', 'time']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9049\n",
            "    After Stopword Removal: ['low', '-', 'level', 'generally', 'means', ':', 'computer', 'science', ',', 'low', '-', 'level', 'programming', 'language', 'programming', 'language', 'provides', 'little', 'abstraction', 'computer', \"'\", 'instruction', 'set', 'architecture', '.']\n",
            "    After Regex: ['low', 'level', 'generally', 'means', 'computer', 'science', 'low', 'level', 'programming', 'language', 'programming', 'language', 'provides', 'little', 'abstraction', 'computer', 'instruction', 'set', 'architecture']\n",
            "    After Lemmatization: ['low', 'level', 'generally', 'mean', 'computer', 'science', 'low', 'level', 'programming', 'language', 'programming', 'language', 'provides', 'little', 'abstraction', 'computer', 'instruction', 'set', 'architecture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9050\n",
            "    After Stopword Removal: ['paying', 'debts', 'charity', ',', 'exterior', 'duties', 'neighbours', ',', 'heart', 'fixed', 'god', ',', 'purely', 'divine', 'view', ';', ',', 'even', 'public', 'actions', ',', 'deposits', 'intention', 'sentiments', 'bosom', 'god', 'redeemer', ',', 'regard', 'creatures', 'considers', 'god', 'holy', '.']\n",
            "    After Regex: ['paying', 'debts', 'charity', 'exterior', 'duties', 'neighbours', 'heart', 'fixed', 'god', 'purely', 'divine', 'view', 'even', 'public', 'actions', 'deposits', 'intention', 'sentiments', 'bosom', 'god', 'redeemer', 'regard', 'creatures', 'considers', 'god', 'holy']\n",
            "    After Lemmatization: ['paying', 'debt', 'charity', 'exterior', 'duty', 'neighbour', 'heart', 'fixed', 'god', 'purely', 'divine', 'view', 'even', 'public', 'action', 'deposit', 'intention', 'sentiment', 'bosom', 'god', 'redeemer', 'regard', 'creature', 'considers', 'god', 'holy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9051\n",
            "    After Stopword Removal: ['americans', 'steadily', 'working', 'overall', 'debt', 'levels', ',', 'including', 'mortgages', '.']\n",
            "    After Regex: ['americans', 'steadily', 'working', 'overall', 'debt', 'levels', 'including', 'mortgages']\n",
            "    After Lemmatization: ['american', 'steadily', 'working', 'overall', 'debt', 'level', 'including', 'mortgage']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9052\n",
            "    After Stopword Removal: ['original', 'marconi', 'transmitters', 'installed', 'brookmans', 'park', 'continued', 'use', 'second', 'world', 'war', '.']\n",
            "    After Regex: ['original', 'marconi', 'transmitters', 'installed', 'brookmans', 'park', 'continued', 'use', 'second', 'world', 'war']\n",
            "    After Lemmatization: ['original', 'marconi', 'transmitter', 'installed', 'brookmans', 'park', 'continued', 'use', 'second', 'world', 'war']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9053\n",
            "    After Stopword Removal: ['visit', 'digestive', 'disorders', 'comprehensive', 'view', 'possible', 'causes', 'abdominal', 'pain', '.']\n",
            "    After Regex: ['visit', 'digestive', 'disorders', 'comprehensive', 'view', 'possible', 'causes', 'abdominal', 'pain']\n",
            "    After Lemmatization: ['visit', 'digestive', 'disorder', 'comprehensive', 'view', 'possible', 'cause', 'abdominal', 'pain']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9054\n",
            "    After Stopword Removal: ['worthwhile', 'describing', 'feelings', '?']\n",
            "    After Regex: ['worthwhile', 'describing', 'feelings']\n",
            "    After Lemmatization: ['worthwhile', 'describing', 'feeling']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9055\n",
            "    After Stopword Removal: ['vision', 'led', 'disintegration', 'yugoslavia', ',', 'deaths', 'thousands', 'suffering', 'millions', '.']\n",
            "    After Regex: ['vision', 'led', 'disintegration', 'yugoslavia', 'deaths', 'thousands', 'suffering', 'millions']\n",
            "    After Lemmatization: ['vision', 'led', 'disintegration', 'yugoslavia', 'death', 'thousand', 'suffering', 'million']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9056\n",
            "    After Stopword Removal: ['estimate', ',', '6', 'percent', '2', 'percent', ',', 'respectively', ',', '69', 'million', 'americans', 'voted', 'obama', '.']\n",
            "    After Regex: ['estimate', 'percent', 'percent', 'respectively', 'million', 'americans', 'voted', 'obama']\n",
            "    After Lemmatization: ['estimate', 'percent', 'percent', 'respectively', 'million', 'american', 'voted', 'obama']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9057\n",
            "    After Stopword Removal: ['series', 'production', 'begins', ',', 'components', 'undergo', 'comprehensive', 'simulations', 'tests', '.']\n",
            "    After Regex: ['series', 'production', 'begins', 'components', 'undergo', 'comprehensive', 'simulations', 'tests']\n",
            "    After Lemmatization: ['series', 'production', 'begin', 'component', 'undergo', 'comprehensive', 'simulation', 'test']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9058\n",
            "    After Stopword Removal: ['many', 'people', ',95%', 'world', \"'\", 'population', ',', 'live', '!']\n",
            "    After Regex: ['many', 'people', 'world', 'population', 'live']\n",
            "    After Lemmatization: ['many', 'people', 'world', 'population', 'live']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9059\n",
            "    After Stopword Removal: ['quiet', 'shade', 'terra', 'cotta', ',', 'colour', 'deep', 'historical', 'roots', ',', 'deliberately', 'chosen', 'act', 'dominant', 'shade', 'interior', '.']\n",
            "    After Regex: ['quiet', 'shade', 'terra', 'cotta', 'colour', 'deep', 'historical', 'roots', 'deliberately', 'chosen', 'act', 'dominant', 'shade', 'interior']\n",
            "    After Lemmatization: ['quiet', 'shade', 'terra', 'cotta', 'colour', 'deep', 'historical', 'root', 'deliberately', 'chosen', 'act', 'dominant', 'shade', 'interior']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9060\n",
            "    After Stopword Removal: ['event', 'massive', 'scope', ',', 'taking', 'place', 'texas', ',', 'state', 'us', 'census', 'data', 'reveals', 'highest', 'percentage', 'uninsured', 'people', 'country', '25.2', 'percent', '.']\n",
            "    After Regex: ['event', 'massive', 'scope', 'taking', 'place', 'texas', 'state', 'us', 'census', 'data', 'reveals', 'highest', 'percentage', 'uninsured', 'people', 'country', 'percent']\n",
            "    After Lemmatization: ['event', 'massive', 'scope', 'taking', 'place', 'texas', 'state', 'u', 'census', 'data', 'reveals', 'highest', 'percentage', 'uninsured', 'people', 'country', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9061\n",
            "    After Stopword Removal: ['ligands', 'used', ':', 'metal', 'ions', ',', 'pt', '(', 'ii', ')-', 'ru', '(', 'iii', ')-', 'containing', 'potentially', 'anticancer', 'drugs', ',', 'antibiotics', 'porphyrins', 'metalloporphyrins', '.']\n",
            "    After Regex: ['ligands', 'used', 'metal', 'ions', 'pt', 'ii', 'ru', 'iii', 'containing', 'potentially', 'anticancer', 'drugs', 'antibiotics', 'porphyrins', 'metalloporphyrins']\n",
            "    After Lemmatization: ['ligand', 'used', 'metal', 'ion', 'pt', 'ii', 'ru', 'iii', 'containing', 'potentially', 'anticancer', 'drug', 'antibiotic', 'porphyrin', 'metalloporphyrins']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9062\n",
            "    After Stopword Removal: ['cap', 'reached', 'group', 'well', 'educated', 'non', '-', 'adecos', ',', 'including', 'involved', 'studies', 'change', 'state', '.']\n",
            "    After Regex: ['cap', 'reached', 'group', 'well', 'educated', 'non', 'adecos', 'including', 'involved', 'studies', 'change', 'state']\n",
            "    After Lemmatization: ['cap', 'reached', 'group', 'well', 'educated', 'non', 'adecos', 'including', 'involved', 'study', 'change', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9063\n",
            "    After Stopword Removal: ['adding', 'one', 'multiplying', 'get', '(3+1)(1+1)=4', 'x', '2=8.']\n",
            "    After Regex: ['adding', 'one', 'multiplying', 'get', 'x']\n",
            "    After Lemmatization: ['adding', 'one', 'multiplying', 'get', 'x']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9064\n",
            "    After Stopword Removal: ['andrews', 'experimental', 'forest', 'transformed', 'way', 'think', 'dead', 'wood', 'forests', '.']\n",
            "    After Regex: ['andrews', 'experimental', 'forest', 'transformed', 'way', 'think', 'dead', 'wood', 'forests']\n",
            "    After Lemmatization: ['andrew', 'experimental', 'forest', 'transformed', 'way', 'think', 'dead', 'wood', 'forest']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9065\n",
            "    After Stopword Removal: ['plan', 'also', 'includes', 'tougher', 'federal', 'laws', 'gun', 'trafficking', 'straw', 'purchases', ',', 'occur', 'person', 'legally', 'buys', 'firearm', 'sells', 'criminal', 'someone', 'else', 'barred', 'owning', 'weapon', '.']\n",
            "    After Regex: ['plan', 'also', 'includes', 'tougher', 'federal', 'laws', 'gun', 'trafficking', 'straw', 'purchases', 'occur', 'person', 'legally', 'buys', 'firearm', 'sells', 'criminal', 'someone', 'else', 'barred', 'owning', 'weapon']\n",
            "    After Lemmatization: ['plan', 'also', 'includes', 'tougher', 'federal', 'law', 'gun', 'trafficking', 'straw', 'purchase', 'occur', 'person', 'legally', 'buy', 'firearm', 'sell', 'criminal', 'someone', 'else', 'barred', 'owning', 'weapon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9066\n",
            "    After Stopword Removal: ['world', 'war', 'ii', 'hungary', 'jews', 'budapest', ',', 'raoul', 'wallenberg', 'man', '.']\n",
            "    After Regex: ['world', 'war', 'ii', 'hungary', 'jews', 'budapest', 'raoul', 'wallenberg', 'man']\n",
            "    After Lemmatization: ['world', 'war', 'ii', 'hungary', 'jew', 'budapest', 'raoul', 'wallenberg', 'man']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9067\n",
            "    After Stopword Removal: ['ideology', 'also', 'helps', 'white', 'americans', 'ignore', '/', 'rationalise', 'racialized', 'disparities', 'distribution', 'resources', '.']\n",
            "    After Regex: ['ideology', 'also', 'helps', 'white', 'americans', 'ignore', 'rationalise', 'racialized', 'disparities', 'distribution', 'resources']\n",
            "    After Lemmatization: ['ideology', 'also', 'help', 'white', 'american', 'ignore', 'rationalise', 'racialized', 'disparity', 'distribution', 'resource']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9068\n",
            "    After Stopword Removal: ['azure', ',', \"'\", 'virtual', 'machines', 'running', ',', 'rather', 'virtual', 'processes', ',', 'kind', 'ability', 'migrate', 'applications', 'one', 'machine', 'another', 'must', 'exist', '.']\n",
            "    After Regex: ['azure', 'virtual', 'machines', 'running', 'rather', 'virtual', 'processes', 'kind', 'ability', 'migrate', 'applications', 'one', 'machine', 'another', 'must', 'exist']\n",
            "    After Lemmatization: ['azure', 'virtual', 'machine', 'running', 'rather', 'virtual', 'process', 'kind', 'ability', 'migrate', 'application', 'one', 'machine', 'another', 'must', 'exist']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9069\n",
            "    After Stopword Removal: ['natural', 'gas', 'prices', 'affiliated', 'gas', 'prices', '.']\n",
            "    After Regex: ['natural', 'gas', 'prices', 'affiliated', 'gas', 'prices']\n",
            "    After Lemmatization: ['natural', 'gas', 'price', 'affiliated', 'gas', 'price']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9070\n",
            "    After Stopword Removal: ['would', 'adjust', 'frequency', 'rf', 'receiver', 'tv', 'set', '.']\n",
            "    After Regex: ['would', 'adjust', 'frequency', 'rf', 'receiver', 'tv', 'set']\n",
            "    After Lemmatization: ['would', 'adjust', 'frequency', 'rf', 'receiver', 'tv', 'set']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9071\n",
            "    After Stopword Removal: ['frequently', 'explain', 'beliefs', 'non', '-', 'mormons', ',', 'black', 'converts', ',', 'even', '.']\n",
            "    After Regex: ['frequently', 'explain', 'beliefs', 'non', 'mormons', 'black', 'converts', 'even']\n",
            "    After Lemmatization: ['frequently', 'explain', 'belief', 'non', 'mormon', 'black', 'convert', 'even']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9072\n",
            "    After Stopword Removal: ['users', 'also', 'explore', 'red', 'planet', 'eyes', 'mars', 'rovers', 'mars', 'missions', ',', 'providing', 'unique', 'perspective', 'entire', 'planet', '.']\n",
            "    After Regex: ['users', 'also', 'explore', 'red', 'planet', 'eyes', 'mars', 'rovers', 'mars', 'missions', 'providing', 'unique', 'perspective', 'entire', 'planet']\n",
            "    After Lemmatization: ['user', 'also', 'explore', 'red', 'planet', 'eye', 'mar', 'rover', 'mar', 'mission', 'providing', 'unique', 'perspective', 'entire', 'planet']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9073\n",
            "    After Stopword Removal: ['learned', 'around', 'thousand', 'wild', 'elephants', 'lived', 'nearby', 'forests', 'regularly', 'seen', 'amongst', 'tea', 'come', 'search', 'food', ',', 'closest', 'got', 'seeing', 'dung', 'flattened', 'grass', 'though', '.']\n",
            "    After Regex: ['learned', 'around', 'thousand', 'wild', 'elephants', 'lived', 'nearby', 'forests', 'regularly', 'seen', 'amongst', 'tea', 'come', 'search', 'food', 'closest', 'got', 'seeing', 'dung', 'flattened', 'grass', 'though']\n",
            "    After Lemmatization: ['learned', 'around', 'thousand', 'wild', 'elephant', 'lived', 'nearby', 'forest', 'regularly', 'seen', 'amongst', 'tea', 'come', 'search', 'food', 'closest', 'got', 'seeing', 'dung', 'flattened', 'grass', 'though']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9074\n",
            "    After Stopword Removal: ['god', 'tells', 'moses', \"'\", 'send', 'quail', 'israel', 'eat', ';', 'meal', ',', 'week', \"'\", 'meals', ',', 'month', ',', 'comes', 'noses', 'loathe', '!']\n",
            "    After Regex: ['god', 'tells', 'moses', 'send', 'quail', 'israel', 'eat', 'meal', 'week', 'meals', 'month', 'comes', 'noses', 'loathe']\n",
            "    After Lemmatization: ['god', 'tell', 'moses', 'send', 'quail', 'israel', 'eat', 'meal', 'week', 'meal', 'month', 'come', 'nose', 'loathe']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9075\n",
            "    After Stopword Removal: ['harshest', ',', 'hostile', 'environment', 'earth', '.']\n",
            "    After Regex: ['harshest', 'hostile', 'environment', 'earth']\n",
            "    After Lemmatization: ['harshest', 'hostile', 'environment', 'earth']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9076\n",
            "    After Stopword Removal: ['vibrating', 'tuning', 'fork', 'placed', 'middle', 'forehead', 'help', 'diagnose', 'one', '-', 'sided', 'hearing', 'loss', '.']\n",
            "    After Regex: ['vibrating', 'tuning', 'fork', 'placed', 'middle', 'forehead', 'help', 'diagnose', 'one', 'sided', 'hearing', 'loss']\n",
            "    After Lemmatization: ['vibrating', 'tuning', 'fork', 'placed', 'middle', 'forehead', 'help', 'diagnose', 'one', 'sided', 'hearing', 'loss']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9077\n",
            "    After Stopword Removal: ['show', 'class', 'written', 'l', '2', 'word', 'matches', 'one', '4', 'pictures', '.']\n",
            "    After Regex: ['show', 'class', 'written', 'l', 'word', 'matches', 'one', 'pictures']\n",
            "    After Lemmatization: ['show', 'class', 'written', 'l', 'word', 'match', 'one', 'picture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9078\n",
            "    After Stopword Removal: ['copyright', 'infringement', 'theft', 'quite', 'distinct', 'definitions', '.']\n",
            "    After Regex: ['copyright', 'infringement', 'theft', 'quite', 'distinct', 'definitions']\n",
            "    After Lemmatization: ['copyright', 'infringement', 'theft', 'quite', 'distinct', 'definition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9079\n",
            "    After Stopword Removal: ['impact', 'brain', 'skull', 'temporarily', 'changes', 'cells', 'brain', 'function', '.']\n",
            "    After Regex: ['impact', 'brain', 'skull', 'temporarily', 'changes', 'cells', 'brain', 'function']\n",
            "    After Lemmatization: ['impact', 'brain', 'skull', 'temporarily', 'change', 'cell', 'brain', 'function']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9080\n",
            "    After Stopword Removal: ['overpopulated', 'crappie', 'usually', \"'\", 'get', 'bigger', '3', '4', 'inches', '.']\n",
            "    After Regex: ['overpopulated', 'crappie', 'usually', 'get', 'bigger', 'inches']\n",
            "    After Lemmatization: ['overpopulated', 'crappie', 'usually', 'get', 'bigger', 'inch']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9081\n",
            "    After Stopword Removal: ['jaroslav', 'pelikan', 'lists', 'several', 'meanings', 'greek', 'logos', ':', 'mind', ',', 'power', ',', 'deed', ',', 'reason', ',', 'structure', ',', 'purpose', '(58).']\n",
            "    After Regex: ['jaroslav', 'pelikan', 'lists', 'several', 'meanings', 'greek', 'logos', 'mind', 'power', 'deed', 'reason', 'structure', 'purpose']\n",
            "    After Lemmatization: ['jaroslav', 'pelikan', 'list', 'several', 'meaning', 'greek', 'logo', 'mind', 'power', 'deed', 'reason', 'structure', 'purpose']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9082\n",
            "    After Stopword Removal: ['although', 'benefits', 'work', 'schedules', 'provided', 'advantages', 'public', 'employers', ',', 'heightened', 'demand', 'workers', 'increasingly', 'closed', 'gap', 'private', 'employers', 'enriched', 'benefits', 'developed', 'flexible', 'scheduling', '.']\n",
            "    After Regex: ['although', 'benefits', 'work', 'schedules', 'provided', 'advantages', 'public', 'employers', 'heightened', 'demand', 'workers', 'increasingly', 'closed', 'gap', 'private', 'employers', 'enriched', 'benefits', 'developed', 'flexible', 'scheduling']\n",
            "    After Lemmatization: ['although', 'benefit', 'work', 'schedule', 'provided', 'advantage', 'public', 'employer', 'heightened', 'demand', 'worker', 'increasingly', 'closed', 'gap', 'private', 'employer', 'enriched', 'benefit', 'developed', 'flexible', 'scheduling']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9083\n",
            "    After Stopword Removal: ['sea', 'bottom', 'area', 'undulating', 'sand', 'plain', ',', 'zelitsky', 'said', '.']\n",
            "    After Regex: ['sea', 'bottom', 'area', 'undulating', 'sand', 'plain', 'zelitsky', 'said']\n",
            "    After Lemmatization: ['sea', 'bottom', 'area', 'undulating', 'sand', 'plain', 'zelitsky', 'said']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9084\n",
            "    After Stopword Removal: ['film', 'composite', 'film', 'comprised', 'membrane', '1', 'coating', '2.']\n",
            "    After Regex: ['film', 'composite', 'film', 'comprised', 'membrane', 'coating']\n",
            "    After Lemmatization: ['film', 'composite', 'film', 'comprised', 'membrane', 'coating']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9085\n",
            "    After Stopword Removal: ['two', 'studies', 'four', 'rcts', 'high', 'annualized', 'risk', 'estimates', ',', 'included', 'populations', 'many', 'risk', 'factors', 'dm', '.']\n",
            "    After Regex: ['two', 'studies', 'four', 'rcts', 'high', 'annualized', 'risk', 'estimates', 'included', 'populations', 'many', 'risk', 'factors', 'dm']\n",
            "    After Lemmatization: ['two', 'study', 'four', 'rcts', 'high', 'annualized', 'risk', 'estimate', 'included', 'population', 'many', 'risk', 'factor', 'dm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9086\n",
            "    After Stopword Removal: ['whichever', 'type', 'vegetables', 'choose', 'plant', ',', 'sure', 'look', 'directions', 'seed', 'packet', 'plant', 'tag', '.']\n",
            "    After Regex: ['whichever', 'type', 'vegetables', 'choose', 'plant', 'sure', 'look', 'directions', 'seed', 'packet', 'plant', 'tag']\n",
            "    After Lemmatization: ['whichever', 'type', 'vegetable', 'choose', 'plant', 'sure', 'look', 'direction', 'seed', 'packet', 'plant', 'tag']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9087\n",
            "    After Stopword Removal: ['early', 'december', 'park', 'service', 'officials', 'removed', 'documents', 'artifacts', 'ellis', 'island', 'avoid', 'possible', 'damage', 'resulting', 'continued', 'lack', 'power', 'preservation', ',', 'digital', 'first', 'media', 'reported', '.']\n",
            "    After Regex: ['early', 'december', 'park', 'service', 'officials', 'removed', 'documents', 'artifacts', 'ellis', 'island', 'avoid', 'possible', 'damage', 'resulting', 'continued', 'lack', 'power', 'preservation', 'digital', 'first', 'media', 'reported']\n",
            "    After Lemmatization: ['early', 'december', 'park', 'service', 'official', 'removed', 'document', 'artifact', 'elli', 'island', 'avoid', 'possible', 'damage', 'resulting', 'continued', 'lack', 'power', 'preservation', 'digital', 'first', 'medium', 'reported']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9088\n",
            "    After Stopword Removal: ['begun', 'citizens', 'fed', 'frustrated', 'improvements', 'statute', 'autonomy', '2006', 'court', 'ruling', 'watered', 'charter', '2010,', 'catalan', 'independence', 'movement', 'model', 'democratic', ',', 'non', '-', 'violent', ',', 'creative', 'even', 'cheerful', 'activism', ',', 'evidenced', 'massive', 'demonstrations', ',', 'candle', 'lightings', ',', 'bike', 'rides', ',', 'conferences', ',', 'debates', ',', 'videos', ',', 'books', ',', 'balloons', ',', 'much', ',', 'mobilising', 'huge', 'numbers', 'population', '.']\n",
            "    After Regex: ['begun', 'citizens', 'fed', 'frustrated', 'improvements', 'statute', 'autonomy', 'court', 'ruling', 'watered', 'charter', 'catalan', 'independence', 'movement', 'model', 'democratic', 'non', 'violent', 'creative', 'even', 'cheerful', 'activism', 'evidenced', 'massive', 'demonstrations', 'candle', 'lightings', 'bike', 'rides', 'conferences', 'debates', 'videos', 'books', 'balloons', 'much', 'mobilising', 'huge', 'numbers', 'population']\n",
            "    After Lemmatization: ['begun', 'citizen', 'fed', 'frustrated', 'improvement', 'statute', 'autonomy', 'court', 'ruling', 'watered', 'charter', 'catalan', 'independence', 'movement', 'model', 'democratic', 'non', 'violent', 'creative', 'even', 'cheerful', 'activism', 'evidenced', 'massive', 'demonstration', 'candle', 'lighting', 'bike', 'ride', 'conference', 'debate', 'video', 'book', 'balloon', 'much', 'mobilising', 'huge', 'number', 'population']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9089\n",
            "    After Stopword Removal: ['creation', 'torah', 'scroll', 'jewish', 'scribe', 'handcrafted', 'copy', 'process', ',', 'executed', 'highest', 'precision', 'possible', '.']\n",
            "    After Regex: ['creation', 'torah', 'scroll', 'jewish', 'scribe', 'handcrafted', 'copy', 'process', 'executed', 'highest', 'precision', 'possible']\n",
            "    After Lemmatization: ['creation', 'torah', 'scroll', 'jewish', 'scribe', 'handcrafted', 'copy', 'process', 'executed', 'highest', 'precision', 'possible']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9090\n",
            "    After Stopword Removal: ['fair', 'feds', '(', 'unlike', 'others', ')', 'large', 'within', '-', 'institution', 'variance', 'quality', ',', 'mostly', 'terms', 'colleagues', 'quantity', '/', 'quality', 'policy', 'work', '.']\n",
            "    After Regex: ['fair', 'feds', 'unlike', 'others', 'large', 'within', 'institution', 'variance', 'quality', 'mostly', 'terms', 'colleagues', 'quantity', 'quality', 'policy', 'work']\n",
            "    After Lemmatization: ['fair', 'fed', 'unlike', 'others', 'large', 'within', 'institution', 'variance', 'quality', 'mostly', 'term', 'colleague', 'quantity', 'quality', 'policy', 'work']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9091\n",
            "    After Stopword Removal: ['fundamental', 'ambiguity', 'missionary', 'discourse', ',', 'course', ',', 'attempt', 'communicate', 'truths', 'across', 'idioms', ',', 'introduce', 'message', 'originated', 'another', 'historical', 'moment', 'present', '.']\n",
            "    After Regex: ['fundamental', 'ambiguity', 'missionary', 'discourse', 'course', 'attempt', 'communicate', 'truths', 'across', 'idioms', 'introduce', 'message', 'originated', 'another', 'historical', 'moment', 'present']\n",
            "    After Lemmatization: ['fundamental', 'ambiguity', 'missionary', 'discourse', 'course', 'attempt', 'communicate', 'truth', 'across', 'idiom', 'introduce', 'message', 'originated', 'another', 'historical', 'moment', 'present']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9092\n",
            "    After Stopword Removal: ['select', 'committee', 'infrastructure', 'trying', 'find', 'ways', 'save', 'money', 'increase', 'efficiencies', 'combining', 'division', 'highways', 'governing', 'authority', 'state', \"'\", 'turnpike', ',', 'simply', 'combining', 'two', 'would', 'create', 'serious', 'legal', 'implications', 'state', '.']\n",
            "    After Regex: ['select', 'committee', 'infrastructure', 'trying', 'find', 'ways', 'save', 'money', 'increase', 'efficiencies', 'combining', 'division', 'highways', 'governing', 'authority', 'state', 'turnpike', 'simply', 'combining', 'two', 'would', 'create', 'serious', 'legal', 'implications', 'state']\n",
            "    After Lemmatization: ['select', 'committee', 'infrastructure', 'trying', 'find', 'way', 'save', 'money', 'increase', 'efficiency', 'combining', 'division', 'highway', 'governing', 'authority', 'state', 'turnpike', 'simply', 'combining', 'two', 'would', 'create', 'serious', 'legal', 'implication', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9093\n",
            "    After Stopword Removal: ['four', '80-', 'foot', '-', 'wide', 'standard', 'lots', 'facing', 'would', 'need', '4,800', 'square', 'feet', '(160', 'x', '30)', 'street', 'paving', '.']\n",
            "    After Regex: ['four', 'foot', 'wide', 'standard', 'lots', 'facing', 'would', 'need', 'square', 'feet', 'x', 'street', 'paving']\n",
            "    After Lemmatization: ['four', 'foot', 'wide', 'standard', 'lot', 'facing', 'would', 'need', 'square', 'foot', 'x', 'street', 'paving']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9094\n",
            "    After Stopword Removal: ['london', 'transport', 'introduced', 'routemaster', 'bus', '1956.']\n",
            "    After Regex: ['london', 'transport', 'introduced', 'routemaster', 'bus']\n",
            "    After Lemmatization: ['london', 'transport', 'introduced', 'routemaster', 'bus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9095\n",
            "    After Stopword Removal: ['main', 'reason', 'india', 'still', 'heavily', 'dependent', 'imports', 'compared', 'several', 'major', 'economies', '.']\n",
            "    After Regex: ['main', 'reason', 'india', 'still', 'heavily', 'dependent', 'imports', 'compared', 'several', 'major', 'economies']\n",
            "    After Lemmatization: ['main', 'reason', 'india', 'still', 'heavily', 'dependent', 'import', 'compared', 'several', 'major', 'economy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9096\n",
            "    After Stopword Removal: ['interestingly', 'divide', 'year', 'according', 'earths', 'travel', 'one', 'turn', 'around', 'sun', 'day', 'according', 'earth', 'spinning', 'one', 'revolution', ',', 'organising', 'days', 'seven', 'calling', 'weeks', 'natural', 'connection', '.']\n",
            "    After Regex: ['interestingly', 'divide', 'year', 'according', 'earths', 'travel', 'one', 'turn', 'around', 'sun', 'day', 'according', 'earth', 'spinning', 'one', 'revolution', 'organising', 'days', 'seven', 'calling', 'weeks', 'natural', 'connection']\n",
            "    After Lemmatization: ['interestingly', 'divide', 'year', 'according', 'earth', 'travel', 'one', 'turn', 'around', 'sun', 'day', 'according', 'earth', 'spinning', 'one', 'revolution', 'organising', 'day', 'seven', 'calling', 'week', 'natural', 'connection']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9097\n",
            "    After Stopword Removal: ['hong', 'kong', 'broadband', 'network', 'leader', 'fibre', 'investment', '15', 'years', ',', 'prompted', 'telcos', 'also', 'invest', 'make', 'hong', 'kong', 'one', 'best', 'connected', 'fibre', 'cities', 'world', '.']\n",
            "    After Regex: ['hong', 'kong', 'broadband', 'network', 'leader', 'fibre', 'investment', 'years', 'prompted', 'telcos', 'also', 'invest', 'make', 'hong', 'kong', 'one', 'best', 'connected', 'fibre', 'cities', 'world']\n",
            "    After Lemmatization: ['hong', 'kong', 'broadband', 'network', 'leader', 'fibre', 'investment', 'year', 'prompted', 'telco', 'also', 'invest', 'make', 'hong', 'kong', 'one', 'best', 'connected', 'fibre', 'city', 'world']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9098\n",
            "    After Stopword Removal: ['generosity', 'cash', ',', '-', 'kind', ',', 'volunteers', 'awareness', 'support', 'special', 'olympics', 'programs', 'events', 'around', 'world', ',', 'coke', 'fostered', 'acceptance', 'inclusion', 'millions', 'special', 'olympics', 'athletes', '.']\n",
            "    After Regex: ['generosity', 'cash', 'kind', 'volunteers', 'awareness', 'support', 'special', 'olympics', 'programs', 'events', 'around', 'world', 'coke', 'fostered', 'acceptance', 'inclusion', 'millions', 'special', 'olympics', 'athletes']\n",
            "    After Lemmatization: ['generosity', 'cash', 'kind', 'volunteer', 'awareness', 'support', 'special', 'olympics', 'program', 'event', 'around', 'world', 'coke', 'fostered', 'acceptance', 'inclusion', 'million', 'special', 'olympics', 'athlete']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9099\n",
            "    After Stopword Removal: ['cnsnews', 'says', 'health', 'human', 'services', 'secretary', ',', 'kathleen', 'sebelius', 'spoke', 'department', 'justice', 'summit', 'april', '2,2012', 'washington', ',', '.', 'c', '.', 'youth', 'violence', \"'\", 'chronic', 'health', 'issue', \"'\", 'united', 'states', '.']\n",
            "    After Regex: ['cnsnews', 'says', 'health', 'human', 'services', 'secretary', 'kathleen', 'sebelius', 'spoke', 'department', 'justice', 'summit', 'april', 'washington', 'c', 'youth', 'violence', 'chronic', 'health', 'issue', 'united', 'states']\n",
            "    After Lemmatization: ['cnsnews', 'say', 'health', 'human', 'service', 'secretary', 'kathleen', 'sebelius', 'spoke', 'department', 'justice', 'summit', 'april', 'washington', 'c', 'youth', 'violence', 'chronic', 'health', 'issue', 'united', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9100\n",
            "    After Stopword Removal: ['researcher', 'consultant', 'climate', ',', 'disasters', 'development', 'worked', 'thirty', 'organizations', '50', 'countries', '.']\n",
            "    After Regex: ['researcher', 'consultant', 'climate', 'disasters', 'development', 'worked', 'thirty', 'organizations', 'countries']\n",
            "    After Lemmatization: ['researcher', 'consultant', 'climate', 'disaster', 'development', 'worked', 'thirty', 'organization', 'country']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9101\n",
            "    After Stopword Removal: ['even', 'clay', 'pots', 'allowed', 'water', 'left', '.']\n",
            "    After Regex: ['even', 'clay', 'pots', 'allowed', 'water', 'left']\n",
            "    After Lemmatization: ['even', 'clay', 'pot', 'allowed', 'water', 'left']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9102\n",
            "    After Stopword Removal: ['devoted', 'enthusiastic', 'art', 'museum', 'lovers', ',', 'members', 'president', \"'\", 'circle', 'continue', 'custom', 'primary', 'montreal', 'museum', 'fine', 'arts', 'founders', ',', '1860,', 'believed', 'importance', 'creating', 'museum', 'international', 'calibre', 'metropolis', '.']\n",
            "    After Regex: ['devoted', 'enthusiastic', 'art', 'museum', 'lovers', 'members', 'president', 'circle', 'continue', 'custom', 'primary', 'montreal', 'museum', 'fine', 'arts', 'founders', 'believed', 'importance', 'creating', 'museum', 'international', 'calibre', 'metropolis']\n",
            "    After Lemmatization: ['devoted', 'enthusiastic', 'art', 'museum', 'lover', 'member', 'president', 'circle', 'continue', 'custom', 'primary', 'montreal', 'museum', 'fine', 'art', 'founder', 'believed', 'importance', 'creating', 'museum', 'international', 'calibre', 'metropolis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9103\n",
            "    After Stopword Removal: ['finally', ',', 'made', 'orally', 'exceptional', 'circumstances', 'justify', 'imminent', 'death', ',', 'epidemic', 'war', '(', 'article', '506).']\n",
            "    After Regex: ['finally', 'made', 'orally', 'exceptional', 'circumstances', 'justify', 'imminent', 'death', 'epidemic', 'war', 'article']\n",
            "    After Lemmatization: ['finally', 'made', 'orally', 'exceptional', 'circumstance', 'justify', 'imminent', 'death', 'epidemic', 'war', 'article']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9104\n",
            "    After Stopword Removal: [\"'\", 'average', 'united', 'states', '.']\n",
            "    After Regex: ['average', 'united', 'states']\n",
            "    After Lemmatization: ['average', 'united', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9105\n",
            "    After Stopword Removal: ['tax', 'cuts', 'jobs', 'act', 'amended', 'section', '118', 'internal', 'revenue', 'code', 'upon', 'exclusion', 'based', '.']\n",
            "    After Regex: ['tax', 'cuts', 'jobs', 'act', 'amended', 'section', 'internal', 'revenue', 'code', 'upon', 'exclusion', 'based']\n",
            "    After Lemmatization: ['tax', 'cut', 'job', 'act', 'amended', 'section', 'internal', 'revenue', 'code', 'upon', 'exclusion', 'based']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9106\n",
            "    After Stopword Removal: ['four', 'bric', 'countries', 'registered', 'lower', 'rates', 'growth', '2011', 'owing', 'declining', 'demand', 'europe', ',', 'particular', ',', 'dwindling', 'domestic', 'demand', 'economic', 'policy', 'aimed', 'stemming', 'inflation', 'asia', 'latin', 'america', '.']\n",
            "    After Regex: ['four', 'bric', 'countries', 'registered', 'lower', 'rates', 'growth', 'owing', 'declining', 'demand', 'europe', 'particular', 'dwindling', 'domestic', 'demand', 'economic', 'policy', 'aimed', 'stemming', 'inflation', 'asia', 'latin', 'america']\n",
            "    After Lemmatization: ['four', 'bric', 'country', 'registered', 'lower', 'rate', 'growth', 'owing', 'declining', 'demand', 'europe', 'particular', 'dwindling', 'domestic', 'demand', 'economic', 'policy', 'aimed', 'stemming', 'inflation', 'asia', 'latin', 'america']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9107\n",
            "    After Stopword Removal: ['cam', 'neely', 'foundation', 'cancer', 'care', ',', 'neely', 'house', ',', 'neely', 'cancer', 'fund', 'exist', 'one', 'reason', '-', 'help', 'cancer', 'patients', 'families', 'treatment', '.']\n",
            "    After Regex: ['cam', 'neely', 'foundation', 'cancer', 'care', 'neely', 'house', 'neely', 'cancer', 'fund', 'exist', 'one', 'reason', 'help', 'cancer', 'patients', 'families', 'treatment']\n",
            "    After Lemmatization: ['cam', 'neely', 'foundation', 'cancer', 'care', 'neely', 'house', 'neely', 'cancer', 'fund', 'exist', 'one', 'reason', 'help', 'cancer', 'patient', 'family', 'treatment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9108\n",
            "    After Stopword Removal: ['sustiva', 'truvada', 'widely', 'prescribed', 'antiretroviral', 'treatment', 'regimen', 'u', '.', '.,', 'physicians', 'securities', 'analysts', 'predict', 'people', 'currently', 'taking', 'drugs', 'separately', 'switch', 'new', 'combination', 'pill', '(', 'kaiser', 'daily', 'hiv', '/', 'aids', 'report', ',7/10).']\n",
            "    After Regex: ['sustiva', 'truvada', 'widely', 'prescribed', 'antiretroviral', 'treatment', 'regimen', 'u', 'physicians', 'securities', 'analysts', 'predict', 'people', 'currently', 'taking', 'drugs', 'separately', 'switch', 'new', 'combination', 'pill', 'kaiser', 'daily', 'hiv', 'aids', 'report']\n",
            "    After Lemmatization: ['sustiva', 'truvada', 'widely', 'prescribed', 'antiretroviral', 'treatment', 'regimen', 'u', 'physician', 'security', 'analyst', 'predict', 'people', 'currently', 'taking', 'drug', 'separately', 'switch', 'new', 'combination', 'pill', 'kaiser', 'daily', 'hiv', 'aid', 'report']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9109\n",
            "    After Stopword Removal: ['results', 'clinical', 'trials', 'many', 'compounds', 'believed', 'inhibit', 'activity', 'dpp', '-4', 'tested', 'humans', '.']\n",
            "    After Regex: ['results', 'clinical', 'trials', 'many', 'compounds', 'believed', 'inhibit', 'activity', 'dpp', 'tested', 'humans']\n",
            "    After Lemmatization: ['result', 'clinical', 'trial', 'many', 'compound', 'believed', 'inhibit', 'activity', 'dpp', 'tested', 'human']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9110\n",
            "    After Stopword Removal: ['may', 'lead', 'c', '-', 'section', 'cause', 'fetal', 'distress', '.']\n",
            "    After Regex: ['may', 'lead', 'c', 'section', 'cause', 'fetal', 'distress']\n",
            "    After Lemmatization: ['may', 'lead', 'c', 'section', 'cause', 'fetal', 'distress']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9111\n",
            "    After Stopword Removal: ['depth', 'knowledge', 'tax', 'exempt', 'bond', 'tax', 'credit', 'areas', 'allows', 'us', 'add', 'tremendous', 'value', 'economic', 'development', 'projects', '.']\n",
            "    After Regex: ['depth', 'knowledge', 'tax', 'exempt', 'bond', 'tax', 'credit', 'areas', 'allows', 'us', 'add', 'tremendous', 'value', 'economic', 'development', 'projects']\n",
            "    After Lemmatization: ['depth', 'knowledge', 'tax', 'exempt', 'bond', 'tax', 'credit', 'area', 'allows', 'u', 'add', 'tremendous', 'value', 'economic', 'development', 'project']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9112\n",
            "    After Stopword Removal: ['explains', 'rejection', 'church', 'state', ',', 'office', 'sacrament', ',', 'oath', 'war', 'many', 'things', 'common', '.']\n",
            "    After Regex: ['explains', 'rejection', 'church', 'state', 'office', 'sacrament', 'oath', 'war', 'many', 'things', 'common']\n",
            "    After Lemmatization: ['explains', 'rejection', 'church', 'state', 'office', 'sacrament', 'oath', 'war', 'many', 'thing', 'common']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9113\n",
            "    After Stopword Removal: ['comparison', 'mitral', 'annulus', 'geometry', 'patients', 'ischemic', 'non', '-', 'ischemic', 'functional', 'mitral', 'regurgita', '-', 'tion', '.']\n",
            "    After Regex: ['comparison', 'mitral', 'annulus', 'geometry', 'patients', 'ischemic', 'non', 'ischemic', 'functional', 'mitral', 'regurgita', 'tion']\n",
            "    After Lemmatization: ['comparison', 'mitral', 'annulus', 'geometry', 'patient', 'ischemic', 'non', 'ischemic', 'functional', 'mitral', 'regurgita', 'tion']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9114\n",
            "    After Stopword Removal: ['programme', 'throws', 'surprising', 'new', 'information', ':', 'like', 'sleeping', 'well', 'keep', 'alzheimer', \"'\", 'away', 'taking', 'new', 'language', 'course', 'possibly', 'beneficial', 'drug', '.']\n",
            "    After Regex: ['programme', 'throws', 'surprising', 'new', 'information', 'like', 'sleeping', 'well', 'keep', 'alzheimer', 'away', 'taking', 'new', 'language', 'course', 'possibly', 'beneficial', 'drug']\n",
            "    After Lemmatization: ['programme', 'throw', 'surprising', 'new', 'information', 'like', 'sleeping', 'well', 'keep', 'alzheimer', 'away', 'taking', 'new', 'language', 'course', 'possibly', 'beneficial', 'drug']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9115\n",
            "    After Stopword Removal: ['canadian', 'animal', 'assistance', 'team', '(', 'caat', ')', 'group', 'volunteers', 'travel', 'communities', 'spay', ',', 'neuter', 'give', 'vaccines', 'dogs', 'cats', '.']\n",
            "    After Regex: ['canadian', 'animal', 'assistance', 'team', 'caat', 'group', 'volunteers', 'travel', 'communities', 'spay', 'neuter', 'give', 'vaccines', 'dogs', 'cats']\n",
            "    After Lemmatization: ['canadian', 'animal', 'assistance', 'team', 'caat', 'group', 'volunteer', 'travel', 'community', 'spay', 'neuter', 'give', 'vaccine', 'dog', 'cat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9116\n",
            "    After Stopword Removal: ['sixth', 'day', 'creation', '(', 'friday', ')', 'day', 'mankind', 'created', '.']\n",
            "    After Regex: ['sixth', 'day', 'creation', 'friday', 'day', 'mankind', 'created']\n",
            "    After Lemmatization: ['sixth', 'day', 'creation', 'friday', 'day', 'mankind', 'created']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9117\n",
            "    After Stopword Removal: ['something', 'ruined', 'water', 'getting', '?']\n",
            "    After Regex: ['something', 'ruined', 'water', 'getting']\n",
            "    After Lemmatization: ['something', 'ruined', 'water', 'getting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9118\n",
            "    After Stopword Removal: ['team', 'increased', 'size', 'building', 'decreased', 'area', 'windows', 'order', 'reduce', 'solar', 'gain', ';', 'also', 'placed', 'strategically', 'within', 'structure', '.']\n",
            "    After Regex: ['team', 'increased', 'size', 'building', 'decreased', 'area', 'windows', 'order', 'reduce', 'solar', 'gain', 'also', 'placed', 'strategically', 'within', 'structure']\n",
            "    After Lemmatization: ['team', 'increased', 'size', 'building', 'decreased', 'area', 'window', 'order', 'reduce', 'solar', 'gain', 'also', 'placed', 'strategically', 'within', 'structure']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9119\n",
            "    After Stopword Removal: ['bill', 'could', 'lead', 'serious', 'violations', 'parents', \"'\", 'rights', 'protect', 'seriously', 'ill', 'children', ',', 'since', 'doctor', 'issue', '-', '-', 'resuscitate', 'order', 'even', 'parent', \"'\", 'objection', '.']\n",
            "    After Regex: ['bill', 'could', 'lead', 'serious', 'violations', 'parents', 'rights', 'protect', 'seriously', 'ill', 'children', 'since', 'doctor', 'issue', 'resuscitate', 'order', 'even', 'parent', 'objection']\n",
            "    After Lemmatization: ['bill', 'could', 'lead', 'serious', 'violation', 'parent', 'right', 'protect', 'seriously', 'ill', 'child', 'since', 'doctor', 'issue', 'resuscitate', 'order', 'even', 'parent', 'objection']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9120\n",
            "    After Stopword Removal: ['–', 'safeguarding', 'money', 'savings', 'may', 'little', 'bit', 'problem', ',', 'therefore', 'take', 'adequate', 'steps', 'plan', 'effectively', '.']\n",
            "    After Regex: ['safeguarding', 'money', 'savings', 'may', 'little', 'bit', 'problem', 'therefore', 'take', 'adequate', 'steps', 'plan', 'effectively']\n",
            "    After Lemmatization: ['safeguarding', 'money', 'saving', 'may', 'little', 'bit', 'problem', 'therefore', 'take', 'adequate', 'step', 'plan', 'effectively']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9121\n",
            "    After Stopword Removal: ['decided', 'use', 'chapel', 'site', 'carolyn', \"'\", 'initiation', 'tradition', ':', 'familiarity', 'would', 'serve', 'anchor', '.']\n",
            "    After Regex: ['decided', 'use', 'chapel', 'site', 'carolyn', 'initiation', 'tradition', 'familiarity', 'would', 'serve', 'anchor']\n",
            "    After Lemmatization: ['decided', 'use', 'chapel', 'site', 'carolyn', 'initiation', 'tradition', 'familiarity', 'would', 'serve', 'anchor']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9122\n",
            "    After Stopword Removal: ['propose', 'integration', 'high', '-', 'resolution', 'imaging', 'sonar', 'seabed', 'auv', \"'\", 'suite', 'sensors', 'enable', 'sizing', 'demersal', 'fishes', 'address', 'important', 'issue', 'low', 'ambient', 'light', 'levels', 'affecting', 'video', 'data', 'collection', 'deep', '-', 'water', 'surveys', 'nighttime', 'deployments', '.']\n",
            "    After Regex: ['propose', 'integration', 'high', 'resolution', 'imaging', 'sonar', 'seabed', 'auv', 'suite', 'sensors', 'enable', 'sizing', 'demersal', 'fishes', 'address', 'important', 'issue', 'low', 'ambient', 'light', 'levels', 'affecting', 'video', 'data', 'collection', 'deep', 'water', 'surveys', 'nighttime', 'deployments']\n",
            "    After Lemmatization: ['propose', 'integration', 'high', 'resolution', 'imaging', 'sonar', 'seabed', 'auv', 'suite', 'sensor', 'enable', 'sizing', 'demersal', 'fish', 'address', 'important', 'issue', 'low', 'ambient', 'light', 'level', 'affecting', 'video', 'data', 'collection', 'deep', 'water', 'survey', 'nighttime', 'deployment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9123\n",
            "    After Stopword Removal: ['one', 'kind', 'rehab', '?']\n",
            "    After Regex: ['one', 'kind', 'rehab']\n",
            "    After Lemmatization: ['one', 'kind', 'rehab']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9124\n",
            "    After Stopword Removal: ['kumar', 'pointed', 'took', 'charge', 'chief', 'minister', '2005,', '45', 'grid', 'sub', '-', 'stations', 'bihar', ',', 'network', 'provide', 'electricity', '.']\n",
            "    After Regex: ['kumar', 'pointed', 'took', 'charge', 'chief', 'minister', 'grid', 'sub', 'stations', 'bihar', 'network', 'provide', 'electricity']\n",
            "    After Lemmatization: ['kumar', 'pointed', 'took', 'charge', 'chief', 'minister', 'grid', 'sub', 'station', 'bihar', 'network', 'provide', 'electricity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9125\n",
            "    After Stopword Removal: ['old', 'testament', 'must', 'interpreted', 'christ', '-', 'centered', 'manner', '.']\n",
            "    After Regex: ['old', 'testament', 'must', 'interpreted', 'christ', 'centered', 'manner']\n",
            "    After Lemmatization: ['old', 'testament', 'must', 'interpreted', 'christ', 'centered', 'manner']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9126\n",
            "    After Stopword Removal: ['best', 'way', '?']\n",
            "    After Regex: ['best', 'way']\n",
            "    After Lemmatization: ['best', 'way']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9127\n",
            "    After Stopword Removal: ['disadvantages', 'parents', 'risk', 'losing', 'true', 'independence', 'judgment', ',', 'parents', 'limited', 'school', 'district', \"'\", 'approved', 'list', 'evaluators', '.']\n",
            "    After Regex: ['disadvantages', 'parents', 'risk', 'losing', 'true', 'independence', 'judgment', 'parents', 'limited', 'school', 'district', 'approved', 'list', 'evaluators']\n",
            "    After Lemmatization: ['disadvantage', 'parent', 'risk', 'losing', 'true', 'independence', 'judgment', 'parent', 'limited', 'school', 'district', 'approved', 'list', 'evaluator']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9128\n",
            "    After Stopword Removal: ['sampling', 'done', 'next', 'year', 'purposes', 'house', 'apportionment', ',', 'could', 'result', 'two', 'versions', 'nation', \"'\", '2000', 'population', ':', 'one', 'allocation', 'seats', 'house', ',', 'second', 'everything', 'else', '.']\n",
            "    After Regex: ['sampling', 'done', 'next', 'year', 'purposes', 'house', 'apportionment', 'could', 'result', 'two', 'versions', 'nation', 'population', 'one', 'allocation', 'seats', 'house', 'second', 'everything', 'else']\n",
            "    After Lemmatization: ['sampling', 'done', 'next', 'year', 'purpose', 'house', 'apportionment', 'could', 'result', 'two', 'version', 'nation', 'population', 'one', 'allocation', 'seat', 'house', 'second', 'everything', 'else']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9129\n",
            "    After Stopword Removal: ['fact', ',', 'destitute', 'people', 'due', 'share', 'believers', \"'\", 'wealth', '(', 'qur', \"'\", ',51:19).']\n",
            "    After Regex: ['fact', 'destitute', 'people', 'due', 'share', 'believers', 'wealth', 'qur']\n",
            "    After Lemmatization: ['fact', 'destitute', 'people', 'due', 'share', 'believer', 'wealth', 'qur']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9130\n",
            "    After Stopword Removal: ['perl', 'array', 'functions', 'part', 'larger', 'online', 'lecture', 'perl', 'contains', 'lots', 'good', 'simple', 'examples', '.']\n",
            "    After Regex: ['perl', 'array', 'functions', 'part', 'larger', 'online', 'lecture', 'perl', 'contains', 'lots', 'good', 'simple', 'examples']\n",
            "    After Lemmatization: ['perl', 'array', 'function', 'part', 'larger', 'online', 'lecture', 'perl', 'contains', 'lot', 'good', 'simple', 'example']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9131\n",
            "    After Stopword Removal: [\"'\", 'well', 'use', 'podium', 'classrooms', 'encourage', 'student', 'idealism', ',', 'whatever', 'political', 'direction', ',', 'including', 'breaches', 'boundaries', \"'\", 'deemed', 'politically', 'possible', '.']\n",
            "    After Regex: ['well', 'use', 'podium', 'classrooms', 'encourage', 'student', 'idealism', 'whatever', 'political', 'direction', 'including', 'breaches', 'boundaries', 'deemed', 'politically', 'possible']\n",
            "    After Lemmatization: ['well', 'use', 'podium', 'classroom', 'encourage', 'student', 'idealism', 'whatever', 'political', 'direction', 'including', 'breach', 'boundary', 'deemed', 'politically', 'possible']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9132\n",
            "    After Stopword Removal: ['ideas', 'available', ':', 'chronicle', 'higher', 'learning', '.']\n",
            "    After Regex: ['ideas', 'available', 'chronicle', 'higher', 'learning']\n",
            "    After Lemmatization: ['idea', 'available', 'chronicle', 'higher', 'learning']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9133\n",
            "    After Stopword Removal: ['mccloud', 'river', 'unique', 'among', 'california', \"'\", 'larger', 'rivers', 'water', 'derives', 'springs', 'underground', 'lava', 'aquifers', 'rather', 'rainfall', 'snowfall', '.']\n",
            "    After Regex: ['mccloud', 'river', 'unique', 'among', 'california', 'larger', 'rivers', 'water', 'derives', 'springs', 'underground', 'lava', 'aquifers', 'rather', 'rainfall', 'snowfall']\n",
            "    After Lemmatization: ['mccloud', 'river', 'unique', 'among', 'california', 'larger', 'river', 'water', 'derives', 'spring', 'underground', 'lava', 'aquifer', 'rather', 'rainfall', 'snowfall']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9134\n",
            "    After Stopword Removal: ['write', 'want', 'go', 'college', 'essays', '?']\n",
            "    After Regex: ['write', 'want', 'go', 'college', 'essays']\n",
            "    After Lemmatization: ['write', 'want', 'go', 'college', 'essay']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9135\n",
            "    After Stopword Removal: ['scientific', 'terms', 'call', 'denaturing', 'protein', \"'\", 'molecular', 'structure', 'see', 'botox', 'wearing', 'quickly', '.']\n",
            "    After Regex: ['scientific', 'terms', 'call', 'denaturing', 'protein', 'molecular', 'structure', 'see', 'botox', 'wearing', 'quickly']\n",
            "    After Lemmatization: ['scientific', 'term', 'call', 'denaturing', 'protein', 'molecular', 'structure', 'see', 'botox', 'wearing', 'quickly']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9136\n",
            "    After Stopword Removal: ['holds', '8.55%', 'voting', 'power', 'international', 'development', 'association', ',', 'world', 'bank', \"'\", 'fund', 'poorest', 'countries', '.']\n",
            "    After Regex: ['holds', 'voting', 'power', 'international', 'development', 'association', 'world', 'bank', 'fund', 'poorest', 'countries']\n",
            "    After Lemmatization: ['hold', 'voting', 'power', 'international', 'development', 'association', 'world', 'bank', 'fund', 'poorest', 'country']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9137\n",
            "    After Stopword Removal: ['explore', 'develop', 'musical', 'ideas', 'based', 'sources', 'imagination', 'sounds', 'environment', '.']\n",
            "    After Regex: ['explore', 'develop', 'musical', 'ideas', 'based', 'sources', 'imagination', 'sounds', 'environment']\n",
            "    After Lemmatization: ['explore', 'develop', 'musical', 'idea', 'based', 'source', 'imagination', 'sound', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9138\n",
            "    After Stopword Removal: ['though', 'water', 'remained', 'plentiful', ',', 'vast', 'tropical', 'temperate', 'forests', 'still', 'stood', 'strangers', 'saw', ',', 'oceans', 'shimmered', 'fishes', 'never', 'met', 'strand', 'twine', ',', 'malthus', 'divined', 'trouble', 'brewing', '.']\n",
            "    After Regex: ['though', 'water', 'remained', 'plentiful', 'vast', 'tropical', 'temperate', 'forests', 'still', 'stood', 'strangers', 'saw', 'oceans', 'shimmered', 'fishes', 'never', 'met', 'strand', 'twine', 'malthus', 'divined', 'trouble', 'brewing']\n",
            "    After Lemmatization: ['though', 'water', 'remained', 'plentiful', 'vast', 'tropical', 'temperate', 'forest', 'still', 'stood', 'stranger', 'saw', 'ocean', 'shimmered', 'fish', 'never', 'met', 'strand', 'twine', 'malthus', 'divined', 'trouble', 'brewing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9139\n",
            "    After Stopword Removal: ['humans', 'also', 'part', 'sacrificial', 'offerings', 'major', 'pyramids', '.']\n",
            "    After Regex: ['humans', 'also', 'part', 'sacrificial', 'offerings', 'major', 'pyramids']\n",
            "    After Lemmatization: ['human', 'also', 'part', 'sacrificial', 'offering', 'major', 'pyramid']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9140\n",
            "    After Stopword Removal: ['contrary', 'friedman', \"'\", 'skepticism', 'forming', 'greek', 'government', ',', 'center', '-', 'left', 'arrangement', 'quickly', 'formed', '.']\n",
            "    After Regex: ['contrary', 'friedman', 'skepticism', 'forming', 'greek', 'government', 'center', 'left', 'arrangement', 'quickly', 'formed']\n",
            "    After Lemmatization: ['contrary', 'friedman', 'skepticism', 'forming', 'greek', 'government', 'center', 'left', 'arrangement', 'quickly', 'formed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9141\n",
            "    After Stopword Removal: ['markers', 'bone', 'formation', 'lower', 'calorie', 'restriction', 'group', 'remained', 'low', '-', 'calorie', 'diet', 'calorie', 'restriction', 'plus', 'exercise', 'group', '.']\n",
            "    After Regex: ['markers', 'bone', 'formation', 'lower', 'calorie', 'restriction', 'group', 'remained', 'low', 'calorie', 'diet', 'calorie', 'restriction', 'plus', 'exercise', 'group']\n",
            "    After Lemmatization: ['marker', 'bone', 'formation', 'lower', 'calorie', 'restriction', 'group', 'remained', 'low', 'calorie', 'diet', 'calorie', 'restriction', 'plus', 'exercise', 'group']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9142\n",
            "    After Stopword Removal: ['boaters', 'already', 'prohibited', 'transporting', 'exotic', 'species', 'visible', 'naked', 'eye', ',', 'adult', 'zebra', 'mussels', '.']\n",
            "    After Regex: ['boaters', 'already', 'prohibited', 'transporting', 'exotic', 'species', 'visible', 'naked', 'eye', 'adult', 'zebra', 'mussels']\n",
            "    After Lemmatization: ['boater', 'already', 'prohibited', 'transporting', 'exotic', 'specie', 'visible', 'naked', 'eye', 'adult', 'zebra', 'mussel']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9143\n",
            "    After Stopword Removal: ['insert', 'antennae', 'foam', ';', 'glue', 'antennae', ',', 'eyes', 'mouth', 'place', '.']\n",
            "    After Regex: ['insert', 'antennae', 'foam', 'glue', 'antennae', 'eyes', 'mouth', 'place']\n",
            "    After Lemmatization: ['insert', 'antenna', 'foam', 'glue', 'antenna', 'eye', 'mouth', 'place']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9144\n",
            "    After Stopword Removal: ['actually', ',', 'proper', 'term', 'couples', ',', 'couples', 'imply', 'child', '.']\n",
            "    After Regex: ['actually', 'proper', 'term', 'couples', 'couples', 'imply', 'child']\n",
            "    After Lemmatization: ['actually', 'proper', 'term', 'couple', 'couple', 'imply', 'child']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9145\n",
            "    After Stopword Removal: ['immune', 'systems', 'may', 'evolved', 'deal', 'radiation', 'upper', 'reaches', 'naturally', '-', 'occurring', 'levels', '(', 'example', 'life', 'naturally', 'radioactive', 'granite', 'bedrock', ').']\n",
            "    After Regex: ['immune', 'systems', 'may', 'evolved', 'deal', 'radiation', 'upper', 'reaches', 'naturally', 'occurring', 'levels', 'example', 'life', 'naturally', 'radioactive', 'granite', 'bedrock']\n",
            "    After Lemmatization: ['immune', 'system', 'may', 'evolved', 'deal', 'radiation', 'upper', 'reach', 'naturally', 'occurring', 'level', 'example', 'life', 'naturally', 'radioactive', 'granite', 'bedrock']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9146\n",
            "    After Stopword Removal: ['child', 'cleverer', 'teacher', 'allowed', 'take', 'class', 'teacher', 'stand', 'back', 'dunce', \"'\", 'hat', '.']\n",
            "    After Regex: ['child', 'cleverer', 'teacher', 'allowed', 'take', 'class', 'teacher', 'stand', 'back', 'dunce', 'hat']\n",
            "    After Lemmatization: ['child', 'cleverer', 'teacher', 'allowed', 'take', 'class', 'teacher', 'stand', 'back', 'dunce', 'hat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9147\n",
            "    After Stopword Removal: ['true', 'soap', 'defined', 'salt', 'formed', 'reaction', 'oil', 'fat', 'alkali', '—', 'old', 'ancient', 'process', '.']\n",
            "    After Regex: ['true', 'soap', 'defined', 'salt', 'formed', 'reaction', 'oil', 'fat', 'alkali', 'old', 'ancient', 'process']\n",
            "    After Lemmatization: ['true', 'soap', 'defined', 'salt', 'formed', 'reaction', 'oil', 'fat', 'alkali', 'old', 'ancient', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9148\n",
            "    After Stopword Removal: ['japan', 'country', '30', 'percent', 'population', '60,', '2050', '60', 'countries', ',', 'china', 'canada', 'albania', ',', 'boat', '.']\n",
            "    After Regex: ['japan', 'country', 'percent', 'population', 'countries', 'china', 'canada', 'albania', 'boat']\n",
            "    After Lemmatization: ['japan', 'country', 'percent', 'population', 'country', 'china', 'canada', 'albania', 'boat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9149\n",
            "    After Stopword Removal: ['system', 'uses', 'exhaust', 'heat', 'natural', '-', 'gas', '-', 'fired', 'combustion', 'turbine', 'heat', 'recovery', 'steam', 'generator', '.']\n",
            "    After Regex: ['system', 'uses', 'exhaust', 'heat', 'natural', 'gas', 'fired', 'combustion', 'turbine', 'heat', 'recovery', 'steam', 'generator']\n",
            "    After Lemmatization: ['system', 'us', 'exhaust', 'heat', 'natural', 'gas', 'fired', 'combustion', 'turbine', 'heat', 'recovery', 'steam', 'generator']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9150\n",
            "    After Stopword Removal: ['use', 'bottled', 'disinfected', 'water', 'drinking', 'cooking', 'public', 'water', 'supplies', 'declared', 'safe', '.']\n",
            "    After Regex: ['use', 'bottled', 'disinfected', 'water', 'drinking', 'cooking', 'public', 'water', 'supplies', 'declared', 'safe']\n",
            "    After Lemmatization: ['use', 'bottled', 'disinfected', 'water', 'drinking', 'cooking', 'public', 'water', 'supply', 'declared', 'safe']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9151\n",
            "    After Stopword Removal: ['major', 'objective', 'participants', 'develop', 'analytical', 'decision', '-', 'making', 'methods', 'help', 'reduce', 'presence', 'use', 'toxins', 'hazards', '.']\n",
            "    After Regex: ['major', 'objective', 'participants', 'develop', 'analytical', 'decision', 'making', 'methods', 'help', 'reduce', 'presence', 'use', 'toxins', 'hazards']\n",
            "    After Lemmatization: ['major', 'objective', 'participant', 'develop', 'analytical', 'decision', 'making', 'method', 'help', 'reduce', 'presence', 'use', 'toxin', 'hazard']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9152\n",
            "    After Stopword Removal: ['compression', 'algorithms', ',', 'decompressor', 'recover', 'dropped', ',', 'erroneous', ',', 'mis', '-', 'ordered', 'datagrams', ',', 'propagate', 'errors', 'catastrophically', 'peers', 'reset', 'initial', 'state', '.']\n",
            "    After Regex: ['compression', 'algorithms', 'decompressor', 'recover', 'dropped', 'erroneous', 'mis', 'ordered', 'datagrams', 'propagate', 'errors', 'catastrophically', 'peers', 'reset', 'initial', 'state']\n",
            "    After Lemmatization: ['compression', 'algorithm', 'decompressor', 'recover', 'dropped', 'erroneous', 'mi', 'ordered', 'datagrams', 'propagate', 'error', 'catastrophically', 'peer', 'reset', 'initial', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9153\n",
            "    After Stopword Removal: ['forecasts', 'represent', 'many', 'essential', 'conclusions', 'reached', 'current', 'state', 'market', ',', 'works', 'behaves', 'different', 'macro', 'micro', 'conditions', '.']\n",
            "    After Regex: ['forecasts', 'represent', 'many', 'essential', 'conclusions', 'reached', 'current', 'state', 'market', 'works', 'behaves', 'different', 'macro', 'micro', 'conditions']\n",
            "    After Lemmatization: ['forecast', 'represent', 'many', 'essential', 'conclusion', 'reached', 'current', 'state', 'market', 'work', 'behaves', 'different', 'macro', 'micro', 'condition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9154\n",
            "    After Stopword Removal: ['government', 'wants', 'mixed', 'age', 'couples', '–', 'one', 'person', 'pension', 'credit', 'age', 'younger', '–', 'claim', 'universal', 'credit', '.']\n",
            "    After Regex: ['government', 'wants', 'mixed', 'age', 'couples', 'one', 'person', 'pension', 'credit', 'age', 'younger', 'claim', 'universal', 'credit']\n",
            "    After Lemmatization: ['government', 'want', 'mixed', 'age', 'couple', 'one', 'person', 'pension', 'credit', 'age', 'younger', 'claim', 'universal', 'credit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9155\n",
            "    After Stopword Removal: ['issue', 'demonstrates', 'quality', 'humanity', 'associated', 'moral', 'responsibility', ',', 'integrity', ',', 'honesty', 'transparency', 'gearing', 'ensure', 'survival', 'humans', ',', 'living', 'beings', '.']\n",
            "    After Regex: ['issue', 'demonstrates', 'quality', 'humanity', 'associated', 'moral', 'responsibility', 'integrity', 'honesty', 'transparency', 'gearing', 'ensure', 'survival', 'humans', 'living', 'beings']\n",
            "    After Lemmatization: ['issue', 'demonstrates', 'quality', 'humanity', 'associated', 'moral', 'responsibility', 'integrity', 'honesty', 'transparency', 'gearing', 'ensure', 'survival', 'human', 'living', 'being']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9156\n",
            "    After Stopword Removal: ['historically', ',', 'people', 'blindness', 'visual', 'impairments', 'limited', 'using', 'braille', 'read', '.']\n",
            "    After Regex: ['historically', 'people', 'blindness', 'visual', 'impairments', 'limited', 'using', 'braille', 'read']\n",
            "    After Lemmatization: ['historically', 'people', 'blindness', 'visual', 'impairment', 'limited', 'using', 'braille', 'read']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9157\n",
            "    After Stopword Removal: ['several', 'years', ',', 'gruppi', 'di', 'acquisti', 'solidale', '(', 'gas', ')', 'italy', 'formed', 'backbone', 'italian', 'solidarity', 'economy', 'movement', '.']\n",
            "    After Regex: ['several', 'years', 'gruppi', 'di', 'acquisti', 'solidale', 'gas', 'italy', 'formed', 'backbone', 'italian', 'solidarity', 'economy', 'movement']\n",
            "    After Lemmatization: ['several', 'year', 'gruppi', 'di', 'acquisti', 'solidale', 'gas', 'italy', 'formed', 'backbone', 'italian', 'solidarity', 'economy', 'movement']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9158\n",
            "    After Stopword Removal: ['500', 'c', 'pre', '-', 'incubation', ',', 'mutant', 'lost', 'much', 'high', 'temperature', 'enhancements', 'observed', 'low', 'dmso', 'compositions', '.']\n",
            "    After Regex: ['c', 'pre', 'incubation', 'mutant', 'lost', 'much', 'high', 'temperature', 'enhancements', 'observed', 'low', 'dmso', 'compositions']\n",
            "    After Lemmatization: ['c', 'pre', 'incubation', 'mutant', 'lost', 'much', 'high', 'temperature', 'enhancement', 'observed', 'low', 'dmso', 'composition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9159\n",
            "    After Stopword Removal: [':', 'papua', 'new', 'guinea', 'atlas', ':', 'nation', 'transition', ',', 'david', 'king', '&', 'stephen', 'ranck', ',', 'eds', '.']\n",
            "    After Regex: ['papua', 'new', 'guinea', 'atlas', 'nation', 'transition', 'david', 'king', 'stephen', 'ranck', 'eds']\n",
            "    After Lemmatization: ['papua', 'new', 'guinea', 'atlas', 'nation', 'transition', 'david', 'king', 'stephen', 'ranck', 'ed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9160\n",
            "    After Stopword Removal: ['consists', 'remarkable', 'semi', '-', 'open', 'pavilion', 'pavilions', 'hot', 'spring', 'hot', 'springs', 'made', 'intricately', 'carved', 'mythical', 'stone', 'caves', '.']\n",
            "    After Regex: ['consists', 'remarkable', 'semi', 'open', 'pavilion', 'pavilions', 'hot', 'spring', 'hot', 'springs', 'made', 'intricately', 'carved', 'mythical', 'stone', 'caves']\n",
            "    After Lemmatization: ['consists', 'remarkable', 'semi', 'open', 'pavilion', 'pavilion', 'hot', 'spring', 'hot', 'spring', 'made', 'intricately', 'carved', 'mythical', 'stone', 'cave']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9161\n",
            "    After Stopword Removal: ['reduction', 'georgia', \"'\", 'preterm', 'birth', 'rate', '–', '13.8', 'percent', '13.2', 'percent', '–', 'part', 'national', 'trend', '.']\n",
            "    After Regex: ['reduction', 'georgia', 'preterm', 'birth', 'rate', 'percent', 'percent', 'part', 'national', 'trend']\n",
            "    After Lemmatization: ['reduction', 'georgia', 'preterm', 'birth', 'rate', 'percent', 'percent', 'part', 'national', 'trend']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9162\n",
            "    After Stopword Removal: ['blue', 'watercolour', 'engulfs', 'body', 'naked', 'woman', 'drawing', 'rodin', 'called', 'rock', 'recalls', ',', 'many', 'works', ',', 'sensual', 'primordial', 'universe', 'ocean', 'depths', '.']\n",
            "    After Regex: ['blue', 'watercolour', 'engulfs', 'body', 'naked', 'woman', 'drawing', 'rodin', 'called', 'rock', 'recalls', 'many', 'works', 'sensual', 'primordial', 'universe', 'ocean', 'depths']\n",
            "    After Lemmatization: ['blue', 'watercolour', 'engulfs', 'body', 'naked', 'woman', 'drawing', 'rodin', 'called', 'rock', 'recall', 'many', 'work', 'sensual', 'primordial', 'universe', 'ocean', 'depth']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9163\n",
            "    After Stopword Removal: ['time', ',', 'could', 'cite', 'progress', 'first', 'term', '--', 'economic', 'recover', ',', 'end', 'one', 'war', 'winding', '--', 'offering', 'broader', 'prescription', 'expanding', 'opportunity', 'justice', 'americans', ',', 'keeping', 'nation', \"'\", 'founding', 'principles', '.']\n",
            "    After Regex: ['time', 'could', 'cite', 'progress', 'first', 'term', 'economic', 'recover', 'end', 'one', 'war', 'winding', 'offering', 'broader', 'prescription', 'expanding', 'opportunity', 'justice', 'americans', 'keeping', 'nation', 'founding', 'principles']\n",
            "    After Lemmatization: ['time', 'could', 'cite', 'progress', 'first', 'term', 'economic', 'recover', 'end', 'one', 'war', 'winding', 'offering', 'broader', 'prescription', 'expanding', 'opportunity', 'justice', 'american', 'keeping', 'nation', 'founding', 'principle']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9164\n",
            "    After Stopword Removal: ['write', 'list', ':', 'sentence', 'two', 'scene', '.']\n",
            "    After Regex: ['write', 'list', 'sentence', 'two', 'scene']\n",
            "    After Lemmatization: ['write', 'list', 'sentence', 'two', 'scene']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9165\n",
            "    After Stopword Removal: ['hearing', 'letter', 'name', 'prior', 'detection', 'task', 'increased', 'perceptual', 'sensitivity', '(', '′).']\n",
            "    After Regex: ['hearing', 'letter', 'name', 'prior', 'detection', 'task', 'increased', 'perceptual', 'sensitivity']\n",
            "    After Lemmatization: ['hearing', 'letter', 'name', 'prior', 'detection', 'task', 'increased', 'perceptual', 'sensitivity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9166\n",
            "    After Stopword Removal: ['increase', 'exports', 'stable', 'domestic', 'demand', 'generate', 'growth', 'business', 'investment', ',', 'along', 'increase', 'national', 'savings', ',', 'narrow', 'current', 'account', 'deficit', '3', 'percent', 'gdp', '2018.']\n",
            "    After Regex: ['increase', 'exports', 'stable', 'domestic', 'demand', 'generate', 'growth', 'business', 'investment', 'along', 'increase', 'national', 'savings', 'narrow', 'current', 'account', 'deficit', 'percent', 'gdp']\n",
            "    After Lemmatization: ['increase', 'export', 'stable', 'domestic', 'demand', 'generate', 'growth', 'business', 'investment', 'along', 'increase', 'national', 'saving', 'narrow', 'current', 'account', 'deficit', 'percent', 'gdp']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9167\n",
            "    After Stopword Removal: ['fairtrade', 'movement', 'one', 'powerful', 'responses', 'plight', 'producers', 'developing', 'countries', 'long', 'excluded', 'benefits', 'international', 'trade', '.']\n",
            "    After Regex: ['fairtrade', 'movement', 'one', 'powerful', 'responses', 'plight', 'producers', 'developing', 'countries', 'long', 'excluded', 'benefits', 'international', 'trade']\n",
            "    After Lemmatization: ['fairtrade', 'movement', 'one', 'powerful', 'response', 'plight', 'producer', 'developing', 'country', 'long', 'excluded', 'benefit', 'international', 'trade']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9168\n",
            "    After Stopword Removal: ['journal', 'applied', 'phycology', ',9(6):(533-539).']\n",
            "    After Regex: ['journal', 'applied', 'phycology']\n",
            "    After Lemmatization: ['journal', 'applied', 'phycology']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9169\n",
            "    After Stopword Removal: ['day', 'one', ':', 'write', 'poem', 'line', 'starts', 'letter', 'first', 'name', '(', 'acrostic', ').']\n",
            "    After Regex: ['day', 'one', 'write', 'poem', 'line', 'starts', 'letter', 'first', 'name', 'acrostic']\n",
            "    After Lemmatization: ['day', 'one', 'write', 'poem', 'line', 'start', 'letter', 'first', 'name', 'acrostic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9170\n",
            "    After Stopword Removal: ['goal', 'inpatient', 'treatment', 'provide', 'protective', 'environment', 'includes', 'medical', 'stabilization', ',', 'support', ',', 'treatment', 'psychiatric', 'addictive', 'disorders', ',', 'supervision', '.']\n",
            "    After Regex: ['goal', 'inpatient', 'treatment', 'provide', 'protective', 'environment', 'includes', 'medical', 'stabilization', 'support', 'treatment', 'psychiatric', 'addictive', 'disorders', 'supervision']\n",
            "    After Lemmatization: ['goal', 'inpatient', 'treatment', 'provide', 'protective', 'environment', 'includes', 'medical', 'stabilization', 'support', 'treatment', 'psychiatric', 'addictive', 'disorder', 'supervision']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9171\n",
            "    After Stopword Removal: ['jesus', 'spoke', 'words', 'separation', 'foresaw', 'family', 'members', 'friends', 'divided', 'truth', '.']\n",
            "    After Regex: ['jesus', 'spoke', 'words', 'separation', 'foresaw', 'family', 'members', 'friends', 'divided', 'truth']\n",
            "    After Lemmatization: ['jesus', 'spoke', 'word', 'separation', 'foresaw', 'family', 'member', 'friend', 'divided', 'truth']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9172\n",
            "    After Stopword Removal: ['common', 'practice', '?']\n",
            "    After Regex: ['common', 'practice']\n",
            "    After Lemmatization: ['common', 'practice']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9173\n",
            "    After Stopword Removal: ['wing', 'provides', 'expert', 'evaluation', 'validation', 'performance', 'systems', 'throughout', 'design', ',', 'development', ',', 'acquisition', ',', 'sustainment', 'process', 'ensure', 'warfighter', 'technologically', 'superior', ',', 'reliable', ',', 'maintainable', ',', 'sustainable', 'safe', 'systems', '.']\n",
            "    After Regex: ['wing', 'provides', 'expert', 'evaluation', 'validation', 'performance', 'systems', 'throughout', 'design', 'development', 'acquisition', 'sustainment', 'process', 'ensure', 'warfighter', 'technologically', 'superior', 'reliable', 'maintainable', 'sustainable', 'safe', 'systems']\n",
            "    After Lemmatization: ['wing', 'provides', 'expert', 'evaluation', 'validation', 'performance', 'system', 'throughout', 'design', 'development', 'acquisition', 'sustainment', 'process', 'ensure', 'warfighter', 'technologically', 'superior', 'reliable', 'maintainable', 'sustainable', 'safe', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9174\n",
            "    After Stopword Removal: [\"'\", 'needed', ',', 'giugliano', 'said', ',', 'much', 'larger', 'studies', 'involving', 'thousands', 'patients', 'given', 'injections', 'least', 'five', 'years', 'study', 'long', 'term', 'safety', 'efficacy', '.']\n",
            "    After Regex: ['needed', 'giugliano', 'said', 'much', 'larger', 'studies', 'involving', 'thousands', 'patients', 'given', 'injections', 'least', 'five', 'years', 'study', 'long', 'term', 'safety', 'efficacy']\n",
            "    After Lemmatization: ['needed', 'giugliano', 'said', 'much', 'larger', 'study', 'involving', 'thousand', 'patient', 'given', 'injection', 'least', 'five', 'year', 'study', 'long', 'term', 'safety', 'efficacy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9175\n",
            "    After Stopword Removal: ['cool', 'air', 'lake', 'also', 'retards', 'growing', 'season', ',', 'diminishes', 'threat', 'damaging', 'late', 'spring', 'frosts', '.']\n",
            "    After Regex: ['cool', 'air', 'lake', 'also', 'retards', 'growing', 'season', 'diminishes', 'threat', 'damaging', 'late', 'spring', 'frosts']\n",
            "    After Lemmatization: ['cool', 'air', 'lake', 'also', 'retard', 'growing', 'season', 'diminishes', 'threat', 'damaging', 'late', 'spring', 'frost']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9176\n",
            "    After Stopword Removal: ['also', 'great', 'way', 'get', 'raw', 'eggs', 'diet', ',', 'providing', 'brain', '-', 'powering', 'choline', 'vitamin', 'k', '2.']\n",
            "    After Regex: ['also', 'great', 'way', 'get', 'raw', 'eggs', 'diet', 'providing', 'brain', 'powering', 'choline', 'vitamin', 'k']\n",
            "    After Lemmatization: ['also', 'great', 'way', 'get', 'raw', 'egg', 'diet', 'providing', 'brain', 'powering', 'choline', 'vitamin', 'k']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9177\n",
            "    After Stopword Removal: ['officially', ',', 'pope', \"'\", 'trip', 'coincide', '400', 'th', 'anniversary', 'discovery', 'statue', 'virgin', 'charity', '.']\n",
            "    After Regex: ['officially', 'pope', 'trip', 'coincide', 'th', 'anniversary', 'discovery', 'statue', 'virgin', 'charity']\n",
            "    After Lemmatization: ['officially', 'pope', 'trip', 'coincide', 'th', 'anniversary', 'discovery', 'statue', 'virgin', 'charity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9178\n",
            "    After Stopword Removal: ['another', '$15', 'million', 'go', 'department', 'environmental', 'quality', 'fixing', 'leaking', 'landfills', '.']\n",
            "    After Regex: ['another', 'million', 'go', 'department', 'environmental', 'quality', 'fixing', 'leaking', 'landfills']\n",
            "    After Lemmatization: ['another', 'million', 'go', 'department', 'environmental', 'quality', 'fixing', 'leaking', 'landfill']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9179\n",
            "    After Stopword Removal: ['eggs', 'hatch', '4', '12', 'days', 'larvae', 'attach', 'rocks', 'means', 'posterior', 'organ', 'armed', 'small', 'hooks', '.']\n",
            "    After Regex: ['eggs', 'hatch', 'days', 'larvae', 'attach', 'rocks', 'means', 'posterior', 'organ', 'armed', 'small', 'hooks']\n",
            "    After Lemmatization: ['egg', 'hatch', 'day', 'larva', 'attach', 'rock', 'mean', 'posterior', 'organ', 'armed', 'small', 'hook']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9180\n",
            "    After Stopword Removal: ['know', 'muslim', 'women', 'face', 'considerable', 'pressure', 'get', 'involved', 'politics', '.']\n",
            "    After Regex: ['know', 'muslim', 'women', 'face', 'considerable', 'pressure', 'get', 'involved', 'politics']\n",
            "    After Lemmatization: ['know', 'muslim', 'woman', 'face', 'considerable', 'pressure', 'get', 'involved', 'politics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9181\n",
            "    After Stopword Removal: ['myzone', 'system', 'motivates', 'measures', 'people', 'shape', ',', 'size', ',', 'age', 'interest', 'get', 'active', 'anywhere', '.']\n",
            "    After Regex: ['myzone', 'system', 'motivates', 'measures', 'people', 'shape', 'size', 'age', 'interest', 'get', 'active', 'anywhere']\n",
            "    After Lemmatization: ['myzone', 'system', 'motivates', 'measure', 'people', 'shape', 'size', 'age', 'interest', 'get', 'active', 'anywhere']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9182\n",
            "    After Stopword Removal: ['brought', 'jewish', 'family', 'disregard', 'personal', 'space', 'tactful', 'enquiry', ',', 'rachel', \"'\", 'work', 'explores', 'idiosyncratic', 'constructs', 'surround', 'social', 'interactions', ',', 'forcing', 'deeper', 'examination', 'cultural', 'habits', '.']\n",
            "    After Regex: ['brought', 'jewish', 'family', 'disregard', 'personal', 'space', 'tactful', 'enquiry', 'rachel', 'work', 'explores', 'idiosyncratic', 'constructs', 'surround', 'social', 'interactions', 'forcing', 'deeper', 'examination', 'cultural', 'habits']\n",
            "    After Lemmatization: ['brought', 'jewish', 'family', 'disregard', 'personal', 'space', 'tactful', 'enquiry', 'rachel', 'work', 'explores', 'idiosyncratic', 'construct', 'surround', 'social', 'interaction', 'forcing', 'deeper', 'examination', 'cultural', 'habit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9183\n",
            "    After Stopword Removal: ['frostbite', 'severe', 'reaction', 'cold', 'exposure', 'permanently', 'damage', 'victim', '.']\n",
            "    After Regex: ['frostbite', 'severe', 'reaction', 'cold', 'exposure', 'permanently', 'damage', 'victim']\n",
            "    After Lemmatization: ['frostbite', 'severe', 'reaction', 'cold', 'exposure', 'permanently', 'damage', 'victim']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9184\n",
            "    After Stopword Removal: ['contact', 'details', ',', 'bank', 'details', ',', 'login', 'credentials', '–', 'sensitive', 'data', 'potential', 'target', 'threats', '.']\n",
            "    After Regex: ['contact', 'details', 'bank', 'details', 'login', 'credentials', 'sensitive', 'data', 'potential', 'target', 'threats']\n",
            "    After Lemmatization: ['contact', 'detail', 'bank', 'detail', 'login', 'credential', 'sensitive', 'data', 'potential', 'target', 'threat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9185\n",
            "    After Stopword Removal: ['built', '-', 'heart', 'rate', 'sensors', 'help', 'monitor', 'heart', 'rate', 'throughout', 'workout', '.']\n",
            "    After Regex: ['built', 'heart', 'rate', 'sensors', 'help', 'monitor', 'heart', 'rate', 'throughout', 'workout']\n",
            "    After Lemmatization: ['built', 'heart', 'rate', 'sensor', 'help', 'monitor', 'heart', 'rate', 'throughout', 'workout']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9186\n",
            "    After Stopword Removal: ['condition', 'caused', 'application', 'acidifying', 'fertilizer', '.']\n",
            "    After Regex: ['condition', 'caused', 'application', 'acidifying', 'fertilizer']\n",
            "    After Lemmatization: ['condition', 'caused', 'application', 'acidifying', 'fertilizer']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9187\n",
            "    After Stopword Removal: ['europe', 'stretches', 'shores', 'mediterranean', 'arctic', 'circle', 'atlantic', 'ocean', 'black', 'sea', '.']\n",
            "    After Regex: ['europe', 'stretches', 'shores', 'mediterranean', 'arctic', 'circle', 'atlantic', 'ocean', 'black', 'sea']\n",
            "    After Lemmatization: ['europe', 'stretch', 'shore', 'mediterranean', 'arctic', 'circle', 'atlantic', 'ocean', 'black', 'sea']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9188\n",
            "    After Stopword Removal: ['could', 'heavy', 'blanket', 'help', 'fall', 'asleep', '?']\n",
            "    After Regex: ['could', 'heavy', 'blanket', 'help', 'fall', 'asleep']\n",
            "    After Lemmatization: ['could', 'heavy', 'blanket', 'help', 'fall', 'asleep']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9189\n",
            "    After Stopword Removal: ['value', 'gold', 'rising', 'recently', ',', 'people', 'unwanted', 'jewelry', 'taking', 'precious', 'metal', 'resale', 'shops', '.']\n",
            "    After Regex: ['value', 'gold', 'rising', 'recently', 'people', 'unwanted', 'jewelry', 'taking', 'precious', 'metal', 'resale', 'shops']\n",
            "    After Lemmatization: ['value', 'gold', 'rising', 'recently', 'people', 'unwanted', 'jewelry', 'taking', 'precious', 'metal', 'resale', 'shop']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9190\n",
            "    After Stopword Removal: ['nf', '-', 'κ', 'b', 'also', 'implicated', 'processes', 'synaptic', 'plasticity', 'memory', '.']\n",
            "    After Regex: ['nf', 'b', 'also', 'implicated', 'processes', 'synaptic', 'plasticity', 'memory']\n",
            "    After Lemmatization: ['nf', 'b', 'also', 'implicated', 'process', 'synaptic', 'plasticity', 'memory']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9191\n",
            "    After Stopword Removal: ['.', 'wulf', ',', 'president', 'national', 'academy', 'engineering', '.']\n",
            "    After Regex: ['wulf', 'president', 'national', 'academy', 'engineering']\n",
            "    After Lemmatization: ['wulf', 'president', 'national', 'academy', 'engineering']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9192\n",
            "    After Stopword Removal: ['stop', 'creating', 'tension', 'remain', 'peaceful', 'harmonious', '?']\n",
            "    After Regex: ['stop', 'creating', 'tension', 'remain', 'peaceful', 'harmonious']\n",
            "    After Lemmatization: ['stop', 'creating', 'tension', 'remain', 'peaceful', 'harmonious']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9193\n",
            "    After Stopword Removal: ['precision', 'stereo', 'optics', 'produce', 'high', '-', 'resolution', ',', 'three', '-', 'dimensional', 'images', '.']\n",
            "    After Regex: ['precision', 'stereo', 'optics', 'produce', 'high', 'resolution', 'three', 'dimensional', 'images']\n",
            "    After Lemmatization: ['precision', 'stereo', 'optic', 'produce', 'high', 'resolution', 'three', 'dimensional', 'image']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9194\n",
            "    After Stopword Removal: [\"'\", 'unclear', 'levels', 'bpa', 'health', 'problems', ',', 'bpa', 'studies', 'animals', 'applied', 'humans', '.']\n",
            "    After Regex: ['unclear', 'levels', 'bpa', 'health', 'problems', 'bpa', 'studies', 'animals', 'applied', 'humans']\n",
            "    After Lemmatization: ['unclear', 'level', 'bpa', 'health', 'problem', 'bpa', 'study', 'animal', 'applied', 'human']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9195\n",
            "    After Stopword Removal: ['oulehle', ',', 'f', '.,', 'mcdowell', ',', 'w', '.', 'h', '.,', 'aitkenhead', '-', 'peterson', ',', 'j', '.', '.,', 'krám', ',', 'p', '.,', 'hruška', ',', 'j', '.,', 'navrátil', ',', '.,', 'buzek', ',', 'f', '.,', 'fottová', ',', '.(2008)', 'long', '-', 'term', 'trends', 'stream', 'nitrate', 'concentrations', 'losses', 'across', 'watersheds', 'undergoing', 'recovery', 'acidification', 'czech', 'republic', '.']\n",
            "    After Regex: ['oulehle', 'f', 'mcdowell', 'w', 'h', 'aitkenhead', 'peterson', 'j', 'krm', 'p', 'hruka', 'j', 'navrtil', 'buzek', 'f', 'fottov', 'long', 'term', 'trends', 'stream', 'nitrate', 'concentrations', 'losses', 'across', 'watersheds', 'undergoing', 'recovery', 'acidification', 'czech', 'republic']\n",
            "    After Lemmatization: ['oulehle', 'f', 'mcdowell', 'w', 'h', 'aitkenhead', 'peterson', 'j', 'krm', 'p', 'hruka', 'j', 'navrtil', 'buzek', 'f', 'fottov', 'long', 'term', 'trend', 'stream', 'nitrate', 'concentration', 'loss', 'across', 'watershed', 'undergoing', 'recovery', 'acidification', 'czech', 'republic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9196\n",
            "    After Stopword Removal: ['program', 'burdened', 'sluggish', 'economy', ',', 'reduced', 'tax', 'revenue', '.']\n",
            "    After Regex: ['program', 'burdened', 'sluggish', 'economy', 'reduced', 'tax', 'revenue']\n",
            "    After Lemmatization: ['program', 'burdened', 'sluggish', 'economy', 'reduced', 'tax', 'revenue']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9197\n",
            "    After Stopword Removal: ['lieutenant', 'westermann', 'concluded', \",'\", 'ever', 'greater', 'panic', 'spreading', 'among', 'jews', 'due', 'great', 'heat', ',', 'overloading', 'train', 'cars', 'stink', 'dead', 'unloading', 'train', 'cars', ',', '2,000', 'jews', 'found', 'dead', 'train', 'made', 'transport', 'almost', 'unworkable', \".'\"]\n",
            "    After Regex: ['lieutenant', 'westermann', 'concluded', 'ever', 'greater', 'panic', 'spreading', 'among', 'jews', 'due', 'great', 'heat', 'overloading', 'train', 'cars', 'stink', 'dead', 'unloading', 'train', 'cars', 'jews', 'found', 'dead', 'train', 'made', 'transport', 'almost', 'unworkable']\n",
            "    After Lemmatization: ['lieutenant', 'westermann', 'concluded', 'ever', 'greater', 'panic', 'spreading', 'among', 'jew', 'due', 'great', 'heat', 'overloading', 'train', 'car', 'stink', 'dead', 'unloading', 'train', 'car', 'jew', 'found', 'dead', 'train', 'made', 'transport', 'almost', 'unworkable']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9198\n",
            "    After Stopword Removal: ['iniquitous', 'situation', 'would', 'easily', 'continued', '-', 'called', 'arab', 'spring', ',', 'began', 'demolishing', 'status', 'quo', 'governing', 'arab', 'countries', '.']\n",
            "    After Regex: ['iniquitous', 'situation', 'would', 'easily', 'continued', 'called', 'arab', 'spring', 'began', 'demolishing', 'status', 'quo', 'governing', 'arab', 'countries']\n",
            "    After Lemmatization: ['iniquitous', 'situation', 'would', 'easily', 'continued', 'called', 'arab', 'spring', 'began', 'demolishing', 'status', 'quo', 'governing', 'arab', 'country']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9199\n",
            "    After Stopword Removal: ['fourteen', 'colleges', 'showed', 'decline', '.']\n",
            "    After Regex: ['fourteen', 'colleges', 'showed', 'decline']\n",
            "    After Lemmatization: ['fourteen', 'college', 'showed', 'decline']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9200\n",
            "    After Stopword Removal: ['verse', 'tells', 'us', 'finish', 'prayer', 'god', 'hears', 'us', '.', 'answers', 'get', 'prayer', 'finished', '.', 'matthew', '6:8', 'heavenly', 'father', 'knows', 'need', 'ask', '.', 'bible', 'suggests', 'go', 'area', 'room', '-', '-', 'words', 'talk', 'private', '.', 'people', 'repeat', 'words', 'prayer', '.', 'father', 'knows', 'need', 'hears', 'prayers', 'go', '.', 'ever', 'asked', 'prayer', '?']\n",
            "    After Regex: ['verse', 'tells', 'us', 'finish', 'prayer', 'god', 'hears', 'us', 'answers', 'get', 'prayer', 'finished', 'matthew', 'heavenly', 'father', 'knows', 'need', 'ask', 'bible', 'suggests', 'go', 'area', 'room', 'words', 'talk', 'private', 'people', 'repeat', 'words', 'prayer', 'father', 'knows', 'need', 'hears', 'prayers', 'go', 'ever', 'asked', 'prayer']\n",
            "    After Lemmatization: ['verse', 'tell', 'u', 'finish', 'prayer', 'god', 'hears', 'u', 'answer', 'get', 'prayer', 'finished', 'matthew', 'heavenly', 'father', 'know', 'need', 'ask', 'bible', 'suggests', 'go', 'area', 'room', 'word', 'talk', 'private', 'people', 'repeat', 'word', 'prayer', 'father', 'know', 'need', 'hears', 'prayer', 'go', 'ever', 'asked', 'prayer']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9201\n",
            "    After Stopword Removal: ['curriculum', ':', 'wee', 'learn', '.']\n",
            "    After Regex: ['curriculum', 'wee', 'learn']\n",
            "    After Lemmatization: ['curriculum', 'wee', 'learn']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9202\n",
            "    After Stopword Removal: ['abandon', 'principle', 'hallowed', 'time', ',', 'namely', 'government', 'cannot', 'persuade', 'houses', 'parliament', 'end', 'session', 'agree', 'bill', ',', 'bill', 'falls', '.']\n",
            "    After Regex: ['abandon', 'principle', 'hallowed', 'time', 'namely', 'government', 'cannot', 'persuade', 'houses', 'parliament', 'end', 'session', 'agree', 'bill', 'bill', 'falls']\n",
            "    After Lemmatization: ['abandon', 'principle', 'hallowed', 'time', 'namely', 'government', 'cannot', 'persuade', 'house', 'parliament', 'end', 'session', 'agree', 'bill', 'bill', 'fall']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9203\n",
            "    After Stopword Removal: ['president', 'aliyev', 'noted', 'reaching', 'peak', 'oil', 'production', 'subsequent', 'decline', 'natural', 'process', ',', 'problem', 'failure', 'fulfill', 'presented', 'forecasts', '.']\n",
            "    After Regex: ['president', 'aliyev', 'noted', 'reaching', 'peak', 'oil', 'production', 'subsequent', 'decline', 'natural', 'process', 'problem', 'failure', 'fulfill', 'presented', 'forecasts']\n",
            "    After Lemmatization: ['president', 'aliyev', 'noted', 'reaching', 'peak', 'oil', 'production', 'subsequent', 'decline', 'natural', 'process', 'problem', 'failure', 'fulfill', 'presented', 'forecast']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9204\n",
            "    After Stopword Removal: ['due', 'relatively', 'small', 'sizes', ',', 'industries', 'mainly', 'serve', 'local', 'markets', 'causing', 'lower', 'capacity', 'usage', ',', 'low', 'productivity', 'unemployment', '.']\n",
            "    After Regex: ['due', 'relatively', 'small', 'sizes', 'industries', 'mainly', 'serve', 'local', 'markets', 'causing', 'lower', 'capacity', 'usage', 'low', 'productivity', 'unemployment']\n",
            "    After Lemmatization: ['due', 'relatively', 'small', 'size', 'industry', 'mainly', 'serve', 'local', 'market', 'causing', 'lower', 'capacity', 'usage', 'low', 'productivity', 'unemployment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9205\n",
            "    After Stopword Removal: ['prices', 'food', 'non', '-', 'alcoholic', 'beverages', 'rose', '4.8', 'percent', 'annually', ',', 'clothing', 'footwear', 'prices', 'decreases', '0.8', 'percent', '.']\n",
            "    After Regex: ['prices', 'food', 'non', 'alcoholic', 'beverages', 'rose', 'percent', 'annually', 'clothing', 'footwear', 'prices', 'decreases', 'percent']\n",
            "    After Lemmatization: ['price', 'food', 'non', 'alcoholic', 'beverage', 'rose', 'percent', 'annually', 'clothing', 'footwear', 'price', 'decrease', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9206\n",
            "    After Stopword Removal: ['clarification', 'new', 'policy', 'emphasizes', 'distinction', 'activities', 'beliefs', 'organization', 'government', 'determines', 'receive', 'funding', '.']\n",
            "    After Regex: ['clarification', 'new', 'policy', 'emphasizes', 'distinction', 'activities', 'beliefs', 'organization', 'government', 'determines', 'receive', 'funding']\n",
            "    After Lemmatization: ['clarification', 'new', 'policy', 'emphasizes', 'distinction', 'activity', 'belief', 'organization', 'government', 'determines', 'receive', 'funding']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9207\n",
            "    After Stopword Removal: ['fewer', 'fewer', 'folks', 'contributed', 'success', 'economy', 'actually', 'benefited', 'success', '.']\n",
            "    After Regex: ['fewer', 'fewer', 'folks', 'contributed', 'success', 'economy', 'actually', 'benefited', 'success']\n",
            "    After Lemmatization: ['fewer', 'fewer', 'folk', 'contributed', 'success', 'economy', 'actually', 'benefited', 'success']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9208\n",
            "    After Stopword Removal: ['others', 'suggested', 'recycling', 'plastics', 'purpose', '.']\n",
            "    After Regex: ['others', 'suggested', 'recycling', 'plastics', 'purpose']\n",
            "    After Lemmatization: ['others', 'suggested', 'recycling', 'plastic', 'purpose']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9209\n",
            "    After Stopword Removal: ['100,000', 'people', 'united', 'states', 'need', 'transplant', ',', 'yet', '25', 'percent', 'receive', 'vital', 'organs', '.']\n",
            "    After Regex: ['people', 'united', 'states', 'need', 'transplant', 'yet', 'percent', 'receive', 'vital', 'organs']\n",
            "    After Lemmatization: ['people', 'united', 'state', 'need', 'transplant', 'yet', 'percent', 'receive', 'vital', 'organ']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9210\n",
            "    After Stopword Removal: ['webinars', 'help', '!']\n",
            "    After Regex: ['webinars', 'help']\n",
            "    After Lemmatization: ['webinars', 'help']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9211\n",
            "    After Stopword Removal: ['gay', 'marriage', 'threat', 'religious', 'freedom', '?']\n",
            "    After Regex: ['gay', 'marriage', 'threat', 'religious', 'freedom']\n",
            "    After Lemmatization: ['gay', 'marriage', 'threat', 'religious', 'freedom']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9212\n",
            "    After Stopword Removal: ['feel', 'though', 'ignored', ',', 'marginalized', ',', 'degree', ',', 'vilified', '.']\n",
            "    After Regex: ['feel', 'though', 'ignored', 'marginalized', 'degree', 'vilified']\n",
            "    After Lemmatization: ['feel', 'though', 'ignored', 'marginalized', 'degree', 'vilified']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9213\n",
            "    After Stopword Removal: ['core', 'idea', 'implicit', 'doctrine', 'stare', 'decisis', '.']\n",
            "    After Regex: ['core', 'idea', 'implicit', 'doctrine', 'stare', 'decisis']\n",
            "    After Lemmatization: ['core', 'idea', 'implicit', 'doctrine', 'stare', 'decisis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9214\n",
            "    After Stopword Removal: ['trail', 'wound', 'hill', 'ancient', 'theatre', ',', '2', 'nd', 'century', 'bce', '.']\n",
            "    After Regex: ['trail', 'wound', 'hill', 'ancient', 'theatre', 'nd', 'century', 'bce']\n",
            "    After Lemmatization: ['trail', 'wound', 'hill', 'ancient', 'theatre', 'nd', 'century', 'bce']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9215\n",
            "    After Stopword Removal: ['major', 'conditions', 'lives', 'determined', 'israeli', 'military', 'occupation', 'areas', '.']\n",
            "    After Regex: ['major', 'conditions', 'lives', 'determined', 'israeli', 'military', 'occupation', 'areas']\n",
            "    After Lemmatization: ['major', 'condition', 'life', 'determined', 'israeli', 'military', 'occupation', 'area']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9216\n",
            "    After Stopword Removal: ['dan', 'represents', 'discrimination', 'judgment', ',', 'choosing', 'good', 'evil', '.']\n",
            "    After Regex: ['dan', 'represents', 'discrimination', 'judgment', 'choosing', 'good', 'evil']\n",
            "    After Lemmatization: ['dan', 'represents', 'discrimination', 'judgment', 'choosing', 'good', 'evil']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9217\n",
            "    After Stopword Removal: ['means', 'dramatic', 'skies', ',', 'important', 'element', 'flat', 'landscape', '.']\n",
            "    After Regex: ['means', 'dramatic', 'skies', 'important', 'element', 'flat', 'landscape']\n",
            "    After Lemmatization: ['mean', 'dramatic', 'sky', 'important', 'element', 'flat', 'landscape']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9218\n",
            "    After Stopword Removal: ['tamari', 'origin', 'japanese', 'soy', 'sauce', '.']\n",
            "    After Regex: ['tamari', 'origin', 'japanese', 'soy', 'sauce']\n",
            "    After Lemmatization: ['tamari', 'origin', 'japanese', 'soy', 'sauce']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9219\n",
            "    After Stopword Removal: ['plasmid', '-', 'containing', 'bacteria', 'decreased', 'significantly', 'vector', 'without', 'active', 'partitioning', 'system', ',', 'plasmid', 'loss', 'already', 'visible', '60', 'th', 'generation', '.']\n",
            "    After Regex: ['plasmid', 'containing', 'bacteria', 'decreased', 'significantly', 'vector', 'without', 'active', 'partitioning', 'system', 'plasmid', 'loss', 'already', 'visible', 'th', 'generation']\n",
            "    After Lemmatization: ['plasmid', 'containing', 'bacteria', 'decreased', 'significantly', 'vector', 'without', 'active', 'partitioning', 'system', 'plasmid', 'loss', 'already', 'visible', 'th', 'generation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9220\n",
            "    After Stopword Removal: ['intending', 'permit', 'hiring', 'qualified', 'teacher', ',', 'effect', 'defines', 'qualified', 'teacher', 'good', 'standing', 'already', 'employed', 'county', '.']\n",
            "    After Regex: ['intending', 'permit', 'hiring', 'qualified', 'teacher', 'effect', 'defines', 'qualified', 'teacher', 'good', 'standing', 'already', 'employed', 'county']\n",
            "    After Lemmatization: ['intending', 'permit', 'hiring', 'qualified', 'teacher', 'effect', 'defines', 'qualified', 'teacher', 'good', 'standing', 'already', 'employed', 'county']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9221\n",
            "    After Stopword Removal: ['every', 'student', 'end', 'class', 'knew', 'energy', 'came', ',', 'according', 'post', '-', 'project', 'evaluations', ',', 'said', 'never', 'imagined', 'labor', '-', 'intensive', 'exacting', 'good', 'sociological', 'research', '.']\n",
            "    After Regex: ['every', 'student', 'end', 'class', 'knew', 'energy', 'came', 'according', 'post', 'project', 'evaluations', 'said', 'never', 'imagined', 'labor', 'intensive', 'exacting', 'good', 'sociological', 'research']\n",
            "    After Lemmatization: ['every', 'student', 'end', 'class', 'knew', 'energy', 'came', 'according', 'post', 'project', 'evaluation', 'said', 'never', 'imagined', 'labor', 'intensive', 'exacting', 'good', 'sociological', 'research']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9222\n",
            "    After Stopword Removal: ['damaging', 'factor', 'friction', 'massive', 'amount', 'heat', 'produced', ',', 'ruin', 'engine', '.']\n",
            "    After Regex: ['damaging', 'factor', 'friction', 'massive', 'amount', 'heat', 'produced', 'ruin', 'engine']\n",
            "    After Lemmatization: ['damaging', 'factor', 'friction', 'massive', 'amount', 'heat', 'produced', 'ruin', 'engine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9223\n",
            "    After Stopword Removal: ['called', 'sentences', ',', 'reflection', 'focus', '.']\n",
            "    After Regex: ['called', 'sentences', 'reflection', 'focus']\n",
            "    After Lemmatization: ['called', 'sentence', 'reflection', 'focus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9224\n",
            "    After Stopword Removal: ['example', ',', 'placement', 'brain', 'regions', 'high', 'interconnectivity', 'adjacent', 'one', 'another', 'reduce', 'total', 'length', 'axons', '.']\n",
            "    After Regex: ['example', 'placement', 'brain', 'regions', 'high', 'interconnectivity', 'adjacent', 'one', 'another', 'reduce', 'total', 'length', 'axons']\n",
            "    After Lemmatization: ['example', 'placement', 'brain', 'region', 'high', 'interconnectivity', 'adjacent', 'one', 'another', 'reduce', 'total', 'length', 'axon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9225\n",
            "    After Stopword Removal: ['comes', 'growing', 'quality', 'plants', ',', 'safety', 'consistency', 'vital', '.']\n",
            "    After Regex: ['comes', 'growing', 'quality', 'plants', 'safety', 'consistency', 'vital']\n",
            "    After Lemmatization: ['come', 'growing', 'quality', 'plant', 'safety', 'consistency', 'vital']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9226\n",
            "    After Stopword Removal: ['patient', \"'\", 'permission', ',', 'reports', 'forwarded', 'food', 'drug', 'administration', ',', 'manufacturer', 'inform', 'improper', 'pharmaceutical', 'labeling', ',', 'packaging', ',', 'issues', 'may', 'cause', 'errors', '.']\n",
            "    After Regex: ['patient', 'permission', 'reports', 'forwarded', 'food', 'drug', 'administration', 'manufacturer', 'inform', 'improper', 'pharmaceutical', 'labeling', 'packaging', 'issues', 'may', 'cause', 'errors']\n",
            "    After Lemmatization: ['patient', 'permission', 'report', 'forwarded', 'food', 'drug', 'administration', 'manufacturer', 'inform', 'improper', 'pharmaceutical', 'labeling', 'packaging', 'issue', 'may', 'cause', 'error']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9227\n",
            "    After Stopword Removal: ['going', 'forward', ',', 'would', 'improve', 'security', 'remaining', 'recommendations', '9-11', 'commission', 'implemented', ',', 'including', 'government', 'action', 'federal', 'standards', 'birth', 'certificates', 'driver', \"'\", 'licenses', 'allocation', 'radio', 'spectrum', 'space', ',', 'first', 'responders', 'need', 'repeat', 'communications', 'breakdowns', 'many', 'experienced', '2001.']\n",
            "    After Regex: ['going', 'forward', 'would', 'improve', 'security', 'remaining', 'recommendations', 'commission', 'implemented', 'including', 'government', 'action', 'federal', 'standards', 'birth', 'certificates', 'driver', 'licenses', 'allocation', 'radio', 'spectrum', 'space', 'first', 'responders', 'need', 'repeat', 'communications', 'breakdowns', 'many', 'experienced']\n",
            "    After Lemmatization: ['going', 'forward', 'would', 'improve', 'security', 'remaining', 'recommendation', 'commission', 'implemented', 'including', 'government', 'action', 'federal', 'standard', 'birth', 'certificate', 'driver', 'license', 'allocation', 'radio', 'spectrum', 'space', 'first', 'responder', 'need', 'repeat', 'communication', 'breakdown', 'many', 'experienced']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9228\n",
            "    After Stopword Removal: ['arts', '&', 'labor', 'dedicated', 'exposing', 'rectifying', 'economic', 'inequalities', 'exploitative', 'working', 'conditions', 'fields', 'direct', 'action', 'educational', 'initiatives', '.']\n",
            "    After Regex: ['arts', 'labor', 'dedicated', 'exposing', 'rectifying', 'economic', 'inequalities', 'exploitative', 'working', 'conditions', 'fields', 'direct', 'action', 'educational', 'initiatives']\n",
            "    After Lemmatization: ['art', 'labor', 'dedicated', 'exposing', 'rectifying', 'economic', 'inequality', 'exploitative', 'working', 'condition', 'field', 'direct', 'action', 'educational', 'initiative']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9229\n",
            "    After Stopword Removal: ['confrontation', 'clause', 'united', 'states', 'constitution', 'operates', 'two', 'ways', 'determining', 'admissibility', 'hearsay', 'statements', '.']\n",
            "    After Regex: ['confrontation', 'clause', 'united', 'states', 'constitution', 'operates', 'two', 'ways', 'determining', 'admissibility', 'hearsay', 'statements']\n",
            "    After Lemmatization: ['confrontation', 'clause', 'united', 'state', 'constitution', 'operates', 'two', 'way', 'determining', 'admissibility', 'hearsay', 'statement']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9230\n",
            "    After Stopword Removal: ['california', 'legislature', 'mobilized', '$20', 'million', 'emergency', 'funding', 'california', 'conservation', 'corps', '(', 'ccc', ')', 'purchase', 'distribute', 'free', 'cfls', 'nearly', 'half', 'million', 'low', '-', 'income', 'households', '.']\n",
            "    After Regex: ['california', 'legislature', 'mobilized', 'million', 'emergency', 'funding', 'california', 'conservation', 'corps', 'ccc', 'purchase', 'distribute', 'free', 'cfls', 'nearly', 'half', 'million', 'low', 'income', 'households']\n",
            "    After Lemmatization: ['california', 'legislature', 'mobilized', 'million', 'emergency', 'funding', 'california', 'conservation', 'corp', 'ccc', 'purchase', 'distribute', 'free', 'cfls', 'nearly', 'half', 'million', 'low', 'income', 'household']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9231\n",
            "    After Stopword Removal: ['christianity', 'gave', 'indians', 'one', 'flavoured', 'european', 'culture', ',', 'accepted', 'change', 'names', ',', 'surnames', ',', 'food', 'habits', ',', 'undergoing', 'total', 'change', 'lifestyle', ',', 'customs', 'even', 'language', '.']\n",
            "    After Regex: ['christianity', 'gave', 'indians', 'one', 'flavoured', 'european', 'culture', 'accepted', 'change', 'names', 'surnames', 'food', 'habits', 'undergoing', 'total', 'change', 'lifestyle', 'customs', 'even', 'language']\n",
            "    After Lemmatization: ['christianity', 'gave', 'indian', 'one', 'flavoured', 'european', 'culture', 'accepted', 'change', 'name', 'surname', 'food', 'habit', 'undergoing', 'total', 'change', 'lifestyle', 'custom', 'even', 'language']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9232\n",
            "    After Stopword Removal: ['treat', 'heirlooms', ',', 'north', 'americans', 'regard', 'gold', ',', 'silver', ',', 'diamonds', '.']\n",
            "    After Regex: ['treat', 'heirlooms', 'north', 'americans', 'regard', 'gold', 'silver', 'diamonds']\n",
            "    After Lemmatization: ['treat', 'heirloom', 'north', 'american', 'regard', 'gold', 'silver', 'diamond']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9233\n",
            "    After Stopword Removal: ['bible', 'much', 'say', 'blood', 'jesus', '.']\n",
            "    After Regex: ['bible', 'much', 'say', 'blood', 'jesus']\n",
            "    After Lemmatization: ['bible', 'much', 'say', 'blood', 'jesus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9234\n",
            "    After Stopword Removal: ['small', 'amounts', 'botox', 'injected', 'face', 'using', 'disposable', 'syringe', ',', 'causes', 'temporary', 'muscle', 'paralysis', 'facial', 'muscles', 'contract', 'cause', 'wrinkles', '.']\n",
            "    After Regex: ['small', 'amounts', 'botox', 'injected', 'face', 'using', 'disposable', 'syringe', 'causes', 'temporary', 'muscle', 'paralysis', 'facial', 'muscles', 'contract', 'cause', 'wrinkles']\n",
            "    After Lemmatization: ['small', 'amount', 'botox', 'injected', 'face', 'using', 'disposable', 'syringe', 'cause', 'temporary', 'muscle', 'paralysis', 'facial', 'muscle', 'contract', 'cause', 'wrinkle']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9235\n",
            "    After Stopword Removal: ['us', 'fish', '&', 'wildlife', 'service', 'biological', 'report', '88(24).']\n",
            "    After Regex: ['us', 'fish', 'wildlife', 'service', 'biological', 'report']\n",
            "    After Lemmatization: ['u', 'fish', 'wildlife', 'service', 'biological', 'report']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9236\n",
            "    After Stopword Removal: ['excellent', 'schools', ',', 'park', 'system', 'year', '-', 'round', 'community', 'activities', 'also', 'make', 'quality', 'life', 'enjoyable', 'rewarding', '.']\n",
            "    After Regex: ['excellent', 'schools', 'park', 'system', 'year', 'round', 'community', 'activities', 'also', 'make', 'quality', 'life', 'enjoyable', 'rewarding']\n",
            "    After Lemmatization: ['excellent', 'school', 'park', 'system', 'year', 'round', 'community', 'activity', 'also', 'make', 'quality', 'life', 'enjoyable', 'rewarding']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9237\n",
            "    After Stopword Removal: ['common', 'reason', 'detailed', 'history', 'specifi', '-', 'cally', 'include', 'asking', 'relevance', 'injury', ',', 'fracture', 'one', 'methods', 'require', 'high', 'glucocorticoid', 'doses', '.']\n",
            "    After Regex: ['common', 'reason', 'detailed', 'history', 'specifi', 'cally', 'include', 'asking', 'relevance', 'injury', 'fracture', 'one', 'methods', 'require', 'high', 'glucocorticoid', 'doses']\n",
            "    After Lemmatization: ['common', 'reason', 'detailed', 'history', 'specifi', 'cally', 'include', 'asking', 'relevance', 'injury', 'fracture', 'one', 'method', 'require', 'high', 'glucocorticoid', 'dos']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9238\n",
            "    After Stopword Removal: ['dec', '.15,', 'ncaa', 'suspended', 'plans', 'give', 'athletes', '$2,000', 'stipend', 'living', 'costs', 'covered', 'scholarships', 'least', '125', 'schools', 'objected', '.']\n",
            "    After Regex: ['dec', 'ncaa', 'suspended', 'plans', 'give', 'athletes', 'stipend', 'living', 'costs', 'covered', 'scholarships', 'least', 'schools', 'objected']\n",
            "    After Lemmatization: ['dec', 'ncaa', 'suspended', 'plan', 'give', 'athlete', 'stipend', 'living', 'cost', 'covered', 'scholarship', 'least', 'school', 'objected']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9239\n",
            "    After Stopword Removal: ['focus', 'many', 'questions', 'centered', 'around', 'honey', 'actually', 'honey', 'eat', '.']\n",
            "    After Regex: ['focus', 'many', 'questions', 'centered', 'around', 'honey', 'actually', 'honey', 'eat']\n",
            "    After Lemmatization: ['focus', 'many', 'question', 'centered', 'around', 'honey', 'actually', 'honey', 'eat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9240\n",
            "    After Stopword Removal: ['simple', 'lesson', ',', 'written', 'teach', 'kindergärtners', 'god', 'always', 'always', 'comfort', ',', 'good', 'reminder', 'us', 'adults', '.']\n",
            "    After Regex: ['simple', 'lesson', 'written', 'teach', 'kindergrtners', 'god', 'always', 'always', 'comfort', 'good', 'reminder', 'us', 'adults']\n",
            "    After Lemmatization: ['simple', 'lesson', 'written', 'teach', 'kindergrtners', 'god', 'always', 'always', 'comfort', 'good', 'reminder', 'u', 'adult']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9241\n",
            "    After Stopword Removal: ['safety', 'assured', 'metals', 'remain', 'site', 'cannot', 'lent', '.']\n",
            "    After Regex: ['safety', 'assured', 'metals', 'remain', 'site', 'cannot', 'lent']\n",
            "    After Lemmatization: ['safety', 'assured', 'metal', 'remain', 'site', 'cannot', 'lent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9242\n",
            "    After Stopword Removal: ['firm', 'co', '-', 'founded', 'whole', 'foods', 'market', 'chairman', 'john', 'elstrott', 'talks', 'top', 'consumer', 'healthcare', 'firms', 'developing', 'medical', 'foods', 'containing', 'novel', 'ingredient', 'claimed', 'improve', 'blood', 'glucose', 'control', 'metabolic', 'function', 'pre', '-', 'diabetics', 'type', '2', 'diabetics', '.']\n",
            "    After Regex: ['firm', 'co', 'founded', 'whole', 'foods', 'market', 'chairman', 'john', 'elstrott', 'talks', 'top', 'consumer', 'healthcare', 'firms', 'developing', 'medical', 'foods', 'containing', 'novel', 'ingredient', 'claimed', 'improve', 'blood', 'glucose', 'control', 'metabolic', 'function', 'pre', 'diabetics', 'type', 'diabetics']\n",
            "    After Lemmatization: ['firm', 'co', 'founded', 'whole', 'food', 'market', 'chairman', 'john', 'elstrott', 'talk', 'top', 'consumer', 'healthcare', 'firm', 'developing', 'medical', 'food', 'containing', 'novel', 'ingredient', 'claimed', 'improve', 'blood', 'glucose', 'control', 'metabolic', 'function', 'pre', 'diabetic', 'type', 'diabetic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9243\n",
            "    After Stopword Removal: ['calculated', 'subtracting', 'total', 'loan', 'amount', 'estimated', 'value', '.']\n",
            "    After Regex: ['calculated', 'subtracting', 'total', 'loan', 'amount', 'estimated', 'value']\n",
            "    After Lemmatization: ['calculated', 'subtracting', 'total', 'loan', 'amount', 'estimated', 'value']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9244\n",
            "    After Stopword Removal: ['mounting', 'evidence', 'detectable', 'relationship', 'exists', 'extreme', 'fire', 'years', 'west', 'pacific', 'ocean', 'circulation', 'anomalies', '.']\n",
            "    After Regex: ['mounting', 'evidence', 'detectable', 'relationship', 'exists', 'extreme', 'fire', 'years', 'west', 'pacific', 'ocean', 'circulation', 'anomalies']\n",
            "    After Lemmatization: ['mounting', 'evidence', 'detectable', 'relationship', 'exists', 'extreme', 'fire', 'year', 'west', 'pacific', 'ocean', 'circulation', 'anomaly']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9245\n",
            "    After Stopword Removal: ['journal', 'british', 'studies', ',', 'vol', '.']\n",
            "    After Regex: ['journal', 'british', 'studies', 'vol']\n",
            "    After Lemmatization: ['journal', 'british', 'study', 'vol']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9246\n",
            "    After Stopword Removal: ['campus', 'vast', '—243', 'acres', '2.4', 'million', 'square', 'feet', 'clinic', 'space', '—', 'serves', 'almost', '1', 'million', 'individuals', 'year', '.']\n",
            "    After Regex: ['campus', 'vast', 'acres', 'million', 'square', 'feet', 'clinic', 'space', 'serves', 'almost', 'million', 'individuals', 'year']\n",
            "    After Lemmatization: ['campus', 'vast', 'acre', 'million', 'square', 'foot', 'clinic', 'space', 'serf', 'almost', 'million', 'individual', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9247\n",
            "    After Stopword Removal: ['immersion', 'develop', 'intelligence', 'activity', 'system', 'model', 'learning', '.']\n",
            "    After Regex: ['immersion', 'develop', 'intelligence', 'activity', 'system', 'model', 'learning']\n",
            "    After Lemmatization: ['immersion', 'develop', 'intelligence', 'activity', 'system', 'model', 'learning']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9248\n",
            "    After Stopword Removal: ['biggest', 'reason', 'touch', 'preserve', 'scientific', 'status', '.']\n",
            "    After Regex: ['biggest', 'reason', 'touch', 'preserve', 'scientific', 'status']\n",
            "    After Lemmatization: ['biggest', 'reason', 'touch', 'preserve', 'scientific', 'status']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9249\n",
            "    After Stopword Removal: ['jaws', 'well', 'adapted', 'since', 'held', 'together', 'stretchy', 'ligaments', 'enable', 'open', 'wide', 'open', 'enough', 'swallow', 'prey', 'larger', '.']\n",
            "    After Regex: ['jaws', 'well', 'adapted', 'since', 'held', 'together', 'stretchy', 'ligaments', 'enable', 'open', 'wide', 'open', 'enough', 'swallow', 'prey', 'larger']\n",
            "    After Lemmatization: ['jaw', 'well', 'adapted', 'since', 'held', 'together', 'stretchy', 'ligament', 'enable', 'open', 'wide', 'open', 'enough', 'swallow', 'prey', 'larger']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9250\n",
            "    After Stopword Removal: ['furthermore', ',', 'professorships', 'attract', 'best', 'young', 'investigators', 'field', 'benefit', 'working', 'senior', 'scientists', 'thus', 'build', 'research', 'educational', 'base', '.']\n",
            "    After Regex: ['furthermore', 'professorships', 'attract', 'best', 'young', 'investigators', 'field', 'benefit', 'working', 'senior', 'scientists', 'thus', 'build', 'research', 'educational', 'base']\n",
            "    After Lemmatization: ['furthermore', 'professorship', 'attract', 'best', 'young', 'investigator', 'field', 'benefit', 'working', 'senior', 'scientist', 'thus', 'build', 'research', 'educational', 'base']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9251\n",
            "    After Stopword Removal: ['sugar', 'gives', 'form', 'structure', ',', 'well', 'flavour', ',', 'baking', '.']\n",
            "    After Regex: ['sugar', 'gives', 'form', 'structure', 'well', 'flavour', 'baking']\n",
            "    After Lemmatization: ['sugar', 'give', 'form', 'structure', 'well', 'flavour', 'baking']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9252\n",
            "    After Stopword Removal: ['diet', 'composed', 'almost', 'exclusively', 'fish', ',', 'also', 'known', 'eat', 'caimans', ',', 'anacondas', ',', 'snakes', 'even', 'occasional', 'turtle', '(2)(6).', 'top', 'giant', 'otter', 'range', 'species', 'endemic', 'south', 'america', '(', 'except', 'chile', '),', 'east', 'andes', 'mountain', 'chain', '.']\n",
            "    After Regex: ['diet', 'composed', 'almost', 'exclusively', 'fish', 'also', 'known', 'eat', 'caimans', 'anacondas', 'snakes', 'even', 'occasional', 'turtle', 'top', 'giant', 'otter', 'range', 'species', 'endemic', 'south', 'america', 'except', 'chile', 'east', 'andes', 'mountain', 'chain']\n",
            "    After Lemmatization: ['diet', 'composed', 'almost', 'exclusively', 'fish', 'also', 'known', 'eat', 'caiman', 'anaconda', 'snake', 'even', 'occasional', 'turtle', 'top', 'giant', 'otter', 'range', 'specie', 'endemic', 'south', 'america', 'except', 'chile', 'east', 'andes', 'mountain', 'chain']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9253\n",
            "    After Stopword Removal: ['let', \"'\", 'start', 'comparing', 'switch', 'statement', 'performs', 'compared', 'simple', 'array', 'types', '.']\n",
            "    After Regex: ['let', 'start', 'comparing', 'switch', 'statement', 'performs', 'compared', 'simple', 'array', 'types']\n",
            "    After Lemmatization: ['let', 'start', 'comparing', 'switch', 'statement', 'performs', 'compared', 'simple', 'array', 'type']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9254\n",
            "    After Stopword Removal: ['borrelia', 'burgdorferi', ',', 'causative', 'agent', 'lyme', 'disease', ',', 'capable', 'forming', 'biofilm', 'vivo', 'vitro', ',', 'structure', 'well', 'known', 'resistance', 'antimicrobial', 'agents', '.']\n",
            "    After Regex: ['borrelia', 'burgdorferi', 'causative', 'agent', 'lyme', 'disease', 'capable', 'forming', 'biofilm', 'vivo', 'vitro', 'structure', 'well', 'known', 'resistance', 'antimicrobial', 'agents']\n",
            "    After Lemmatization: ['borrelia', 'burgdorferi', 'causative', 'agent', 'lyme', 'disease', 'capable', 'forming', 'biofilm', 'vivo', 'vitro', 'structure', 'well', 'known', 'resistance', 'antimicrobial', 'agent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9255\n",
            "    After Stopword Removal: ['wells', 'fargo', 'first', 'big', 'u', '.', '.', 'bank', 'report', 'fourth', '-', 'quarter', 'results', '.']\n",
            "    After Regex: ['wells', 'fargo', 'first', 'big', 'u', 'bank', 'report', 'fourth', 'quarter', 'results']\n",
            "    After Lemmatization: ['well', 'fargo', 'first', 'big', 'u', 'bank', 'report', 'fourth', 'quarter', 'result']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9256\n",
            "    After Stopword Removal: ['past', '20', 'years', ',', 'agriculture', 'developing', 'countries', 'dominated', 'gradual', 'decline', 'investment', 'shift', 'towards', 'somewhat', 'liberal', 'policy', 'environment', '.']\n",
            "    After Regex: ['past', 'years', 'agriculture', 'developing', 'countries', 'dominated', 'gradual', 'decline', 'investment', 'shift', 'towards', 'somewhat', 'liberal', 'policy', 'environment']\n",
            "    After Lemmatization: ['past', 'year', 'agriculture', 'developing', 'country', 'dominated', 'gradual', 'decline', 'investment', 'shift', 'towards', 'somewhat', 'liberal', 'policy', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9257\n",
            "    After Stopword Removal: ['however', ',', 'also', 'understand', 'everyone', 'would', 'necessary', 'funds', 'bank', 'account', '.']\n",
            "    After Regex: ['however', 'also', 'understand', 'everyone', 'would', 'necessary', 'funds', 'bank', 'account']\n",
            "    After Lemmatization: ['however', 'also', 'understand', 'everyone', 'would', 'necessary', 'fund', 'bank', 'account']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9258\n",
            "    After Stopword Removal: ['acclaimed', 'speed', ',', 'power', 'effectiveness', 'dynamic', 'immersiontm', 'method', ',', 'rosetta', 'stone', 'revolutionary', 'language', '-', 'learning', 'software', 'program', '.']\n",
            "    After Regex: ['acclaimed', 'speed', 'power', 'effectiveness', 'dynamic', 'immersiontm', 'method', 'rosetta', 'stone', 'revolutionary', 'language', 'learning', 'software', 'program']\n",
            "    After Lemmatization: ['acclaimed', 'speed', 'power', 'effectiveness', 'dynamic', 'immersiontm', 'method', 'rosetta', 'stone', 'revolutionary', 'language', 'learning', 'software', 'program']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9259\n",
            "    After Stopword Removal: ['france', 'fairly', 'accurate', 'system', 'exit', 'polling', 'permits', 'news', 'media', 'call', 'election', 'moment', 'polls', 'close', '8', 'p', '.', '.', 'local', 'time', '.']\n",
            "    After Regex: ['france', 'fairly', 'accurate', 'system', 'exit', 'polling', 'permits', 'news', 'media', 'call', 'election', 'moment', 'polls', 'close', 'p', 'local', 'time']\n",
            "    After Lemmatization: ['france', 'fairly', 'accurate', 'system', 'exit', 'polling', 'permit', 'news', 'medium', 'call', 'election', 'moment', 'poll', 'close', 'p', 'local', 'time']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9260\n",
            "    After Stopword Removal: ['journalists', 'provide', 'information', 'voters', 'need', 'evaluate', 'government', 'officials', 'candidates', 'office', '.']\n",
            "    After Regex: ['journalists', 'provide', 'information', 'voters', 'need', 'evaluate', 'government', 'officials', 'candidates', 'office']\n",
            "    After Lemmatization: ['journalist', 'provide', 'information', 'voter', 'need', 'evaluate', 'government', 'official', 'candidate', 'office']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9261\n",
            "    After Stopword Removal: ['moral', ',', 'legal', 'political', 'justification', 'collective', 'punishment', 'venezuelan', 'population', ',', 'based', 'economic', 'sanctions', '.']\n",
            "    After Regex: ['moral', 'legal', 'political', 'justification', 'collective', 'punishment', 'venezuelan', 'population', 'based', 'economic', 'sanctions']\n",
            "    After Lemmatization: ['moral', 'legal', 'political', 'justification', 'collective', 'punishment', 'venezuelan', 'population', 'based', 'economic', 'sanction']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9262\n",
            "    After Stopword Removal: ['use', 'cotton', 'cloth', 'wipe', 'grease', '(', 'however', 'small', ')', 'launder', ',', 'grease', 'ends', 'clogging', 'drains', ',', 'sewers', ',', 'waterways', '.']\n",
            "    After Regex: ['use', 'cotton', 'cloth', 'wipe', 'grease', 'however', 'small', 'launder', 'grease', 'ends', 'clogging', 'drains', 'sewers', 'waterways']\n",
            "    After Lemmatization: ['use', 'cotton', 'cloth', 'wipe', 'grease', 'however', 'small', 'launder', 'grease', 'end', 'clogging', 'drain', 'sewer', 'waterway']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9263\n",
            "    After Stopword Removal: ['governmental', 'tasks', 'included', 'completing', 'arrangements', 'british', 'treasury', 'loan', '£1,500,000', 'pay', 'accumulated', 'debt', 'canadas', 'finish', 'public', 'works', 'welland', 'canal', '.']\n",
            "    After Regex: ['governmental', 'tasks', 'included', 'completing', 'arrangements', 'british', 'treasury', 'loan', 'pay', 'accumulated', 'debt', 'canadas', 'finish', 'public', 'works', 'welland', 'canal']\n",
            "    After Lemmatization: ['governmental', 'task', 'included', 'completing', 'arrangement', 'british', 'treasury', 'loan', 'pay', 'accumulated', 'debt', 'canada', 'finish', 'public', 'work', 'welland', 'canal']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9264\n",
            "    After Stopword Removal: ['ethical', 'question', 'arise', 'involves', 'child', \"'\", 'treatment', ',', 'request', 'consultation', 'ethics', 'committee', '.']\n",
            "    After Regex: ['ethical', 'question', 'arise', 'involves', 'child', 'treatment', 'request', 'consultation', 'ethics', 'committee']\n",
            "    After Lemmatization: ['ethical', 'question', 'arise', 'involves', 'child', 'treatment', 'request', 'consultation', 'ethic', 'committee']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9265\n",
            "    After Stopword Removal: ['cause', 'blindness', ',', 'massive', 'swelling', 'appendages', 'limbs', ',', 'severe', 'malnutrition', 'anemia', '.']\n",
            "    After Regex: ['cause', 'blindness', 'massive', 'swelling', 'appendages', 'limbs', 'severe', 'malnutrition', 'anemia']\n",
            "    After Lemmatization: ['cause', 'blindness', 'massive', 'swelling', 'appendage', 'limb', 'severe', 'malnutrition', 'anemia']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9266\n",
            "    After Stopword Removal: ['whether', 'raise', 'chickens', ',', 'ducks', ',', 'emus', ',', 'quale', ',', 'people', 'always', 'interested', 'naturally', 'produced', 'eggs', '.']\n",
            "    After Regex: ['whether', 'raise', 'chickens', 'ducks', 'emus', 'quale', 'people', 'always', 'interested', 'naturally', 'produced', 'eggs']\n",
            "    After Lemmatization: ['whether', 'raise', 'chicken', 'duck', 'emu', 'quale', 'people', 'always', 'interested', 'naturally', 'produced', 'egg']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9267\n",
            "    After Stopword Removal: ['many', 'ways', ',', 'experts', 'say', ',', 'importance', 'roles', 'played', 'iowa', 'new', 'hampshire', 'increasingly', 'diminished', ',', 'primary', 'calendar', 'grown', 'compressed', 'process', 'gradually', 'become', 'nationalized', '.']\n",
            "    After Regex: ['many', 'ways', 'experts', 'say', 'importance', 'roles', 'played', 'iowa', 'new', 'hampshire', 'increasingly', 'diminished', 'primary', 'calendar', 'grown', 'compressed', 'process', 'gradually', 'become', 'nationalized']\n",
            "    After Lemmatization: ['many', 'way', 'expert', 'say', 'importance', 'role', 'played', 'iowa', 'new', 'hampshire', 'increasingly', 'diminished', 'primary', 'calendar', 'grown', 'compressed', 'process', 'gradually', 'become', 'nationalized']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9268\n",
            "    After Stopword Removal: ['us', 'school', 'work', ',', 'time', 'financial', 'resources', 'limited', '.']\n",
            "    After Regex: ['us', 'school', 'work', 'time', 'financial', 'resources', 'limited']\n",
            "    After Lemmatization: ['u', 'school', 'work', 'time', 'financial', 'resource', 'limited']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9269\n",
            "    After Stopword Removal: ['atmospheric', 'temperature', 'planet', 'would', 'range', '86', 'minus', '126', 'degrees', 'fahrenheit', '(30', 'minus', '88', 'degrees', 'celsius', ')', 'habitable', 'zone', '.']\n",
            "    After Regex: ['atmospheric', 'temperature', 'planet', 'would', 'range', 'minus', 'degrees', 'fahrenheit', 'minus', 'degrees', 'celsius', 'habitable', 'zone']\n",
            "    After Lemmatization: ['atmospheric', 'temperature', 'planet', 'would', 'range', 'minus', 'degree', 'fahrenheit', 'minus', 'degree', 'celsius', 'habitable', 'zone']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9270\n",
            "    After Stopword Removal: ['relationship', 'smoking', 'acne', 'could', 'proven', 'scientifically', '.']\n",
            "    After Regex: ['relationship', 'smoking', 'acne', 'could', 'proven', 'scientifically']\n",
            "    After Lemmatization: ['relationship', 'smoking', 'acne', 'could', 'proven', 'scientifically']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9271\n",
            "    After Stopword Removal: ['signs', 'symptoms', 'nelson', \"'\", 'syndrome', 'result', 'enlarged', 'tumour', 'increased', 'adrenocorticotropic', 'hormone', 'production', '.']\n",
            "    After Regex: ['signs', 'symptoms', 'nelson', 'syndrome', 'result', 'enlarged', 'tumour', 'increased', 'adrenocorticotropic', 'hormone', 'production']\n",
            "    After Lemmatization: ['sign', 'symptom', 'nelson', 'syndrome', 'result', 'enlarged', 'tumour', 'increased', 'adrenocorticotropic', 'hormone', 'production']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9272\n",
            "    After Stopword Removal: ['story', ',', 'like', 'poetry', 'prose', ',', 'written', 'series', 'flashbacks', 'difficult', 'follow', '.']\n",
            "    After Regex: ['story', 'like', 'poetry', 'prose', 'written', 'series', 'flashbacks', 'difficult', 'follow']\n",
            "    After Lemmatization: ['story', 'like', 'poetry', 'prose', 'written', 'series', 'flashback', 'difficult', 'follow']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9273\n",
            "    After Stopword Removal: ['perspective', 'mining', 'project', ',', 'remember', 'continuing', 'pollution', 'nearby', 'quinsam', 'coal', 'litany', 'environmental', 'headaches', 'created', 'watershed', '.']\n",
            "    After Regex: ['perspective', 'mining', 'project', 'remember', 'continuing', 'pollution', 'nearby', 'quinsam', 'coal', 'litany', 'environmental', 'headaches', 'created', 'watershed']\n",
            "    After Lemmatization: ['perspective', 'mining', 'project', 'remember', 'continuing', 'pollution', 'nearby', 'quinsam', 'coal', 'litany', 'environmental', 'headache', 'created', 'watershed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9274\n",
            "    After Stopword Removal: ['went', 'convince', 'fda', 'allow', 'toxic', 'oil', '-', 'products', 'almost', 'everything', 'including', 'food', 'water', 'convincing', 'regulatory', 'agencies', 'things', 'allowable', 'amounts', '.']\n",
            "    After Regex: ['went', 'convince', 'fda', 'allow', 'toxic', 'oil', 'products', 'almost', 'everything', 'including', 'food', 'water', 'convincing', 'regulatory', 'agencies', 'things', 'allowable', 'amounts']\n",
            "    After Lemmatization: ['went', 'convince', 'fda', 'allow', 'toxic', 'oil', 'product', 'almost', 'everything', 'including', 'food', 'water', 'convincing', 'regulatory', 'agency', 'thing', 'allowable', 'amount']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9275\n",
            "    After Stopword Removal: ['prize', 'awarded', 'every', 'year', 'primarily', 'young', 'scientists', 'dlr', 'honor', 'outstanding', 'scientific', 'achievements', 'must', 'documented', 'refereed', 'scientific', 'journals', '.']\n",
            "    After Regex: ['prize', 'awarded', 'every', 'year', 'primarily', 'young', 'scientists', 'dlr', 'honor', 'outstanding', 'scientific', 'achievements', 'must', 'documented', 'refereed', 'scientific', 'journals']\n",
            "    After Lemmatization: ['prize', 'awarded', 'every', 'year', 'primarily', 'young', 'scientist', 'dlr', 'honor', 'outstanding', 'scientific', 'achievement', 'must', 'documented', 'refereed', 'scientific', 'journal']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9276\n",
            "    After Stopword Removal: [',', 'studies', ',', 'different', 'diet', 'stands', '.']\n",
            "    After Regex: ['studies', 'different', 'diet', 'stands']\n",
            "    After Lemmatization: ['study', 'different', 'diet', 'stand']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9277\n",
            "    After Stopword Removal: ['main', 'message', 'smoker', ':', 'vigilant', 'lung', 'cancer', 'screening', '.']\n",
            "    After Regex: ['main', 'message', 'smoker', 'vigilant', 'lung', 'cancer', 'screening']\n",
            "    After Lemmatization: ['main', 'message', 'smoker', 'vigilant', 'lung', 'cancer', 'screening']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9278\n",
            "    After Stopword Removal: ['yet', '(1', 'b', ')', 'must', 'also', 'true', ',', 'central', 'tenet', 'christian', 'theology', 'christ', 'suffered', 'cross', '.']\n",
            "    After Regex: ['yet', 'b', 'must', 'also', 'true', 'central', 'tenet', 'christian', 'theology', 'christ', 'suffered', 'cross']\n",
            "    After Lemmatization: ['yet', 'b', 'must', 'also', 'true', 'central', 'tenet', 'christian', 'theology', 'christ', 'suffered', 'cross']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9279\n",
            "    After Stopword Removal: ['indoor', 'environmental', 'quality', 'credit', 'category', 'promotes', 'strategies', 'improve', 'indoor', 'air', 'well', 'providing', 'access', 'natural', 'daylight', 'views', 'improving', 'acoustics', '.']\n",
            "    After Regex: ['indoor', 'environmental', 'quality', 'credit', 'category', 'promotes', 'strategies', 'improve', 'indoor', 'air', 'well', 'providing', 'access', 'natural', 'daylight', 'views', 'improving', 'acoustics']\n",
            "    After Lemmatization: ['indoor', 'environmental', 'quality', 'credit', 'category', 'promotes', 'strategy', 'improve', 'indoor', 'air', 'well', 'providing', 'access', 'natural', 'daylight', 'view', 'improving', 'acoustic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9280\n",
            "    After Stopword Removal: ['nasya', 'treatment', 'refers', 'ayurvedic', 'therapy', 'includes', 'instillation', 'herbal', 'oils', ',', 'juices', 'powders', 'nasal', 'route', '.']\n",
            "    After Regex: ['nasya', 'treatment', 'refers', 'ayurvedic', 'therapy', 'includes', 'instillation', 'herbal', 'oils', 'juices', 'powders', 'nasal', 'route']\n",
            "    After Lemmatization: ['nasya', 'treatment', 'refers', 'ayurvedic', 'therapy', 'includes', 'instillation', 'herbal', 'oil', 'juice', 'powder', 'nasal', 'route']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9281\n",
            "    After Stopword Removal: ['although', 'widely', 'acknowledged', 'winning', 'sympathies', 'local', 'population', 'key', 'element', 'successful', 'counterinsurgency', 'strategy', ',', 'difficult', 'achieve', 'local', 'military', 'fully', 'respected', '.']\n",
            "    After Regex: ['although', 'widely', 'acknowledged', 'winning', 'sympathies', 'local', 'population', 'key', 'element', 'successful', 'counterinsurgency', 'strategy', 'difficult', 'achieve', 'local', 'military', 'fully', 'respected']\n",
            "    After Lemmatization: ['although', 'widely', 'acknowledged', 'winning', 'sympathy', 'local', 'population', 'key', 'element', 'successful', 'counterinsurgency', 'strategy', 'difficult', 'achieve', 'local', 'military', 'fully', 'respected']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9282\n",
            "    After Stopword Removal: [\"'\", 'probably', 'interpreting', 'three', 'rs', 'kids', 'read', 'books', ',', 'write', 'essays', 'perform', 'pages', 'mathematics', '.']\n",
            "    After Regex: ['probably', 'interpreting', 'three', 'rs', 'kids', 'read', 'books', 'write', 'essays', 'perform', 'pages', 'mathematics']\n",
            "    After Lemmatization: ['probably', 'interpreting', 'three', 'r', 'kid', 'read', 'book', 'write', 'essay', 'perform', 'page', 'mathematics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9283\n",
            "    After Stopword Removal: ['common', 'side', 'effects', 'include', 'discomfort', 'injection', 'site', 'chance', 'injury', 'oral', 'tissues', 'due', 'loss', 'sensation', 'numb', '.']\n",
            "    After Regex: ['common', 'side', 'effects', 'include', 'discomfort', 'injection', 'site', 'chance', 'injury', 'oral', 'tissues', 'due', 'loss', 'sensation', 'numb']\n",
            "    After Lemmatization: ['common', 'side', 'effect', 'include', 'discomfort', 'injection', 'site', 'chance', 'injury', 'oral', 'tissue', 'due', 'loss', 'sensation', 'numb']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9284\n",
            "    After Stopword Removal: ['going', 'water', 'sweating', 'lot', ',', 'chemical', 'blocker', 'would', 'probably', 'best', 'choice', '.']\n",
            "    After Regex: ['going', 'water', 'sweating', 'lot', 'chemical', 'blocker', 'would', 'probably', 'best', 'choice']\n",
            "    After Lemmatization: ['going', 'water', 'sweating', 'lot', 'chemical', 'blocker', 'would', 'probably', 'best', 'choice']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9285\n",
            "    After Stopword Removal: ['however', ',', 'highly', 'toxic', 'chemical', ',', 'known', 'cause', 'acute', 'respiratory', 'distress', 'syndrome', '(', 'ards', ')', 'parkinson', \"'\", 'disease', ',', 'among', 'complications', '.']\n",
            "    After Regex: ['however', 'highly', 'toxic', 'chemical', 'known', 'cause', 'acute', 'respiratory', 'distress', 'syndrome', 'ards', 'parkinson', 'disease', 'among', 'complications']\n",
            "    After Lemmatization: ['however', 'highly', 'toxic', 'chemical', 'known', 'cause', 'acute', 'respiratory', 'distress', 'syndrome', 'ards', 'parkinson', 'disease', 'among', 'complication']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9286\n",
            "    After Stopword Removal: ['louisiana', 'one', 'seven', 'states', 'register', 'license', 'family', 'child', 'care', 'homes', '.']\n",
            "    After Regex: ['louisiana', 'one', 'seven', 'states', 'register', 'license', 'family', 'child', 'care', 'homes']\n",
            "    After Lemmatization: ['louisiana', 'one', 'seven', 'state', 'register', 'license', 'family', 'child', 'care', 'home']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9287\n",
            "    After Stopword Removal: ['14%', 'ceramics', ',8%', 'ceramic', 'ball', 'bearing', ',', '1%', 'bearing', 'balls', '.']\n",
            "    After Regex: ['ceramics', 'ceramic', 'ball', 'bearing', 'bearing', 'balls']\n",
            "    After Lemmatization: ['ceramic', 'ceramic', 'ball', 'bearing', 'bearing', 'ball']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9288\n",
            "    After Stopword Removal: ['also', 'highlights', 'physicality', 'work', 'often', 'exhibited', 'large', '-', 'scale', 'installations', '.']\n",
            "    After Regex: ['also', 'highlights', 'physicality', 'work', 'often', 'exhibited', 'large', 'scale', 'installations']\n",
            "    After Lemmatization: ['also', 'highlight', 'physicality', 'work', 'often', 'exhibited', 'large', 'scale', 'installation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9289\n",
            "    After Stopword Removal: ['hill', 'born', 'iredell', 'county', ',', 'north', 'carolina', ',', '25', 'march', '1811,', 'grandson', 'scots', '-', 'irish', 'immigrants', '.']\n",
            "    After Regex: ['hill', 'born', 'iredell', 'county', 'north', 'carolina', 'march', 'grandson', 'scots', 'irish', 'immigrants']\n",
            "    After Lemmatization: ['hill', 'born', 'iredell', 'county', 'north', 'carolina', 'march', 'grandson', 'scot', 'irish', 'immigrant']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9290\n",
            "    After Stopword Removal: ['apply', 'stroke', 'paint', 'piece', 'paper', 'see', 'single', 'unified', 'revelating', 'vision', 'water', 'paint', 'particle', 'dissolved', ',', 'surface', 'tension', 'makes', 'water', 'stick', 'brush', 'strands', ',', 'hydrogen', 'bonding', 'dye', 'molecules', 'water', ',', 'fibers', 'dead', 'plant', 'make', 'paper', 'lends', 'unique', 'texture', 'medium', 'amongst', 'many', 'different', 'possible', 'media', 'could', 'used', 'render', 'imagine', 'mind', 'day', '-', 'image', 'conjured', 'another', 'moment', 'saw', 'walking', 'bridge', 'building', 'towering', 'around', '.']\n",
            "    After Regex: ['apply', 'stroke', 'paint', 'piece', 'paper', 'see', 'single', 'unified', 'revelating', 'vision', 'water', 'paint', 'particle', 'dissolved', 'surface', 'tension', 'makes', 'water', 'stick', 'brush', 'strands', 'hydrogen', 'bonding', 'dye', 'molecules', 'water', 'fibers', 'dead', 'plant', 'make', 'paper', 'lends', 'unique', 'texture', 'medium', 'amongst', 'many', 'different', 'possible', 'media', 'could', 'used', 'render', 'imagine', 'mind', 'day', 'image', 'conjured', 'another', 'moment', 'saw', 'walking', 'bridge', 'building', 'towering', 'around']\n",
            "    After Lemmatization: ['apply', 'stroke', 'paint', 'piece', 'paper', 'see', 'single', 'unified', 'revelating', 'vision', 'water', 'paint', 'particle', 'dissolved', 'surface', 'tension', 'make', 'water', 'stick', 'brush', 'strand', 'hydrogen', 'bonding', 'dye', 'molecule', 'water', 'fiber', 'dead', 'plant', 'make', 'paper', 'lends', 'unique', 'texture', 'medium', 'amongst', 'many', 'different', 'possible', 'medium', 'could', 'used', 'render', 'imagine', 'mind', 'day', 'image', 'conjured', 'another', 'moment', 'saw', 'walking', 'bridge', 'building', 'towering', 'around']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9291\n",
            "    After Stopword Removal: ['implication', 'vitamin', 'outcome', 'art', '(', 'clinical', 'pregnancy', 'live', 'birth', ')', 'examined', 'numerous', 'studies', '.']\n",
            "    After Regex: ['implication', 'vitamin', 'outcome', 'art', 'clinical', 'pregnancy', 'live', 'birth', 'examined', 'numerous', 'studies']\n",
            "    After Lemmatization: ['implication', 'vitamin', 'outcome', 'art', 'clinical', 'pregnancy', 'live', 'birth', 'examined', 'numerous', 'study']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9292\n",
            "    After Stopword Removal: ['japanese', 'street', 'art', 'influenced', 'impressionists', 'enthusiasts', '.']\n",
            "    After Regex: ['japanese', 'street', 'art', 'influenced', 'impressionists', 'enthusiasts']\n",
            "    After Lemmatization: ['japanese', 'street', 'art', 'influenced', 'impressionist', 'enthusiast']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9293\n",
            "    After Stopword Removal: ['talk', 'bombing', 'munitions', 'factories', ',', 'disrupt', 'mobilisation', ',', 'aircraft', 'factories', 'help', 'win', 'air', 'war', ',', 'often', 'seen', 'wasteful', 'diversions', 'main', 'effort', '.']\n",
            "    After Regex: ['talk', 'bombing', 'munitions', 'factories', 'disrupt', 'mobilisation', 'aircraft', 'factories', 'help', 'win', 'air', 'war', 'often', 'seen', 'wasteful', 'diversions', 'main', 'effort']\n",
            "    After Lemmatization: ['talk', 'bombing', 'munition', 'factory', 'disrupt', 'mobilisation', 'aircraft', 'factory', 'help', 'win', 'air', 'war', 'often', 'seen', 'wasteful', 'diversion', 'main', 'effort']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9294\n",
            "    After Stopword Removal: ['speech', 'timing', 'working', 'memory', 'profoundly', 'deaf', 'children', 'cochlear', 'implantation', '.']\n",
            "    After Regex: ['speech', 'timing', 'working', 'memory', 'profoundly', 'deaf', 'children', 'cochlear', 'implantation']\n",
            "    After Lemmatization: ['speech', 'timing', 'working', 'memory', 'profoundly', 'deaf', 'child', 'cochlear', 'implantation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9295\n",
            "    After Stopword Removal: ['discussion', 'concerned', 'cost', 'demolition', 'disposal', 'materials', ',', 'completion', 'environmental', 'assessment', 'worksheet', ',', 'asbestos', 'study', 'added', 'cost', 'new', 'building', '.']\n",
            "    After Regex: ['discussion', 'concerned', 'cost', 'demolition', 'disposal', 'materials', 'completion', 'environmental', 'assessment', 'worksheet', 'asbestos', 'study', 'added', 'cost', 'new', 'building']\n",
            "    After Lemmatization: ['discussion', 'concerned', 'cost', 'demolition', 'disposal', 'material', 'completion', 'environmental', 'assessment', 'worksheet', 'asbestos', 'study', 'added', 'cost', 'new', 'building']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9296\n",
            "    After Stopword Removal: ['matters', 'may', 'submerged', 'arcane', 'history', 'fail', 'appear', 'presidential', 'teleprompters', ',', 'fair', 'read', 'john', 'adam', \"'\", 'sense', 'islam', 'john', 'quincy', 'adams', 'distilled', ':', 'precept', 'koran', ',', 'perpetual', 'war', 'deny', ',', 'mahomet', 'prophet', 'god', '.']\n",
            "    After Regex: ['matters', 'may', 'submerged', 'arcane', 'history', 'fail', 'appear', 'presidential', 'teleprompters', 'fair', 'read', 'john', 'adam', 'sense', 'islam', 'john', 'quincy', 'adams', 'distilled', 'precept', 'koran', 'perpetual', 'war', 'deny', 'mahomet', 'prophet', 'god']\n",
            "    After Lemmatization: ['matter', 'may', 'submerged', 'arcane', 'history', 'fail', 'appear', 'presidential', 'teleprompter', 'fair', 'read', 'john', 'adam', 'sense', 'islam', 'john', 'quincy', 'adam', 'distilled', 'precept', 'koran', 'perpetual', 'war', 'deny', 'mahomet', 'prophet', 'god']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9297\n",
            "    After Stopword Removal: ['starts', 'training', 'teachers', 'theory', ',', 'conditions', 'learning', ',', 'concern', 'dispositions', 'bring', 'job', ',', '.']\n",
            "    After Regex: ['starts', 'training', 'teachers', 'theory', 'conditions', 'learning', 'concern', 'dispositions', 'bring', 'job']\n",
            "    After Lemmatization: ['start', 'training', 'teacher', 'theory', 'condition', 'learning', 'concern', 'disposition', 'bring', 'job']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9298\n",
            "    After Stopword Removal: ['reason', 'tax', 'dollars', 'fund', 'projects', ',', 'especially', 'test', 'tissues', 'could', 'used', 'treatments', 'already', 'created', '.']\n",
            "    After Regex: ['reason', 'tax', 'dollars', 'fund', 'projects', 'especially', 'test', 'tissues', 'could', 'used', 'treatments', 'already', 'created']\n",
            "    After Lemmatization: ['reason', 'tax', 'dollar', 'fund', 'project', 'especially', 'test', 'tissue', 'could', 'used', 'treatment', 'already', 'created']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9299\n",
            "    After Stopword Removal: ['even', 'uncommon', 'see', 'siblings', 'passing', 'different', 'variants', 'original', 'surname', '.']\n",
            "    After Regex: ['even', 'uncommon', 'see', 'siblings', 'passing', 'different', 'variants', 'original', 'surname']\n",
            "    After Lemmatization: ['even', 'uncommon', 'see', 'sibling', 'passing', 'different', 'variant', 'original', 'surname']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9300\n",
            "    After Stopword Removal: ['undertaking', 'included', 'preserving', 'coating', 'freeboard', ',', 'main', 'deck', ',', 'topside', '100', 'interior', 'tanks', ',', 'along', 'prepping', 'maintaining', 'underwater', 'hull', 'surface', '.']\n",
            "    After Regex: ['undertaking', 'included', 'preserving', 'coating', 'freeboard', 'main', 'deck', 'topside', 'interior', 'tanks', 'along', 'prepping', 'maintaining', 'underwater', 'hull', 'surface']\n",
            "    After Lemmatization: ['undertaking', 'included', 'preserving', 'coating', 'freeboard', 'main', 'deck', 'topside', 'interior', 'tank', 'along', 'prepping', 'maintaining', 'underwater', 'hull', 'surface']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9301\n",
            "    After Stopword Removal: ['patterns', 'identification', 'done', 'incredible', 'speed', ',', 'storage', 'retrieval', 'information', '.']\n",
            "    After Regex: ['patterns', 'identification', 'done', 'incredible', 'speed', 'storage', 'retrieval', 'information']\n",
            "    After Lemmatization: ['pattern', 'identification', 'done', 'incredible', 'speed', 'storage', 'retrieval', 'information']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9302\n",
            "    After Stopword Removal: ['see', ',', 'children', 'intelligent', ',', 'parents', 'must', 'explain', 'many', 'whys', 'things', '.']\n",
            "    After Regex: ['see', 'children', 'intelligent', 'parents', 'must', 'explain', 'many', 'whys', 'things']\n",
            "    After Lemmatization: ['see', 'child', 'intelligent', 'parent', 'must', 'explain', 'many', 'why', 'thing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9303\n",
            "    After Stopword Removal: ['coughing', 'natural', 'reflex', 'protects', 'lungs', '.']\n",
            "    After Regex: ['coughing', 'natural', 'reflex', 'protects', 'lungs']\n",
            "    After Lemmatization: ['coughing', 'natural', 'reflex', 'protects', 'lung']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9304\n",
            "    After Stopword Removal: ['fundamental', 'premise', 'every', 'state', 'complete', 'exclusive', 'sovereignty', 'airspace', 'territory', 'traced', 'convention', 'international', 'civil', 'aviation', '(', 'icao', ',1997).']\n",
            "    After Regex: ['fundamental', 'premise', 'every', 'state', 'complete', 'exclusive', 'sovereignty', 'airspace', 'territory', 'traced', 'convention', 'international', 'civil', 'aviation', 'icao']\n",
            "    After Lemmatization: ['fundamental', 'premise', 'every', 'state', 'complete', 'exclusive', 'sovereignty', 'airspace', 'territory', 'traced', 'convention', 'international', 'civil', 'aviation', 'icao']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9305\n",
            "    After Stopword Removal: ['invoked', 'alamo', ':', 'mere', 'handful', 'freedom', '-', 'loving', 'men', 'standing', 'mexican', 'hordes', ',', 'knowing', 'faced', 'certain', 'death', ',', \"'\", 'carved', 'names', 'history', 'time', ',', 'got', 'ol', \"'\", 'general', 'santa', 'anna', 'thought', \"'\", 'stumbled', 'nest', 'stinging', 'scorpions', 'bumblebees', '.']\n",
            "    After Regex: ['invoked', 'alamo', 'mere', 'handful', 'freedom', 'loving', 'men', 'standing', 'mexican', 'hordes', 'knowing', 'faced', 'certain', 'death', 'carved', 'names', 'history', 'time', 'got', 'ol', 'general', 'santa', 'anna', 'thought', 'stumbled', 'nest', 'stinging', 'scorpions', 'bumblebees']\n",
            "    After Lemmatization: ['invoked', 'alamo', 'mere', 'handful', 'freedom', 'loving', 'men', 'standing', 'mexican', 'horde', 'knowing', 'faced', 'certain', 'death', 'carved', 'name', 'history', 'time', 'got', 'ol', 'general', 'santa', 'anna', 'thought', 'stumbled', 'nest', 'stinging', 'scorpion', 'bumblebee']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9306\n",
            "    After Stopword Removal: ['primary', 'reason', 'lower', 'esophageal', 'sphincter', '(', 'les', ')', \"'\", 'functioning', 'properly', '.']\n",
            "    After Regex: ['primary', 'reason', 'lower', 'esophageal', 'sphincter', 'les', 'functioning', 'properly']\n",
            "    After Lemmatization: ['primary', 'reason', 'lower', 'esophageal', 'sphincter', 'le', 'functioning', 'properly']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9307\n",
            "    After Stopword Removal: ['cultivation', 'beans', 'lamon', 'highlands', 'production', 'piave', 'cheese', 'dolomites', 'important', 'belluno', \"'\", 'economy', '.']\n",
            "    After Regex: ['cultivation', 'beans', 'lamon', 'highlands', 'production', 'piave', 'cheese', 'dolomites', 'important', 'belluno', 'economy']\n",
            "    After Lemmatization: ['cultivation', 'bean', 'lamon', 'highland', 'production', 'piave', 'cheese', 'dolomite', 'important', 'belluno', 'economy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9308\n",
            "    After Stopword Removal: ['meanwhile', ',', 'moscow', 'region', 'prosecutors', 'agreed', 'request', 'greenpeace', 'russia', 'examine', 'legality', 'deforestation', '.']\n",
            "    After Regex: ['meanwhile', 'moscow', 'region', 'prosecutors', 'agreed', 'request', 'greenpeace', 'russia', 'examine', 'legality', 'deforestation']\n",
            "    After Lemmatization: ['meanwhile', 'moscow', 'region', 'prosecutor', 'agreed', 'request', 'greenpeace', 'russia', 'examine', 'legality', 'deforestation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9309\n",
            "    After Stopword Removal: ['america', \"'\", 'energy', 'transition', 'nonfossil', 'energies', 'promises', 'long', ',', 'arduous', 'process', '.']\n",
            "    After Regex: ['america', 'energy', 'transition', 'nonfossil', 'energies', 'promises', 'long', 'arduous', 'process']\n",
            "    After Lemmatization: ['america', 'energy', 'transition', 'nonfossil', 'energy', 'promise', 'long', 'arduous', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9310\n",
            "    After Stopword Removal: ['temperature', 'constant', 'throughout', 'year', ',', 'lowland', 'range', 'cool', '23', 'c', 'evening', 'early', 'morning', '33', 'c', 'shade', 'day', '.']\n",
            "    After Regex: ['temperature', 'constant', 'throughout', 'year', 'lowland', 'range', 'cool', 'c', 'evening', 'early', 'morning', 'c', 'shade', 'day']\n",
            "    After Lemmatization: ['temperature', 'constant', 'throughout', 'year', 'lowland', 'range', 'cool', 'c', 'evening', 'early', 'morning', 'c', 'shade', 'day']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9311\n",
            "    After Stopword Removal: ['editorial', 'page', 'president', 'obama', 'declared', 'denying', 'gay', 'people', 'right', 'wed', 'unfair', 'also', 'legally', 'unsupportable', ',', 'urgent', 'question', 'translate', 'words', 'action', '.']\n",
            "    After Regex: ['editorial', 'page', 'president', 'obama', 'declared', 'denying', 'gay', 'people', 'right', 'wed', 'unfair', 'also', 'legally', 'unsupportable', 'urgent', 'question', 'translate', 'words', 'action']\n",
            "    After Lemmatization: ['editorial', 'page', 'president', 'obama', 'declared', 'denying', 'gay', 'people', 'right', 'wed', 'unfair', 'also', 'legally', 'unsupportable', 'urgent', 'question', 'translate', 'word', 'action']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9312\n",
            "    After Stopword Removal: ['confucius', 'cares', 'poverty', ',', 'squalor', 'misery', 'people', 'china', '.']\n",
            "    After Regex: ['confucius', 'cares', 'poverty', 'squalor', 'misery', 'people', 'china']\n",
            "    After Lemmatization: ['confucius', 'care', 'poverty', 'squalor', 'misery', 'people', 'china']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9313\n",
            "    After Stopword Removal: ['health', 'professionals', 'want', 'know', 'health', 'risks', 'diseases', 'increased', 'current', 'environment', ',', 'best', 'advise', 'patients', ',', 'stay', 'informed', 'emerging', 'disease', 'trends', 'working', 'field', '.']\n",
            "    After Regex: ['health', 'professionals', 'want', 'know', 'health', 'risks', 'diseases', 'increased', 'current', 'environment', 'best', 'advise', 'patients', 'stay', 'informed', 'emerging', 'disease', 'trends', 'working', 'field']\n",
            "    After Lemmatization: ['health', 'professional', 'want', 'know', 'health', 'risk', 'disease', 'increased', 'current', 'environment', 'best', 'advise', 'patient', 'stay', 'informed', 'emerging', 'disease', 'trend', 'working', 'field']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9314\n",
            "    After Stopword Removal: ['active', 'community', 'community', 'interact', 'members', 'discussion', 'malaysia', 'students', '.']\n",
            "    After Regex: ['active', 'community', 'community', 'interact', 'members', 'discussion', 'malaysia', 'students']\n",
            "    After Lemmatization: ['active', 'community', 'community', 'interact', 'member', 'discussion', 'malaysia', 'student']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9315\n",
            "    After Stopword Removal: ['therefore', ',', 'last', '5', 'years', ',', 'many', 'african', 'countries', 'invested', 'financial', 'market', 'infrastructures', '.']\n",
            "    After Regex: ['therefore', 'last', 'years', 'many', 'african', 'countries', 'invested', 'financial', 'market', 'infrastructures']\n",
            "    After Lemmatization: ['therefore', 'last', 'year', 'many', 'african', 'country', 'invested', 'financial', 'market', 'infrastructure']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9316\n",
            "    After Stopword Removal: ['companies', 'cannot', 'convince', 'fda', 'safety', 'drug', ',', 'extremely', 'unlikely', 'agency', 'grant', 'approval', '.']\n",
            "    After Regex: ['companies', 'cannot', 'convince', 'fda', 'safety', 'drug', 'extremely', 'unlikely', 'agency', 'grant', 'approval']\n",
            "    After Lemmatization: ['company', 'cannot', 'convince', 'fda', 'safety', 'drug', 'extremely', 'unlikely', 'agency', 'grant', 'approval']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9317\n",
            "    After Stopword Removal: ['entails', 'people', 'upgrading', 'homes', '—', 'communities', '—', 'feathering', 'nests', 'environmentally', 'aware', 'fashion', ',', 'installing', 'features', 'slash', 'energy', 'consumption', '.']\n",
            "    After Regex: ['entails', 'people', 'upgrading', 'homes', 'communities', 'feathering', 'nests', 'environmentally', 'aware', 'fashion', 'installing', 'features', 'slash', 'energy', 'consumption']\n",
            "    After Lemmatization: ['entail', 'people', 'upgrading', 'home', 'community', 'feathering', 'nest', 'environmentally', 'aware', 'fashion', 'installing', 'feature', 'slash', 'energy', 'consumption']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9318\n",
            "    After Stopword Removal: ['two', 'major', 'candidates', 'president', ',', 'pat', 'buchanan', 'ralph', 'nader', ',', 'excluded', 'debates', 'receive', 'public', 'monies', ',', 'spite', 'quasi', '-', 'official', 'debate', 'commission', \"'\", 'protestations', 'contrary', 'respective', 'parties', 'systematically', 'obstructed', 'restrictive', 'ballot', 'access', 'laws', ',', 'impose', 'nearly', 'impossible', 'conditions', 'new', 'political', 'parties', '.']\n",
            "    After Regex: ['two', 'major', 'candidates', 'president', 'pat', 'buchanan', 'ralph', 'nader', 'excluded', 'debates', 'receive', 'public', 'monies', 'spite', 'quasi', 'official', 'debate', 'commission', 'protestations', 'contrary', 'respective', 'parties', 'systematically', 'obstructed', 'restrictive', 'ballot', 'access', 'laws', 'impose', 'nearly', 'impossible', 'conditions', 'new', 'political', 'parties']\n",
            "    After Lemmatization: ['two', 'major', 'candidate', 'president', 'pat', 'buchanan', 'ralph', 'nader', 'excluded', 'debate', 'receive', 'public', 'monies', 'spite', 'quasi', 'official', 'debate', 'commission', 'protestation', 'contrary', 'respective', 'party', 'systematically', 'obstructed', 'restrictive', 'ballot', 'access', 'law', 'impose', 'nearly', 'impossible', 'condition', 'new', 'political', 'party']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9319\n",
            "    After Stopword Removal: ['tend', 'make', 'almost', 'much', 'fetish', ',', 'last', 'century', ',', 'french', 'german', 'nobles', 'made', 'chase', 'stag', ',', 'carried', 'hunting', 'game', '-', 'preserving', 'point', 'ruinous', 'national', 'life', '.']\n",
            "    After Regex: ['tend', 'make', 'almost', 'much', 'fetish', 'last', 'century', 'french', 'german', 'nobles', 'made', 'chase', 'stag', 'carried', 'hunting', 'game', 'preserving', 'point', 'ruinous', 'national', 'life']\n",
            "    After Lemmatization: ['tend', 'make', 'almost', 'much', 'fetish', 'last', 'century', 'french', 'german', 'noble', 'made', 'chase', 'stag', 'carried', 'hunting', 'game', 'preserving', 'point', 'ruinous', 'national', 'life']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9320\n",
            "    After Stopword Removal: ['children', 'reached', 'age', 'reason', 'brought', 'faith', 'certainly', 'understand', 'sense', 'belonging', '.']\n",
            "    After Regex: ['children', 'reached', 'age', 'reason', 'brought', 'faith', 'certainly', 'understand', 'sense', 'belonging']\n",
            "    After Lemmatization: ['child', 'reached', 'age', 'reason', 'brought', 'faith', 'certainly', 'understand', 'sense', 'belonging']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9321\n",
            "    After Stopword Removal: ['growing', 'postwar', 'japan', ',', 'surrounded', 'childhood', 'mortality', '(', 'including', 'serious', 'illness', ')', 'legacy', 'nuclear', 'destruction', ',', 'informed', 'work', '.']\n",
            "    After Regex: ['growing', 'postwar', 'japan', 'surrounded', 'childhood', 'mortality', 'including', 'serious', 'illness', 'legacy', 'nuclear', 'destruction', 'informed', 'work']\n",
            "    After Lemmatization: ['growing', 'postwar', 'japan', 'surrounded', 'childhood', 'mortality', 'including', 'serious', 'illness', 'legacy', 'nuclear', 'destruction', 'informed', 'work']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9322\n",
            "    After Stopword Removal: ['denver', 'solar', 'water', 'heaters', 'work', 'solar', 'water', 'heater', 'relatively', 'simple', 'device', '.']\n",
            "    After Regex: ['denver', 'solar', 'water', 'heaters', 'work', 'solar', 'water', 'heater', 'relatively', 'simple', 'device']\n",
            "    After Lemmatization: ['denver', 'solar', 'water', 'heater', 'work', 'solar', 'water', 'heater', 'relatively', 'simple', 'device']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9323\n",
            "    After Stopword Removal: ['location', 'helps', 'keep', 'predators', 'bay', ',', '!']\n",
            "    After Regex: ['location', 'helps', 'keep', 'predators', 'bay']\n",
            "    After Lemmatization: ['location', 'help', 'keep', 'predator', 'bay']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9324\n",
            "    After Stopword Removal: ['couples', 'yoga', 'therapy', 'couples', 'yoga', 'medium', 'created', 'build', 'stronger', 'communication', 'intimacy', 'human', 'beings', 'relationship', '.']\n",
            "    After Regex: ['couples', 'yoga', 'therapy', 'couples', 'yoga', 'medium', 'created', 'build', 'stronger', 'communication', 'intimacy', 'human', 'beings', 'relationship']\n",
            "    After Lemmatization: ['couple', 'yoga', 'therapy', 'couple', 'yoga', 'medium', 'created', 'build', 'stronger', 'communication', 'intimacy', 'human', 'being', 'relationship']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9325\n",
            "    After Stopword Removal: ['died', '1890,', 'swept', 'ship', 'drowned', 'going', 'holiday', '.']\n",
            "    After Regex: ['died', 'swept', 'ship', 'drowned', 'going', 'holiday']\n",
            "    After Lemmatization: ['died', 'swept', 'ship', 'drowned', 'going', 'holiday']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9326\n",
            "    After Stopword Removal: ['either', 'keep', 'plants', 'longer', 'spring', 'keep', 'somewhere', 'garden', 'pollinators', 'beneficial', 'insects', 'come', 'stay', 'neighborhood', '.']\n",
            "    After Regex: ['either', 'keep', 'plants', 'longer', 'spring', 'keep', 'somewhere', 'garden', 'pollinators', 'beneficial', 'insects', 'come', 'stay', 'neighborhood']\n",
            "    After Lemmatization: ['either', 'keep', 'plant', 'longer', 'spring', 'keep', 'somewhere', 'garden', 'pollinator', 'beneficial', 'insect', 'come', 'stay', 'neighborhood']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9327\n",
            "    After Stopword Removal: ['national', 'average', '37.6.']\n",
            "    After Regex: ['national', 'average']\n",
            "    After Lemmatization: ['national', 'average']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9328\n",
            "    After Stopword Removal: ['time', 'death', ',', 'march', '18,1867,', 'closest', 'roman', 'catholic', 'cemetery', 'church', 'north', 'american', 'martyrs', 'wardsville', '.']\n",
            "    After Regex: ['time', 'death', 'march', 'closest', 'roman', 'catholic', 'cemetery', 'church', 'north', 'american', 'martyrs', 'wardsville']\n",
            "    After Lemmatization: ['time', 'death', 'march', 'closest', 'roman', 'catholic', 'cemetery', 'church', 'north', 'american', 'martyr', 'wardsville']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9329\n",
            "    After Stopword Removal: ['wheat', 'one', 'largest', 'contributors', 'nationwide', 'obesity', 'epidemic', '—', 'elimination', 'key', 'dramatic', 'weight', 'loss', 'optimal', 'health', '.']\n",
            "    After Regex: ['wheat', 'one', 'largest', 'contributors', 'nationwide', 'obesity', 'epidemic', 'elimination', 'key', 'dramatic', 'weight', 'loss', 'optimal', 'health']\n",
            "    After Lemmatization: ['wheat', 'one', 'largest', 'contributor', 'nationwide', 'obesity', 'epidemic', 'elimination', 'key', 'dramatic', 'weight', 'loss', 'optimal', 'health']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9330\n",
            "    After Stopword Removal: ['vehicle', 'location', 'data', '—', 'come', 'much', 'data', '?']\n",
            "    After Regex: ['vehicle', 'location', 'data', 'come', 'much', 'data']\n",
            "    After Lemmatization: ['vehicle', 'location', 'data', 'come', 'much', 'data']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9331\n",
            "    After Stopword Removal: ['programmable', 'thermostat', 'save', '$150', 'per', 'year', 'heating', 'bills', '.']\n",
            "    After Regex: ['programmable', 'thermostat', 'save', 'per', 'year', 'heating', 'bills']\n",
            "    After Lemmatization: ['programmable', 'thermostat', 'save', 'per', 'year', 'heating', 'bill']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9332\n",
            "    After Stopword Removal: ['direct', 'line', 'yellowknife', 'hay', 'river', 'would', '300', 'kms', '.']\n",
            "    After Regex: ['direct', 'line', 'yellowknife', 'hay', 'river', 'would', 'kms']\n",
            "    After Lemmatization: ['direct', 'line', 'yellowknife', 'hay', 'river', 'would', 'km']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9333\n",
            "    After Stopword Removal: ['absolutely', 'feel', 'must', 'use', 'pesticide', ',', 'choose', 'organic', 'option', 'read', 'label', 'completely', 'apply', 'make', 'sure', 'applying', 'correctly', 'time', 'day', 'least', 'impact', 'pollinating', 'creatures', 'possible', '.']\n",
            "    After Regex: ['absolutely', 'feel', 'must', 'use', 'pesticide', 'choose', 'organic', 'option', 'read', 'label', 'completely', 'apply', 'make', 'sure', 'applying', 'correctly', 'time', 'day', 'least', 'impact', 'pollinating', 'creatures', 'possible']\n",
            "    After Lemmatization: ['absolutely', 'feel', 'must', 'use', 'pesticide', 'choose', 'organic', 'option', 'read', 'label', 'completely', 'apply', 'make', 'sure', 'applying', 'correctly', 'time', 'day', 'least', 'impact', 'pollinating', 'creature', 'possible']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9334\n",
            "    After Stopword Removal: ['take', 'charge', 'child', \"'\", 'asthma', 'home', 'make', 'sure', 'know', 'asthma', 'symptoms', 'watch', 'child', '.']\n",
            "    After Regex: ['take', 'charge', 'child', 'asthma', 'home', 'make', 'sure', 'know', 'asthma', 'symptoms', 'watch', 'child']\n",
            "    After Lemmatization: ['take', 'charge', 'child', 'asthma', 'home', 'make', 'sure', 'know', 'asthma', 'symptom', 'watch', 'child']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9335\n",
            "    After Stopword Removal: ['let', \"'\", 'look', 'algorithmic', 'problem', '.']\n",
            "    After Regex: ['let', 'look', 'algorithmic', 'problem']\n",
            "    After Lemmatization: ['let', 'look', 'algorithmic', 'problem']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9336\n",
            "    After Stopword Removal: ['includes', 'attracting', 'new', 'riders', ',', 'strengthening', 'bike', 'policies', ',', 'expanding', 'city', \"'\", 'bike', 'networks', 'programs', 'support', 'cycling', ',', 'increasing', 'bike', 'parking', 'increasing', 'funding', 'cycling', 'facilities', '.']\n",
            "    After Regex: ['includes', 'attracting', 'new', 'riders', 'strengthening', 'bike', 'policies', 'expanding', 'city', 'bike', 'networks', 'programs', 'support', 'cycling', 'increasing', 'bike', 'parking', 'increasing', 'funding', 'cycling', 'facilities']\n",
            "    After Lemmatization: ['includes', 'attracting', 'new', 'rider', 'strengthening', 'bike', 'policy', 'expanding', 'city', 'bike', 'network', 'program', 'support', 'cycling', 'increasing', 'bike', 'parking', 'increasing', 'funding', 'cycling', 'facility']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9337\n",
            "    After Stopword Removal: ['working', 'partnership', 'researchers', 'michael', 'smith', 'genome', 'sciences', 'centre', ',', 'hamelin', 'sequencing', 'genomes', '20', 'fungal', 'species', ',', 'identifying', 'genes', 'make', 'trees', 'sick', 'looking', 'variation', 'genes', '.']\n",
            "    After Regex: ['working', 'partnership', 'researchers', 'michael', 'smith', 'genome', 'sciences', 'centre', 'hamelin', 'sequencing', 'genomes', 'fungal', 'species', 'identifying', 'genes', 'make', 'trees', 'sick', 'looking', 'variation', 'genes']\n",
            "    After Lemmatization: ['working', 'partnership', 'researcher', 'michael', 'smith', 'genome', 'science', 'centre', 'hamelin', 'sequencing', 'genome', 'fungal', 'specie', 'identifying', 'gene', 'make', 'tree', 'sick', 'looking', 'variation', 'gene']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9338\n",
            "    After Stopword Removal: ['reverse', 'genetics', 'also', 'used', 'improve', 'process', 'making', 'seasonal', 'flu', 'vaccine', ',', 'shaving', 'time', 'uncertainty', 'process', '.']\n",
            "    After Regex: ['reverse', 'genetics', 'also', 'used', 'improve', 'process', 'making', 'seasonal', 'flu', 'vaccine', 'shaving', 'time', 'uncertainty', 'process']\n",
            "    After Lemmatization: ['reverse', 'genetics', 'also', 'used', 'improve', 'process', 'making', 'seasonal', 'flu', 'vaccine', 'shaving', 'time', 'uncertainty', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9339\n",
            "    After Stopword Removal: ['west', 'hills', 'offers', 'quiet', 'neighborhoods', 'houses', 'different', 'styles', 'including', 'capes', ',', 'ranches', ',', 'split', '-', 'levels', ',', 'high', 'ranches', ',', 'colonials', ',', 'contemporaries', ',', 'victorians', '.']\n",
            "    After Regex: ['west', 'hills', 'offers', 'quiet', 'neighborhoods', 'houses', 'different', 'styles', 'including', 'capes', 'ranches', 'split', 'levels', 'high', 'ranches', 'colonials', 'contemporaries', 'victorians']\n",
            "    After Lemmatization: ['west', 'hill', 'offer', 'quiet', 'neighborhood', 'house', 'different', 'style', 'including', 'cape', 'ranch', 'split', 'level', 'high', 'ranch', 'colonial', 'contemporary', 'victorian']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9340\n",
            "    After Stopword Removal: ['seconds', 'quiet', ',', 'researchers', 'cued', 'next', 'sound', 'process', 'began', '.']\n",
            "    After Regex: ['seconds', 'quiet', 'researchers', 'cued', 'next', 'sound', 'process', 'began']\n",
            "    After Lemmatization: ['second', 'quiet', 'researcher', 'cued', 'next', 'sound', 'process', 'began']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9341\n",
            "    After Stopword Removal: ['clearly', 'credit', 'crunch', 'negative', 'impact', 'seeking', 'add', 'increase', 'credit', 'lines', 'secure', 'loan', '.']\n",
            "    After Regex: ['clearly', 'credit', 'crunch', 'negative', 'impact', 'seeking', 'add', 'increase', 'credit', 'lines', 'secure', 'loan']\n",
            "    After Lemmatization: ['clearly', 'credit', 'crunch', 'negative', 'impact', 'seeking', 'add', 'increase', 'credit', 'line', 'secure', 'loan']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9342\n",
            "    After Stopword Removal: ['americans', 'oppose', 'law', 'support', ',', 'blacks', 'notable', 'exception', '.']\n",
            "    After Regex: ['americans', 'oppose', 'law', 'support', 'blacks', 'notable', 'exception']\n",
            "    After Lemmatization: ['american', 'oppose', 'law', 'support', 'black', 'notable', 'exception']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9343\n",
            "    After Stopword Removal: ['hta', 'tailored', 'real', 'time', 'applications', 'whose', 'data', 'carried', 'wireless', 'network', 'environments', '.']\n",
            "    After Regex: ['hta', 'tailored', 'real', 'time', 'applications', 'whose', 'data', 'carried', 'wireless', 'network', 'environments']\n",
            "    After Lemmatization: ['hta', 'tailored', 'real', 'time', 'application', 'whose', 'data', 'carried', 'wireless', 'network', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9344\n",
            "    After Stopword Removal: ['phil', 'gauger', ',', 'isu', \"'\", 'college', 'veterinary', 'medicine', ',', 'address', 'practical', 'swine', 'diagnostic', 'applications', 'oral', 'fluid', 'testing', 'address', 'disease', 'challenges', 'currently', 'affecting', 'pork', 'production', 'systems', '.']\n",
            "    After Regex: ['phil', 'gauger', 'isu', 'college', 'veterinary', 'medicine', 'address', 'practical', 'swine', 'diagnostic', 'applications', 'oral', 'fluid', 'testing', 'address', 'disease', 'challenges', 'currently', 'affecting', 'pork', 'production', 'systems']\n",
            "    After Lemmatization: ['phil', 'gauger', 'isu', 'college', 'veterinary', 'medicine', 'address', 'practical', 'swine', 'diagnostic', 'application', 'oral', 'fluid', 'testing', 'address', 'disease', 'challenge', 'currently', 'affecting', 'pork', 'production', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9345\n",
            "    After Stopword Removal: ['first', 'passenger', 'train', 'arrived', 'august', '8,1876,', 'celebrated', 'mojave', \"'\", 'birthday', '.']\n",
            "    After Regex: ['first', 'passenger', 'train', 'arrived', 'august', 'celebrated', 'mojave', 'birthday']\n",
            "    After Lemmatization: ['first', 'passenger', 'train', 'arrived', 'august', 'celebrated', 'mojave', 'birthday']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9346\n",
            "    After Stopword Removal: ['majority', 'malagasy', 'citizens', 'christian', ',', 'large', 'percentage', 'adheres', 'traditional', 'beliefs', 'less', '10', 'percent', 'muslim', '.']\n",
            "    After Regex: ['majority', 'malagasy', 'citizens', 'christian', 'large', 'percentage', 'adheres', 'traditional', 'beliefs', 'less', 'percent', 'muslim']\n",
            "    After Lemmatization: ['majority', 'malagasy', 'citizen', 'christian', 'large', 'percentage', 'adheres', 'traditional', 'belief', 'less', 'percent', 'muslim']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9347\n",
            "    After Stopword Removal: ['manage', 'endodontic', 'infections', 'local', 'treatment', 'instead', 'antibiotics', '?']\n",
            "    After Regex: ['manage', 'endodontic', 'infections', 'local', 'treatment', 'instead', 'antibiotics']\n",
            "    After Lemmatization: ['manage', 'endodontic', 'infection', 'local', 'treatment', 'instead', 'antibiotic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9348\n",
            "    After Stopword Removal: ['long', \"'\", 'learning', ',', \"'\", 'living', '.']\n",
            "    After Regex: ['long', 'learning', 'living']\n",
            "    After Lemmatization: ['long', 'learning', 'living']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9349\n",
            "    After Stopword Removal: ['new', 'creep', 'experiments', 'sinusoidal', 'compression', '/', 'dilation', 'deformation', 'homeotropic', 'sample', 'smectic', '-', 'liquid', 'crystal', '(8', 'cb', ')', 'show', 'response', 'nonlinear', 'small', 'amplitude', 'deformation', '.']\n",
            "    After Regex: ['new', 'creep', 'experiments', 'sinusoidal', 'compression', 'dilation', 'deformation', 'homeotropic', 'sample', 'smectic', 'liquid', 'crystal', 'cb', 'show', 'response', 'nonlinear', 'small', 'amplitude', 'deformation']\n",
            "    After Lemmatization: ['new', 'creep', 'experiment', 'sinusoidal', 'compression', 'dilation', 'deformation', 'homeotropic', 'sample', 'smectic', 'liquid', 'crystal', 'cb', 'show', 'response', 'nonlinear', 'small', 'amplitude', 'deformation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9350\n",
            "    After Stopword Removal: ['four', 'january', 'word', 'day', 'books', 'come', 'color', 'teacher', 'read', 'loud', 'focus', 'word', 'day', '.']\n",
            "    After Regex: ['four', 'january', 'word', 'day', 'books', 'come', 'color', 'teacher', 'read', 'loud', 'focus', 'word', 'day']\n",
            "    After Lemmatization: ['four', 'january', 'word', 'day', 'book', 'come', 'color', 'teacher', 'read', 'loud', 'focus', 'word', 'day']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9351\n",
            "    After Stopword Removal: ['employees', 'work', 'mandate', 'parks', 'canada', ',', 'preserve', 'present', 'canada', \"'\", 'national', 'heritage', 'current', 'future', 'generations', '.']\n",
            "    After Regex: ['employees', 'work', 'mandate', 'parks', 'canada', 'preserve', 'present', 'canada', 'national', 'heritage', 'current', 'future', 'generations']\n",
            "    After Lemmatization: ['employee', 'work', 'mandate', 'park', 'canada', 'preserve', 'present', 'canada', 'national', 'heritage', 'current', 'future', 'generation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9352\n",
            "    After Stopword Removal: ['eighteenth', 'century', ',', 'part', 'manchester', 'town', 'centre', '(', \"'\", 'really', 'town', 'centre', 'time', ')', 'bounded', '(', 'four', 'sides', ')', 'lever', 'street', ',', 'piccadilly', 'gardens', ',', 'great', 'ancoats', 'street', 'market', 'street', ',', 'belonged', 'ashton', 'lever', '(1729-1788),', 'son', 'sir', 'james', 'darcy', 'lever', ',', 'former', 'high', 'sheriff', 'lancashire', '.']\n",
            "    After Regex: ['eighteenth', 'century', 'part', 'manchester', 'town', 'centre', 'really', 'town', 'centre', 'time', 'bounded', 'four', 'sides', 'lever', 'street', 'piccadilly', 'gardens', 'great', 'ancoats', 'street', 'market', 'street', 'belonged', 'ashton', 'lever', 'son', 'sir', 'james', 'darcy', 'lever', 'former', 'high', 'sheriff', 'lancashire']\n",
            "    After Lemmatization: ['eighteenth', 'century', 'part', 'manchester', 'town', 'centre', 'really', 'town', 'centre', 'time', 'bounded', 'four', 'side', 'lever', 'street', 'piccadilly', 'garden', 'great', 'ancoats', 'street', 'market', 'street', 'belonged', 'ashton', 'lever', 'son', 'sir', 'james', 'darcy', 'lever', 'former', 'high', 'sheriff', 'lancashire']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9353\n",
            "    After Stopword Removal: ['hansen', ',', 'remember', ',', 'climate', 'scientists', 'known', 'global', 'warming', 'fear', 'mongering', 'probably', 'prominent', 'name', 'global', 'warming', 'movement', 'al', 'gore', '.']\n",
            "    After Regex: ['hansen', 'remember', 'climate', 'scientists', 'known', 'global', 'warming', 'fear', 'mongering', 'probably', 'prominent', 'name', 'global', 'warming', 'movement', 'al', 'gore']\n",
            "    After Lemmatization: ['hansen', 'remember', 'climate', 'scientist', 'known', 'global', 'warming', 'fear', 'mongering', 'probably', 'prominent', 'name', 'global', 'warming', 'movement', 'al', 'gore']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9354\n",
            "    After Stopword Removal: [\"'\", 'communication', 'tool', '.']\n",
            "    After Regex: ['communication', 'tool']\n",
            "    After Lemmatization: ['communication', 'tool']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9355\n",
            "    After Stopword Removal: ['process', 'involves', 'shipping', 'remains', 'radford', 'university', ',', 'radford', ',', 'va', '.,', 'osteologists', 'examine', 'evidence', '.']\n",
            "    After Regex: ['process', 'involves', 'shipping', 'remains', 'radford', 'university', 'radford', 'va', 'osteologists', 'examine', 'evidence']\n",
            "    After Lemmatization: ['process', 'involves', 'shipping', 'remains', 'radford', 'university', 'radford', 'va', 'osteologist', 'examine', 'evidence']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9356\n",
            "    After Stopword Removal: ['electron', 'microscopic', 'observation', 'confirmed', 'pulmonary', 'ultrastructure', 'improved', 'remarkably', 'treated', 'group', ':', 'proliferation', 'collagen', '-', 'forming', 'cells', 'infiltration', 'inflammatory', 'cells', ',', 'collagen', 'elastic', 'fibers', 'obviously', 'less', '.']\n",
            "    After Regex: ['electron', 'microscopic', 'observation', 'confirmed', 'pulmonary', 'ultrastructure', 'improved', 'remarkably', 'treated', 'group', 'proliferation', 'collagen', 'forming', 'cells', 'infiltration', 'inflammatory', 'cells', 'collagen', 'elastic', 'fibers', 'obviously', 'less']\n",
            "    After Lemmatization: ['electron', 'microscopic', 'observation', 'confirmed', 'pulmonary', 'ultrastructure', 'improved', 'remarkably', 'treated', 'group', 'proliferation', 'collagen', 'forming', 'cell', 'infiltration', 'inflammatory', 'cell', 'collagen', 'elastic', 'fiber', 'obviously', 'less']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9357\n",
            "    After Stopword Removal: ['imperative', 'deficit', 'shall', 'continue', ',', 'framers', 'tariff', 'bill', 'must', ',', 'course', ',', 'mind', 'total', 'revenues', 'likely', 'produced', 'arrange', 'duties', 'secure', 'adequate', 'income', '.']\n",
            "    After Regex: ['imperative', 'deficit', 'shall', 'continue', 'framers', 'tariff', 'bill', 'must', 'course', 'mind', 'total', 'revenues', 'likely', 'produced', 'arrange', 'duties', 'secure', 'adequate', 'income']\n",
            "    After Lemmatization: ['imperative', 'deficit', 'shall', 'continue', 'framer', 'tariff', 'bill', 'must', 'course', 'mind', 'total', 'revenue', 'likely', 'produced', 'arrange', 'duty', 'secure', 'adequate', 'income']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9358\n",
            "    After Stopword Removal: ['margin', 'wealth', 'god', \"'\", 'treasury', 'based', 'number', 'people', 'devoted', '.']\n",
            "    After Regex: ['margin', 'wealth', 'god', 'treasury', 'based', 'number', 'people', 'devoted']\n",
            "    After Lemmatization: ['margin', 'wealth', 'god', 'treasury', 'based', 'number', 'people', 'devoted']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9359\n",
            "    After Stopword Removal: ['labyrinth', 'walks', 'reportedly', 'rediscovered', 'across', 'country', 'tool', 'meditation', '.']\n",
            "    After Regex: ['labyrinth', 'walks', 'reportedly', 'rediscovered', 'across', 'country', 'tool', 'meditation']\n",
            "    After Lemmatization: ['labyrinth', 'walk', 'reportedly', 'rediscovered', 'across', 'country', 'tool', 'meditation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9360\n",
            "    After Stopword Removal: ['scientific', '\\xad', 'ally', 'assur', '\\xad', 'ed', 'quality', 'seed', 'plant', '–', 'cultivating', 'plants', 'allows', 'us', 'better', 'con', '\\xad', 'trol', 'active', '\\xad', 'gredients', 'possible', 'con', '\\xad', 'tamination', '.']\n",
            "    After Regex: ['scientific', 'ally', 'assur', 'ed', 'quality', 'seed', 'plant', 'cultivating', 'plants', 'allows', 'us', 'better', 'con', 'trol', 'active', 'gredients', 'possible', 'con', 'tamination']\n",
            "    After Lemmatization: ['scientific', 'ally', 'assur', 'ed', 'quality', 'seed', 'plant', 'cultivating', 'plant', 'allows', 'u', 'better', 'con', 'trol', 'active', 'gredients', 'possible', 'con', 'tamination']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9361\n",
            "    After Stopword Removal: ['end', 'punctuation', 'mark', 'indicates', 'pause', '.']\n",
            "    After Regex: ['end', 'punctuation', 'mark', 'indicates', 'pause']\n",
            "    After Lemmatization: ['end', 'punctuation', 'mark', 'indicates', 'pause']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9362\n",
            "    After Stopword Removal: ['research', 'shows', 'even', 'fiber', 'added', 'juice', ',', \"'\", 'satiating', 'whole', 'produce', '.']\n",
            "    After Regex: ['research', 'shows', 'even', 'fiber', 'added', 'juice', 'satiating', 'whole', 'produce']\n",
            "    After Lemmatization: ['research', 'show', 'even', 'fiber', 'added', 'juice', 'satiating', 'whole', 'produce']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9363\n",
            "    After Stopword Removal: ['although', 'swimming', 'included', 'olympic', 'games', 'athens', '1896,', 'competitive', 'swimming', 'developed', 'great', 'britain', 'early', '1800′', 'national', 'swimming', 'society', '.']\n",
            "    After Regex: ['although', 'swimming', 'included', 'olympic', 'games', 'athens', 'competitive', 'swimming', 'developed', 'great', 'britain', 'early', 'national', 'swimming', 'society']\n",
            "    After Lemmatization: ['although', 'swimming', 'included', 'olympic', 'game', 'athens', 'competitive', 'swimming', 'developed', 'great', 'britain', 'early', 'national', 'swimming', 'society']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9364\n",
            "    After Stopword Removal: ['independence', ',', 'many', 'versions', ',', 'one', 'lately', 'received', 'hamburgh', ',', 'american', 'vessel', 'said', \"'\", 'denationalized', \"'\", 'touched', 'british', 'port', ',', 'visited', 'english', 'man', 'war', ',', 'even', 'kept', 'company', '.']\n",
            "    After Regex: ['independence', 'many', 'versions', 'one', 'lately', 'received', 'hamburgh', 'american', 'vessel', 'said', 'denationalized', 'touched', 'british', 'port', 'visited', 'english', 'man', 'war', 'even', 'kept', 'company']\n",
            "    After Lemmatization: ['independence', 'many', 'version', 'one', 'lately', 'received', 'hamburgh', 'american', 'vessel', 'said', 'denationalized', 'touched', 'british', 'port', 'visited', 'english', 'man', 'war', 'even', 'kept', 'company']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9365\n",
            "    After Stopword Removal: ['hence', 'sin', 'simony', ',', 'rife', 'century', 'leading', 'reformation', '.']\n",
            "    After Regex: ['hence', 'sin', 'simony', 'rife', 'century', 'leading', 'reformation']\n",
            "    After Lemmatization: ['hence', 'sin', 'simony', 'rife', 'century', 'leading', 'reformation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9366\n",
            "    After Stopword Removal: ['however', ',', 'insurance', 'may', 'widespread', ',', 'far', 'universal', '.']\n",
            "    After Regex: ['however', 'insurance', 'may', 'widespread', 'far', 'universal']\n",
            "    After Lemmatization: ['however', 'insurance', 'may', 'widespread', 'far', 'universal']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9367\n",
            "    After Stopword Removal: ['technical', 'analysts', 'use', 'ton', 'data', ',', 'often', 'form', 'charts', ',', 'analyze', 'stocks', 'markets', '.']\n",
            "    After Regex: ['technical', 'analysts', 'use', 'ton', 'data', 'often', 'form', 'charts', 'analyze', 'stocks', 'markets']\n",
            "    After Lemmatization: ['technical', 'analyst', 'use', 'ton', 'data', 'often', 'form', 'chart', 'analyze', 'stock', 'market']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9368\n",
            "    After Stopword Removal: ['nearly', '50', 'years', 'protection', ',', 'endangered', 'italian', 'wolves', 'might', 'slaughtered', 'soon', '.']\n",
            "    After Regex: ['nearly', 'years', 'protection', 'endangered', 'italian', 'wolves', 'might', 'slaughtered', 'soon']\n",
            "    After Lemmatization: ['nearly', 'year', 'protection', 'endangered', 'italian', 'wolf', 'might', 'slaughtered', 'soon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9369\n",
            "    After Stopword Removal: ['anything', 'threatens', 'emotional', 'security', ',', \"'\", 'wrongness', \"','\", 'failure', \"'\", \"'\", 'rejection', \"',\", 'something', 'would', 'naturally', 'avoid', 'feeling', '.']\n",
            "    After Regex: ['anything', 'threatens', 'emotional', 'security', 'wrongness', 'failure', 'rejection', 'something', 'would', 'naturally', 'avoid', 'feeling']\n",
            "    After Lemmatization: ['anything', 'threatens', 'emotional', 'security', 'wrongness', 'failure', 'rejection', 'something', 'would', 'naturally', 'avoid', 'feeling']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9370\n",
            "    After Stopword Removal: ['robotics', 'advanced', 'point', 'cnc', 'machine', 'shops', \"'\", 'expectations', 'might', 'outnumber', 'questions', 'practical', 'applications', '.']\n",
            "    After Regex: ['robotics', 'advanced', 'point', 'cnc', 'machine', 'shops', 'expectations', 'might', 'outnumber', 'questions', 'practical', 'applications']\n",
            "    After Lemmatization: ['robotics', 'advanced', 'point', 'cnc', 'machine', 'shop', 'expectation', 'might', 'outnumber', 'question', 'practical', 'application']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9371\n",
            "    After Stopword Removal: ['cools', ',', 'set', 'cloudy', ',', 'malleable', 'mixture', '.']\n",
            "    After Regex: ['cools', 'set', 'cloudy', 'malleable', 'mixture']\n",
            "    After Lemmatization: ['cool', 'set', 'cloudy', 'malleable', 'mixture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9372\n",
            "    After Stopword Removal: ['legislation', 'may', 'come', 'republican', 'congress', '.']\n",
            "    After Regex: ['legislation', 'may', 'come', 'republican', 'congress']\n",
            "    After Lemmatization: ['legislation', 'may', 'come', 'republican', 'congress']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9373\n",
            "    After Stopword Removal: ['magnetic', 'fusion', 'device', 'designed', 'prove', 'feasibility', 'fusion', 'hydrogen', 'nuclei', 'large', '-', 'scale', 'carbon', '-', 'free', 'source', 'energy', '.']\n",
            "    After Regex: ['magnetic', 'fusion', 'device', 'designed', 'prove', 'feasibility', 'fusion', 'hydrogen', 'nuclei', 'large', 'scale', 'carbon', 'free', 'source', 'energy']\n",
            "    After Lemmatization: ['magnetic', 'fusion', 'device', 'designed', 'prove', 'feasibility', 'fusion', 'hydrogen', 'nucleus', 'large', 'scale', 'carbon', 'free', 'source', 'energy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9374\n",
            "    After Stopword Removal: ['could', 'underdeveloped', 'skills', 'led', '?']\n",
            "    After Regex: ['could', 'underdeveloped', 'skills', 'led']\n",
            "    After Lemmatization: ['could', 'underdeveloped', 'skill', 'led']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9375\n",
            "    After Stopword Removal: ['grandparents', 'want', 'warm', ',', 'loving', 'relationships', 'family', 'members', 'aware', 'major', 'reasons', 'conflicts', 'adult', 'children', ',', 'well', 'avoid', 'defuse', 'conflicts', '.']\n",
            "    After Regex: ['grandparents', 'want', 'warm', 'loving', 'relationships', 'family', 'members', 'aware', 'major', 'reasons', 'conflicts', 'adult', 'children', 'well', 'avoid', 'defuse', 'conflicts']\n",
            "    After Lemmatization: ['grandparent', 'want', 'warm', 'loving', 'relationship', 'family', 'member', 'aware', 'major', 'reason', 'conflict', 'adult', 'child', 'well', 'avoid', 'defuse', 'conflict']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9376\n",
            "    After Stopword Removal: ['people', 'find', 'easier', 'practice', 'guided', 'imagery', 'meditation', ',', 'easier', 'focus', 'something', 'rather', 'nothing', '.']\n",
            "    After Regex: ['people', 'find', 'easier', 'practice', 'guided', 'imagery', 'meditation', 'easier', 'focus', 'something', 'rather', 'nothing']\n",
            "    After Lemmatization: ['people', 'find', 'easier', 'practice', 'guided', 'imagery', 'meditation', 'easier', 'focus', 'something', 'rather', 'nothing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9377\n",
            "    After Stopword Removal: ['wife', ',', 'dwina', 'murphy', ',', 'survives', ',', 'lived', '12', 'th', '-', 'century', 'former', 'monastery', 'oxfordshire', 'restored', 'filled', 'statues', 'buddha', 'suits', 'armor', '.']\n",
            "    After Regex: ['wife', 'dwina', 'murphy', 'survives', 'lived', 'th', 'century', 'former', 'monastery', 'oxfordshire', 'restored', 'filled', 'statues', 'buddha', 'suits', 'armor']\n",
            "    After Lemmatization: ['wife', 'dwina', 'murphy', 'survives', 'lived', 'th', 'century', 'former', 'monastery', 'oxfordshire', 'restored', 'filled', 'statue', 'buddha', 'suit', 'armor']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9378\n",
            "    After Stopword Removal: ['arts', 'really', 'integrated', 'manufacturing', 'process', '.']\n",
            "    After Regex: ['arts', 'really', 'integrated', 'manufacturing', 'process']\n",
            "    After Lemmatization: ['art', 'really', 'integrated', 'manufacturing', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9379\n",
            "    After Stopword Removal: ['encourage', 'culture', 'self', '-', 'teaching', ',', 'reflect', 'value', 'lifelong', 'learning', 'ensure', 'future', 'generations', 'able', 'continuously', 'improve', 'adapt', 'change', '.']\n",
            "    After Regex: ['encourage', 'culture', 'self', 'teaching', 'reflect', 'value', 'lifelong', 'learning', 'ensure', 'future', 'generations', 'able', 'continuously', 'improve', 'adapt', 'change']\n",
            "    After Lemmatization: ['encourage', 'culture', 'self', 'teaching', 'reflect', 'value', 'lifelong', 'learning', 'ensure', 'future', 'generation', 'able', 'continuously', 'improve', 'adapt', 'change']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9380\n",
            "    After Stopword Removal: ['basic', 'research', 'area', 'interest', 'development', 'vectors', 'deliver', 'macromolecular', 'therapies', 'brain', '.']\n",
            "    After Regex: ['basic', 'research', 'area', 'interest', 'development', 'vectors', 'deliver', 'macromolecular', 'therapies', 'brain']\n",
            "    After Lemmatization: ['basic', 'research', 'area', 'interest', 'development', 'vector', 'deliver', 'macromolecular', 'therapy', 'brain']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9381\n",
            "    After Stopword Removal: ['previously', ',', 'based', 'skulls', 'isolated', 'bones', ',', 'scientists', 'proposed', 'ignacius', 'archaic', 'primate', ',', 'instead', 'gliding', 'mammal', 'related', 'flying', 'lemurs', '.']\n",
            "    After Regex: ['previously', 'based', 'skulls', 'isolated', 'bones', 'scientists', 'proposed', 'ignacius', 'archaic', 'primate', 'instead', 'gliding', 'mammal', 'related', 'flying', 'lemurs']\n",
            "    After Lemmatization: ['previously', 'based', 'skull', 'isolated', 'bone', 'scientist', 'proposed', 'ignacius', 'archaic', 'primate', 'instead', 'gliding', 'mammal', 'related', 'flying', 'lemur']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9382\n",
            "    After Stopword Removal: ['celebrate', 'heritage', ',', 'people', 'places', 'make', 'singapore', \"'\", 'famous', 'waters', '.']\n",
            "    After Regex: ['celebrate', 'heritage', 'people', 'places', 'make', 'singapore', 'famous', 'waters']\n",
            "    After Lemmatization: ['celebrate', 'heritage', 'people', 'place', 'make', 'singapore', 'famous', 'water']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9383\n",
            "    After Stopword Removal: ['ffta', 'aims', 'strengthen', 'agencies', 'support', 'families', 'caring', 'vulnerable', 'children', '.']\n",
            "    After Regex: ['ffta', 'aims', 'strengthen', 'agencies', 'support', 'families', 'caring', 'vulnerable', 'children']\n",
            "    After Lemmatization: ['ffta', 'aim', 'strengthen', 'agency', 'support', 'family', 'caring', 'vulnerable', 'child']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9384\n",
            "    After Stopword Removal: ['1731', 'settlers', 'canary', 'islands', 'laid', 'town', 'san', 'fernando', 'de', 'béxar', 'near', 'presidio', ',', 'civilian', 'community', 'planned', 'presidio', 'mission', 'established', '.']\n",
            "    After Regex: ['settlers', 'canary', 'islands', 'laid', 'town', 'san', 'fernando', 'de', 'bxar', 'near', 'presidio', 'civilian', 'community', 'planned', 'presidio', 'mission', 'established']\n",
            "    After Lemmatization: ['settler', 'canary', 'island', 'laid', 'town', 'san', 'fernando', 'de', 'bxar', 'near', 'presidio', 'civilian', 'community', 'planned', 'presidio', 'mission', 'established']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9385\n",
            "    After Stopword Removal: ['main', 'structure', 'college', 'administration', ',', 'staff', ',', 'way', 'life', ',', 'impact', 'upon', 'reform', 'events', ',', 'arguments', 'debates', 'advantages', 'disadvantages', 'system', 'also', 'examined', 'assessed', '.']\n",
            "    After Regex: ['main', 'structure', 'college', 'administration', 'staff', 'way', 'life', 'impact', 'upon', 'reform', 'events', 'arguments', 'debates', 'advantages', 'disadvantages', 'system', 'also', 'examined', 'assessed']\n",
            "    After Lemmatization: ['main', 'structure', 'college', 'administration', 'staff', 'way', 'life', 'impact', 'upon', 'reform', 'event', 'argument', 'debate', 'advantage', 'disadvantage', 'system', 'also', 'examined', 'assessed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9386\n",
            "    After Stopword Removal: ['comparable', 'small', 'village', 'four', 'hundred', 'villas', 'occupied', 'expatriates', 'mainly', 'europe', '&', 'america', '.']\n",
            "    After Regex: ['comparable', 'small', 'village', 'four', 'hundred', 'villas', 'occupied', 'expatriates', 'mainly', 'europe', 'america']\n",
            "    After Lemmatization: ['comparable', 'small', 'village', 'four', 'hundred', 'villa', 'occupied', 'expatriate', 'mainly', 'europe', 'america']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9387\n",
            "    After Stopword Removal: ['step', '2:', 'assessment', '–', 'first', '1-3', 'appointments', 'counselor', 'involve', 'semi', '-', 'structured', 'interview', ',', 'counselor', 'gathers', 'information', 'including', 'medical', 'health', ',', 'family', 'history', ',', 'sources', 'stress', ',', 'current', 'relationships', ',', 'past', 'counseling', 'experiences', '.']\n",
            "    After Regex: ['step', 'assessment', 'first', 'appointments', 'counselor', 'involve', 'semi', 'structured', 'interview', 'counselor', 'gathers', 'information', 'including', 'medical', 'health', 'family', 'history', 'sources', 'stress', 'current', 'relationships', 'past', 'counseling', 'experiences']\n",
            "    After Lemmatization: ['step', 'assessment', 'first', 'appointment', 'counselor', 'involve', 'semi', 'structured', 'interview', 'counselor', 'gather', 'information', 'including', 'medical', 'health', 'family', 'history', 'source', 'stress', 'current', 'relationship', 'past', 'counseling', 'experience']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9388\n",
            "    After Stopword Removal: ['debates', 'pivot', 'arcane', 'theological', 'points', 'ninth', 'century', ',', 'time', 'religious', 'empires', 'reigned', ',', 'secular', 'nations', '.']\n",
            "    After Regex: ['debates', 'pivot', 'arcane', 'theological', 'points', 'ninth', 'century', 'time', 'religious', 'empires', 'reigned', 'secular', 'nations']\n",
            "    After Lemmatization: ['debate', 'pivot', 'arcane', 'theological', 'point', 'ninth', 'century', 'time', 'religious', 'empire', 'reigned', 'secular', 'nation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9389\n",
            "    After Stopword Removal: ['hence', ',', 'cu', '2+', 'ions', 'precipitated', 'h', '2', '(', 'g', ')', 'passed', 'solution', '.']\n",
            "    After Regex: ['hence', 'cu', 'ions', 'precipitated', 'h', 'g', 'passed', 'solution']\n",
            "    After Lemmatization: ['hence', 'cu', 'ion', 'precipitated', 'h', 'g', 'passed', 'solution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9390\n",
            "    After Stopword Removal: ['weight', 'ice', 'dam', 'may', 'even', 'cause', 'damage', 'roof', 'gutters', '.']\n",
            "    After Regex: ['weight', 'ice', 'dam', 'may', 'even', 'cause', 'damage', 'roof', 'gutters']\n",
            "    After Lemmatization: ['weight', 'ice', 'dam', 'may', 'even', 'cause', 'damage', 'roof', 'gutter']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9391\n",
            "    After Stopword Removal: ['work', 'environment', ':', 'work', 'environments', 'dusty', 'windy', 'may', 'harmful', 'patients', 'likely', 'develop', 'dry', 'eye', 'syndrome', '.']\n",
            "    After Regex: ['work', 'environment', 'work', 'environments', 'dusty', 'windy', 'may', 'harmful', 'patients', 'likely', 'develop', 'dry', 'eye', 'syndrome']\n",
            "    After Lemmatization: ['work', 'environment', 'work', 'environment', 'dusty', 'windy', 'may', 'harmful', 'patient', 'likely', 'develop', 'dry', 'eye', 'syndrome']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9392\n",
            "    After Stopword Removal: ['educators', 'known', 'long', 'time', 'ask', 'students', \"'\", 'good', ',', \"'\", 'usually', 'reliable', 'predictor', \"'\", 'actually', 'good', '.']\n",
            "    After Regex: ['educators', 'known', 'long', 'time', 'ask', 'students', 'good', 'usually', 'reliable', 'predictor', 'actually', 'good']\n",
            "    After Lemmatization: ['educator', 'known', 'long', 'time', 'ask', 'student', 'good', 'usually', 'reliable', 'predictor', 'actually', 'good']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9393\n",
            "    After Stopword Removal: ['immense', 'operation', '30,000', 'troops', '.']\n",
            "    After Regex: ['immense', 'operation', 'troops']\n",
            "    After Lemmatization: ['immense', 'operation', 'troop']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9394\n",
            "    After Stopword Removal: ['environmental', 'resources', 'water', ',', 'timber', ',', 'wildlife', 'edible', 'plants', 'consumed', 'unsustainable', 'way', 'considerable', 'quantity', 'waste', 'generated', ',', 'polluting', 'soils', 'water', 'tables', '.']\n",
            "    After Regex: ['environmental', 'resources', 'water', 'timber', 'wildlife', 'edible', 'plants', 'consumed', 'unsustainable', 'way', 'considerable', 'quantity', 'waste', 'generated', 'polluting', 'soils', 'water', 'tables']\n",
            "    After Lemmatization: ['environmental', 'resource', 'water', 'timber', 'wildlife', 'edible', 'plant', 'consumed', 'unsustainable', 'way', 'considerable', 'quantity', 'waste', 'generated', 'polluting', 'soil', 'water', 'table']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9395\n",
            "    After Stopword Removal: ['use', 'inverting', 'op', '-', 'amp', ',', 'shown', 'figure', '1.']\n",
            "    After Regex: ['use', 'inverting', 'op', 'amp', 'shown', 'figure']\n",
            "    After Lemmatization: ['use', 'inverting', 'op', 'amp', 'shown', 'figure']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9396\n",
            "    After Stopword Removal: ['please', 'note', 'approximately', 'many', 'centuries', 'medical', 'science', 'investigated', 'importance', 'finger', 'prints', 'allah', 'already', 'indicated', 'importance', 'finger', 'tips', '(', 'finger', 'prints', ').']\n",
            "    After Regex: ['please', 'note', 'approximately', 'many', 'centuries', 'medical', 'science', 'investigated', 'importance', 'finger', 'prints', 'allah', 'already', 'indicated', 'importance', 'finger', 'tips', 'finger', 'prints']\n",
            "    After Lemmatization: ['please', 'note', 'approximately', 'many', 'century', 'medical', 'science', 'investigated', 'importance', 'finger', 'print', 'allah', 'already', 'indicated', 'importance', 'finger', 'tip', 'finger', 'print']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9397\n",
            "    After Stopword Removal: ['condition', 'brings', 'temporary', 'relief', 'person', 'suffering', ',', 'carries', 'lot', 'shame', 'stigma', '.']\n",
            "    After Regex: ['condition', 'brings', 'temporary', 'relief', 'person', 'suffering', 'carries', 'lot', 'shame', 'stigma']\n",
            "    After Lemmatization: ['condition', 'brings', 'temporary', 'relief', 'person', 'suffering', 'carry', 'lot', 'shame', 'stigma']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9398\n",
            "    After Stopword Removal: ['articles', 'confederation', 'perpetual', 'union', 'agreement', 'among', '13', 'original', 'states', 'united', 'states', 'america', 'served', 'first', 'constitution', '.']\n",
            "    After Regex: ['articles', 'confederation', 'perpetual', 'union', 'agreement', 'among', 'original', 'states', 'united', 'states', 'america', 'served', 'first', 'constitution']\n",
            "    After Lemmatization: ['article', 'confederation', 'perpetual', 'union', 'agreement', 'among', 'original', 'state', 'united', 'state', 'america', 'served', 'first', 'constitution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9399\n",
            "    After Stopword Removal: [\"'\", 'consequence', 'separate', 'economies', 'banks', \"'\", 'policies', 'regulations', 'different', 'states', ',', 'unfortunately', \"'\", 'unavoidable', 'difference', '.']\n",
            "    After Regex: ['consequence', 'separate', 'economies', 'banks', 'policies', 'regulations', 'different', 'states', 'unfortunately', 'unavoidable', 'difference']\n",
            "    After Lemmatization: ['consequence', 'separate', 'economy', 'bank', 'policy', 'regulation', 'different', 'state', 'unfortunately', 'unavoidable', 'difference']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9400\n",
            "    After Stopword Removal: ['lead', 'new', 'possibilities', 'research', 'applications', 'fields', 'like', 'neuroscience', ',', 'regenerative', 'medicine', ',', 'biomanufacturing', ',', 'tissue', 'engineering', 'cancer', 'treatment', '.']\n",
            "    After Regex: ['lead', 'new', 'possibilities', 'research', 'applications', 'fields', 'like', 'neuroscience', 'regenerative', 'medicine', 'biomanufacturing', 'tissue', 'engineering', 'cancer', 'treatment']\n",
            "    After Lemmatization: ['lead', 'new', 'possibility', 'research', 'application', 'field', 'like', 'neuroscience', 'regenerative', 'medicine', 'biomanufacturing', 'tissue', 'engineering', 'cancer', 'treatment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9401\n",
            "    After Stopword Removal: ['use', 'one', 'single', 'strand', 'word', '.']\n",
            "    After Regex: ['use', 'one', 'single', 'strand', 'word']\n",
            "    After Lemmatization: ['use', 'one', 'single', 'strand', 'word']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9402\n",
            "    After Stopword Removal: ['experiments', 'performed', ',', 'spectrophotometer', 'cuvettes', 'never', 'broke', '.']\n",
            "    After Regex: ['experiments', 'performed', 'spectrophotometer', 'cuvettes', 'never', 'broke']\n",
            "    After Lemmatization: ['experiment', 'performed', 'spectrophotometer', 'cuvettes', 'never', 'broke']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9403\n",
            "    After Stopword Removal: ['taken', 'sword', ',', 'weapon', 'war', 'fit', 'hand', 'suited', 'disposition', '.']\n",
            "    After Regex: ['taken', 'sword', 'weapon', 'war', 'fit', 'hand', 'suited', 'disposition']\n",
            "    After Lemmatization: ['taken', 'sword', 'weapon', 'war', 'fit', 'hand', 'suited', 'disposition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9404\n",
            "    After Stopword Removal: ['key', 'long', 'term', 'effort', 'army', 'go', 'green', 'training', 'soldiers', '.']\n",
            "    After Regex: ['key', 'long', 'term', 'effort', 'army', 'go', 'green', 'training', 'soldiers']\n",
            "    After Lemmatization: ['key', 'long', 'term', 'effort', 'army', 'go', 'green', 'training', 'soldier']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9405\n",
            "    After Stopword Removal: ['ages', ',', 'similar', 'architectural', 'structures', 'built', 'wish', 'eliminate', 'ordeal', '-', 'ground', 'burial', '.']\n",
            "    After Regex: ['ages', 'similar', 'architectural', 'structures', 'built', 'wish', 'eliminate', 'ordeal', 'ground', 'burial']\n",
            "    After Lemmatization: ['age', 'similar', 'architectural', 'structure', 'built', 'wish', 'eliminate', 'ordeal', 'ground', 'burial']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9406\n",
            "    After Stopword Removal: ['plateau', 'scene', 'dramatic', 'events', 'history', ':-', 'crowds', 'gathered', 'death', 'gladstone', ',', 'death', 'queen', 'victoria', ',', 'assassination', 'john', 'lennon', '.']\n",
            "    After Regex: ['plateau', 'scene', 'dramatic', 'events', 'history', 'crowds', 'gathered', 'death', 'gladstone', 'death', 'queen', 'victoria', 'assassination', 'john', 'lennon']\n",
            "    After Lemmatization: ['plateau', 'scene', 'dramatic', 'event', 'history', 'crowd', 'gathered', 'death', 'gladstone', 'death', 'queen', 'victoria', 'assassination', 'john', 'lennon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9407\n",
            "    After Stopword Removal: ['soil', 'packed', 'loose', ',', 'settles', 'watering', 'marijuana', 'plant', 'sinks', ',', 'sometimes', 'several', 'inches', ',', 'soil', 'might', 'run', 'bottom', 'pot', 'watering', '.']\n",
            "    After Regex: ['soil', 'packed', 'loose', 'settles', 'watering', 'marijuana', 'plant', 'sinks', 'sometimes', 'several', 'inches', 'soil', 'might', 'run', 'bottom', 'pot', 'watering']\n",
            "    After Lemmatization: ['soil', 'packed', 'loose', 'settle', 'watering', 'marijuana', 'plant', 'sink', 'sometimes', 'several', 'inch', 'soil', 'might', 'run', 'bottom', 'pot', 'watering']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9408\n",
            "    After Stopword Removal: ['read', 'information', 'orthodontic', 'treatment', '.']\n",
            "    After Regex: ['read', 'information', 'orthodontic', 'treatment']\n",
            "    After Lemmatization: ['read', 'information', 'orthodontic', 'treatment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9409\n",
            "    After Stopword Removal: ['regardless', 'whether', 'coup', 'allegations', 'true', ',', 'serve', 'important', 'purpose', 'chavez', 'increasing', 'public', \"'\", 'sense', 'government', 'embattled', '.']\n",
            "    After Regex: ['regardless', 'whether', 'coup', 'allegations', 'true', 'serve', 'important', 'purpose', 'chavez', 'increasing', 'public', 'sense', 'government', 'embattled']\n",
            "    After Lemmatization: ['regardless', 'whether', 'coup', 'allegation', 'true', 'serve', 'important', 'purpose', 'chavez', 'increasing', 'public', 'sense', 'government', 'embattled']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9410\n",
            "    After Stopword Removal: ['webs', 'murder', ',', 'jealousy', 'revenge', 'threaten', 'rip', 'apart', 'families', 'destroy', 'entire', 'communities', '.']\n",
            "    After Regex: ['webs', 'murder', 'jealousy', 'revenge', 'threaten', 'rip', 'apart', 'families', 'destroy', 'entire', 'communities']\n",
            "    After Lemmatization: ['web', 'murder', 'jealousy', 'revenge', 'threaten', 'rip', 'apart', 'family', 'destroy', 'entire', 'community']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9411\n",
            "    After Stopword Removal: ['department', 'process', 'constructing', '700', 'single', '-', 'family', 'homes', 'oahu', '.']\n",
            "    After Regex: ['department', 'process', 'constructing', 'single', 'family', 'homes', 'oahu']\n",
            "    After Lemmatization: ['department', 'process', 'constructing', 'single', 'family', 'home', 'oahu']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9412\n",
            "    After Stopword Removal: ['scientific', 'knowledge', 'around', 'using', 'drugs', 'stimulate', 'body', \"'\", 'natural', 'defences', 'cancer', 'becoming', 'prominent', '.']\n",
            "    After Regex: ['scientific', 'knowledge', 'around', 'using', 'drugs', 'stimulate', 'body', 'natural', 'defences', 'cancer', 'becoming', 'prominent']\n",
            "    After Lemmatization: ['scientific', 'knowledge', 'around', 'using', 'drug', 'stimulate', 'body', 'natural', 'defence', 'cancer', 'becoming', 'prominent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9413\n",
            "    After Stopword Removal: ['blood', 'tests', 'generally', 'take', 'long', '.']\n",
            "    After Regex: ['blood', 'tests', 'generally', 'take', 'long']\n",
            "    After Lemmatization: ['blood', 'test', 'generally', 'take', 'long']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9414\n",
            "    After Stopword Removal: ['generation', 'likely', 'keep', 'teeth', 'past', 'generations', ',', 'dental', 'care', 'needed', 'years', 'come', '.']\n",
            "    After Regex: ['generation', 'likely', 'keep', 'teeth', 'past', 'generations', 'dental', 'care', 'needed', 'years', 'come']\n",
            "    After Lemmatization: ['generation', 'likely', 'keep', 'teeth', 'past', 'generation', 'dental', 'care', 'needed', 'year', 'come']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9415\n",
            "    After Stopword Removal: ['part', 'changes', 'lansley', 'reassign', 'fsa', \"'\", 'regulatory', 'aspects', '–', 'including', 'safety', 'hygiene', '–', 'department', 'environment', ',', 'food', 'rural', 'affairs', '(', 'defra', ').']\n",
            "    After Regex: ['part', 'changes', 'lansley', 'reassign', 'fsa', 'regulatory', 'aspects', 'including', 'safety', 'hygiene', 'department', 'environment', 'food', 'rural', 'affairs', 'defra']\n",
            "    After Lemmatization: ['part', 'change', 'lansley', 'reassign', 'fsa', 'regulatory', 'aspect', 'including', 'safety', 'hygiene', 'department', 'environment', 'food', 'rural', 'affair', 'defra']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9416\n",
            "    After Stopword Removal: ['studies', 'done', 'green', 'tea', 'shown', 'works', 'best', 'combined', 'caffeine', '.']\n",
            "    After Regex: ['studies', 'done', 'green', 'tea', 'shown', 'works', 'best', 'combined', 'caffeine']\n",
            "    After Lemmatization: ['study', 'done', 'green', 'tea', 'shown', 'work', 'best', 'combined', 'caffeine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9417\n",
            "    After Stopword Removal: ['average', 'age', 'person', 'becoming', 'president', '55', 'years', '3', 'months', 'old', '.']\n",
            "    After Regex: ['average', 'age', 'person', 'becoming', 'president', 'years', 'months', 'old']\n",
            "    After Lemmatization: ['average', 'age', 'person', 'becoming', 'president', 'year', 'month', 'old']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9418\n",
            "    After Stopword Removal: ['several', 'states', 'currently', 'ban', 'guns', 'places', '.']\n",
            "    After Regex: ['several', 'states', 'currently', 'ban', 'guns', 'places']\n",
            "    After Lemmatization: ['several', 'state', 'currently', 'ban', 'gun', 'place']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9419\n",
            "    After Stopword Removal: ['help', 'reduce', 'much', 'bacteria', \"'\", 'sleeping', ',', 'help', 'clear', 'skin', '.']\n",
            "    After Regex: ['help', 'reduce', 'much', 'bacteria', 'sleeping', 'help', 'clear', 'skin']\n",
            "    After Lemmatization: ['help', 'reduce', 'much', 'bacteria', 'sleeping', 'help', 'clear', 'skin']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9420\n",
            "    After Stopword Removal: ['one', 'must', 'suspect', 'actors', 'influential', 'others', 'constructing', 'social', 'norms', '.']\n",
            "    After Regex: ['one', 'must', 'suspect', 'actors', 'influential', 'others', 'constructing', 'social', 'norms']\n",
            "    After Lemmatization: ['one', 'must', 'suspect', 'actor', 'influential', 'others', 'constructing', 'social', 'norm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9421\n",
            "    After Stopword Removal: ['facilities', 'managers', 'want', 'improve', 'cleaning', 'standards', 'need', 'review', 'current', 'methods', 'products', 'used', 'ensure', 'obtaining', 'best', 'ohs', 'outcomes', 'office', 'facility', \"'\", 'staff', '.']\n",
            "    After Regex: ['facilities', 'managers', 'want', 'improve', 'cleaning', 'standards', 'need', 'review', 'current', 'methods', 'products', 'used', 'ensure', 'obtaining', 'best', 'ohs', 'outcomes', 'office', 'facility', 'staff']\n",
            "    After Lemmatization: ['facility', 'manager', 'want', 'improve', 'cleaning', 'standard', 'need', 'review', 'current', 'method', 'product', 'used', 'ensure', 'obtaining', 'best', 'oh', 'outcome', 'office', 'facility', 'staff']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9422\n",
            "    After Stopword Removal: ['well', ',', 'averages', 'certainly', \"'\", 'tell', 'whole', 'story', \"'\", 'chart', 'economic', 'policy', 'institute', 'showing', 'median', '(', 'mid', '-', 'point', ',', 'average', ')', 'amount', 'family', 'retirement', 'savings', 'age', 'group', '.']\n",
            "    After Regex: ['well', 'averages', 'certainly', 'tell', 'whole', 'story', 'chart', 'economic', 'policy', 'institute', 'showing', 'median', 'mid', 'point', 'average', 'amount', 'family', 'retirement', 'savings', 'age', 'group']\n",
            "    After Lemmatization: ['well', 'average', 'certainly', 'tell', 'whole', 'story', 'chart', 'economic', 'policy', 'institute', 'showing', 'median', 'mid', 'point', 'average', 'amount', 'family', 'retirement', 'saving', 'age', 'group']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9423\n",
            "    After Stopword Removal: ['eventually', 'one', 'infected', 'ticks', 'found', 'among', 'group', 'thirty', '-', 'six', 'males', 'collected', 'apparently', 'healthy', 'horses', '.']\n",
            "    After Regex: ['eventually', 'one', 'infected', 'ticks', 'found', 'among', 'group', 'thirty', 'six', 'males', 'collected', 'apparently', 'healthy', 'horses']\n",
            "    After Lemmatization: ['eventually', 'one', 'infected', 'tick', 'found', 'among', 'group', 'thirty', 'six', 'male', 'collected', 'apparently', 'healthy', 'horse']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9424\n",
            "    After Stopword Removal: ['old', 'school', 'approach', \"'\", 'always', 'done', 'pest', 'management', 'way', '—', 'without', 'fumigation', '—', 'change', '.']\n",
            "    After Regex: ['old', 'school', 'approach', 'always', 'done', 'pest', 'management', 'way', 'without', 'fumigation', 'change']\n",
            "    After Lemmatization: ['old', 'school', 'approach', 'always', 'done', 'pest', 'management', 'way', 'without', 'fumigation', 'change']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9425\n",
            "    After Stopword Removal: ['similar', 'feature', 'suggested', 'scyphozoan', 'jellyfish', 'venoms', 'appear', 'induce', 'opening', 'activation', 'non', '-', 'selective', 'cationic', 'channels', 'subsequent', 'increase', 'inward', 'sodium', '-', 'elicited', 'currents', '[13,14,15,16].']\n",
            "    After Regex: ['similar', 'feature', 'suggested', 'scyphozoan', 'jellyfish', 'venoms', 'appear', 'induce', 'opening', 'activation', 'non', 'selective', 'cationic', 'channels', 'subsequent', 'increase', 'inward', 'sodium', 'elicited', 'currents']\n",
            "    After Lemmatization: ['similar', 'feature', 'suggested', 'scyphozoan', 'jellyfish', 'venom', 'appear', 'induce', 'opening', 'activation', 'non', 'selective', 'cationic', 'channel', 'subsequent', 'increase', 'inward', 'sodium', 'elicited', 'current']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9426\n",
            "    After Stopword Removal: ['one', 'many', 'reasons', 'high', 'cost', 'leica', 'care', 'required', 'properly', 'calibrate', 'system', '.']\n",
            "    After Regex: ['one', 'many', 'reasons', 'high', 'cost', 'leica', 'care', 'required', 'properly', 'calibrate', 'system']\n",
            "    After Lemmatization: ['one', 'many', 'reason', 'high', 'cost', 'leica', 'care', 'required', 'properly', 'calibrate', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9427\n",
            "    After Stopword Removal: ['county', 'staff', 'said', 'could', 'problematic', 'property', 'owner', 'indicated', 'would', 'apply', 'permit', 'fill', 'wetlands', 'area', 'build', 'house', '.']\n",
            "    After Regex: ['county', 'staff', 'said', 'could', 'problematic', 'property', 'owner', 'indicated', 'would', 'apply', 'permit', 'fill', 'wetlands', 'area', 'build', 'house']\n",
            "    After Lemmatization: ['county', 'staff', 'said', 'could', 'problematic', 'property', 'owner', 'indicated', 'would', 'apply', 'permit', 'fill', 'wetland', 'area', 'build', 'house']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9428\n",
            "    After Stopword Removal: ['schrader', \"'\", 'work', 'encompasses', 'variety', 'subjects', 'ranges', 'portraits', 'candids', 'landscapes', '.']\n",
            "    After Regex: ['schrader', 'work', 'encompasses', 'variety', 'subjects', 'ranges', 'portraits', 'candids', 'landscapes']\n",
            "    After Lemmatization: ['schrader', 'work', 'encompasses', 'variety', 'subject', 'range', 'portrait', 'candids', 'landscape']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9429\n",
            "    After Stopword Removal: ['workforce', 'strategist', \"'\", 'role', 'health', 'system', 'establish', 'good', 'control', 'workforce', 'supply', 'demand', 'unless', 'senior', 'leaders', 'key', 'clinicians', 'held', 'accountable', 'task', '.']\n",
            "    After Regex: ['workforce', 'strategist', 'role', 'health', 'system', 'establish', 'good', 'control', 'workforce', 'supply', 'demand', 'unless', 'senior', 'leaders', 'key', 'clinicians', 'held', 'accountable', 'task']\n",
            "    After Lemmatization: ['workforce', 'strategist', 'role', 'health', 'system', 'establish', 'good', 'control', 'workforce', 'supply', 'demand', 'unless', 'senior', 'leader', 'key', 'clinician', 'held', 'accountable', 'task']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9430\n",
            "    After Stopword Removal: ['lte', 'makes', 'cable', '-', 'like', 'reliability', 'security', 'possible', 'wireless', 'technology', '.']\n",
            "    After Regex: ['lte', 'makes', 'cable', 'like', 'reliability', 'security', 'possible', 'wireless', 'technology']\n",
            "    After Lemmatization: ['lte', 'make', 'cable', 'like', 'reliability', 'security', 'possible', 'wireless', 'technology']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9431\n",
            "    After Stopword Removal: ['know', 'carbon', 'pollution', 'coming', '.']\n",
            "    After Regex: ['know', 'carbon', 'pollution', 'coming']\n",
            "    After Lemmatization: ['know', 'carbon', 'pollution', 'coming']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9432\n",
            "    After Stopword Removal: ['due', 'reduced', 'output', 'data', 'rate', 'invention', 'provides', ',', 'variety', 'monolithic', 'technologies', 'may', 'considered', 'second', 'tier', ',', 'cmos', ',', 'silicon', 'bipolar', ',', 'mos', 'vlsi', 'integrated', 'circuits', '.']\n",
            "    After Regex: ['due', 'reduced', 'output', 'data', 'rate', 'invention', 'provides', 'variety', 'monolithic', 'technologies', 'may', 'considered', 'second', 'tier', 'cmos', 'silicon', 'bipolar', 'mos', 'vlsi', 'integrated', 'circuits']\n",
            "    After Lemmatization: ['due', 'reduced', 'output', 'data', 'rate', 'invention', 'provides', 'variety', 'monolithic', 'technology', 'may', 'considered', 'second', 'tier', 'cmos', 'silicon', 'bipolar', 'mo', 'vlsi', 'integrated', 'circuit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9433\n",
            "    After Stopword Removal: ['focus', 'understanding', 'managing', 'occupational', 'therapy', 'practice', ',', 'designing', 'effective', 'client', '-', 'centred', 'interventions', 'becoming', 'safe', 'competent', 'occupational', 'therapist', '.']\n",
            "    After Regex: ['focus', 'understanding', 'managing', 'occupational', 'therapy', 'practice', 'designing', 'effective', 'client', 'centred', 'interventions', 'becoming', 'safe', 'competent', 'occupational', 'therapist']\n",
            "    After Lemmatization: ['focus', 'understanding', 'managing', 'occupational', 'therapy', 'practice', 'designing', 'effective', 'client', 'centred', 'intervention', 'becoming', 'safe', 'competent', 'occupational', 'therapist']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9434\n",
            "    After Stopword Removal: ['vitamin', 'c', 'enhances', 'iron', 'absorption', ',', 'try', 'include', 'plenty', 'sources', 'vitamin', 'c', 'child', \"'\", 'diet', '.']\n",
            "    After Regex: ['vitamin', 'c', 'enhances', 'iron', 'absorption', 'try', 'include', 'plenty', 'sources', 'vitamin', 'c', 'child', 'diet']\n",
            "    After Lemmatization: ['vitamin', 'c', 'enhances', 'iron', 'absorption', 'try', 'include', 'plenty', 'source', 'vitamin', 'c', 'child', 'diet']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9435\n",
            "    After Stopword Removal: ['recommend', 'yes', 'vote', 'measure', 'n', '.', 'bond', 'would', 'biggest', 'yet', 'school', 'district', ',', '2004', 'campaigned', 'successfully', '$140', 'million', 'bond', 'build', 'new', 'schools', '.']\n",
            "    After Regex: ['recommend', 'yes', 'vote', 'measure', 'n', 'bond', 'would', 'biggest', 'yet', 'school', 'district', 'campaigned', 'successfully', 'million', 'bond', 'build', 'new', 'schools']\n",
            "    After Lemmatization: ['recommend', 'yes', 'vote', 'measure', 'n', 'bond', 'would', 'biggest', 'yet', 'school', 'district', 'campaigned', 'successfully', 'million', 'bond', 'build', 'new', 'school']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9436\n",
            "    After Stopword Removal: ['low', 'grip', 'situations', ',', 'sensors', 'trigger', 'system', 'transfer', 'incremental', 'engine', 'torque', 'rear', 'wheels', 'applying', 'braking', 'force', 'inside', 'rear', 'wheel', '.']\n",
            "    After Regex: ['low', 'grip', 'situations', 'sensors', 'trigger', 'system', 'transfer', 'incremental', 'engine', 'torque', 'rear', 'wheels', 'applying', 'braking', 'force', 'inside', 'rear', 'wheel']\n",
            "    After Lemmatization: ['low', 'grip', 'situation', 'sensor', 'trigger', 'system', 'transfer', 'incremental', 'engine', 'torque', 'rear', 'wheel', 'applying', 'braking', 'force', 'inside', 'rear', 'wheel']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9437\n",
            "    After Stopword Removal: ['validation', 'certifies', 'use', 'systems', 'long', 'term', '2', 'enhanced', 'surface', 'water', 'treatment', 'rule', '(', 'lt', '2', 'eswtr', ')', 'released', 'epa', 'november', '2006.']\n",
            "    After Regex: ['validation', 'certifies', 'use', 'systems', 'long', 'term', 'enhanced', 'surface', 'water', 'treatment', 'rule', 'lt', 'eswtr', 'released', 'epa', 'november']\n",
            "    After Lemmatization: ['validation', 'certifies', 'use', 'system', 'long', 'term', 'enhanced', 'surface', 'water', 'treatment', 'rule', 'lt', 'eswtr', 'released', 'epa', 'november']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9438\n",
            "    After Stopword Removal: ['episode', ',', 'johannes', 'de', 'silentio', ',', 'pseudonym', 'author', 'work', ',', 'composes', 'ode', 'faith', 'greatest', 'passions', ',', 'enquires', 'problem', ',', 'interpretation', ',', 'seems', 'central', ',', '.', 'e', '.,', 'teleological', 'suspension', 'ethics', '.']\n",
            "    After Regex: ['episode', 'johannes', 'de', 'silentio', 'pseudonym', 'author', 'work', 'composes', 'ode', 'faith', 'greatest', 'passions', 'enquires', 'problem', 'interpretation', 'seems', 'central', 'e', 'teleological', 'suspension', 'ethics']\n",
            "    After Lemmatization: ['episode', 'johannes', 'de', 'silentio', 'pseudonym', 'author', 'work', 'composes', 'ode', 'faith', 'greatest', 'passion', 'enquires', 'problem', 'interpretation', 'seems', 'central', 'e', 'teleological', 'suspension', 'ethic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9439\n",
            "    After Stopword Removal: ['job', 'losses', 'nothing', 'environmental', 'protection', ',', 'must', 'sure', 'demands', 'cleaning', 'planet', 'include', 'concrete', 'measures', 'create', 'jobs', 'need', '.']\n",
            "    After Regex: ['job', 'losses', 'nothing', 'environmental', 'protection', 'must', 'sure', 'demands', 'cleaning', 'planet', 'include', 'concrete', 'measures', 'create', 'jobs', 'need']\n",
            "    After Lemmatization: ['job', 'loss', 'nothing', 'environmental', 'protection', 'must', 'sure', 'demand', 'cleaning', 'planet', 'include', 'concrete', 'measure', 'create', 'job', 'need']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9440\n",
            "    After Stopword Removal: ['biggest', 'challenge', 'rejuvenation', 'healthcare', 'system', 'garnering', 'funds', '.']\n",
            "    After Regex: ['biggest', 'challenge', 'rejuvenation', 'healthcare', 'system', 'garnering', 'funds']\n",
            "    After Lemmatization: ['biggest', 'challenge', 'rejuvenation', 'healthcare', 'system', 'garnering', 'fund']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9441\n",
            "    After Stopword Removal: ['treatment', 'starts', ',', 'ask', 'health', 'care', 'team', 'possible', 'side', 'effects', 'treatment', 'may', 'change', 'normal', 'activities', '.']\n",
            "    After Regex: ['treatment', 'starts', 'ask', 'health', 'care', 'team', 'possible', 'side', 'effects', 'treatment', 'may', 'change', 'normal', 'activities']\n",
            "    After Lemmatization: ['treatment', 'start', 'ask', 'health', 'care', 'team', 'possible', 'side', 'effect', 'treatment', 'may', 'change', 'normal', 'activity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9442\n",
            "    After Stopword Removal: ['searching', 'ingredients', 'building', 'farms', 'millions', 'dollars', 'worth', 'capital', '.']\n",
            "    After Regex: ['searching', 'ingredients', 'building', 'farms', 'millions', 'dollars', 'worth', 'capital']\n",
            "    After Lemmatization: ['searching', 'ingredient', 'building', 'farm', 'million', 'dollar', 'worth', 'capital']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9443\n",
            "    After Stopword Removal: ['rash', 'commonly', 'seen', 'face', ',', 'especially', 'around', 'mouth', 'occur', 'anywhere', 'body', '.']\n",
            "    After Regex: ['rash', 'commonly', 'seen', 'face', 'especially', 'around', 'mouth', 'occur', 'anywhere', 'body']\n",
            "    After Lemmatization: ['rash', 'commonly', 'seen', 'face', 'especially', 'around', 'mouth', 'occur', 'anywhere', 'body']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9444\n",
            "    After Stopword Removal: ['said', 'men', 'sodom', ',', 'sinners', 'exceedingly', ',', 'sinners', 'lord', ';', ',', 'eye', '-', 'sight', ',', 'notwithstanding', 'kindnesses', 'shewed', ';', 'land', 'sodom', ',', 'like', 'garden', 'eden', 'heretofore', '.']\n",
            "    After Regex: ['said', 'men', 'sodom', 'sinners', 'exceedingly', 'sinners', 'lord', 'eye', 'sight', 'notwithstanding', 'kindnesses', 'shewed', 'land', 'sodom', 'like', 'garden', 'eden', 'heretofore']\n",
            "    After Lemmatization: ['said', 'men', 'sodom', 'sinner', 'exceedingly', 'sinner', 'lord', 'eye', 'sight', 'notwithstanding', 'kindness', 'shewed', 'land', 'sodom', 'like', 'garden', 'eden', 'heretofore']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9445\n",
            "    After Stopword Removal: ['quality', 'quantity', 'samples', 'significant', 'impact', 'accuracy', 'forecasting', 'data', 'increase', 'difficulty', 'real', 'operation', ',', 'less', 'data', 'may', 'contain', 'enough', 'information', 'analysis', '.']\n",
            "    After Regex: ['quality', 'quantity', 'samples', 'significant', 'impact', 'accuracy', 'forecasting', 'data', 'increase', 'difficulty', 'real', 'operation', 'less', 'data', 'may', 'contain', 'enough', 'information', 'analysis']\n",
            "    After Lemmatization: ['quality', 'quantity', 'sample', 'significant', 'impact', 'accuracy', 'forecasting', 'data', 'increase', 'difficulty', 'real', 'operation', 'less', 'data', 'may', 'contain', 'enough', 'information', 'analysis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9446\n",
            "    After Stopword Removal: ['pitiless', 'vision', '—', 'stressed', 'blind', 'chance', 'main', 'determiner', 'struggle', 'survival', 'course', 'evolution', '—', 'upsetting', 'victorians', 'put', 'faith', 'self', '-', 'help', 'hard', 'work', '.']\n",
            "    After Regex: ['pitiless', 'vision', 'stressed', 'blind', 'chance', 'main', 'determiner', 'struggle', 'survival', 'course', 'evolution', 'upsetting', 'victorians', 'put', 'faith', 'self', 'help', 'hard', 'work']\n",
            "    After Lemmatization: ['pitiless', 'vision', 'stressed', 'blind', 'chance', 'main', 'determiner', 'struggle', 'survival', 'course', 'evolution', 'upsetting', 'victorian', 'put', 'faith', 'self', 'help', 'hard', 'work']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9447\n",
            "    After Stopword Removal: ['sequencing', 'virus', 'demonstrated', 'combination', 'human', 'swine', 'genes', 'identified', 'previously', 'circulating', 'influenza', 'viruses', '.']\n",
            "    After Regex: ['sequencing', 'virus', 'demonstrated', 'combination', 'human', 'swine', 'genes', 'identified', 'previously', 'circulating', 'influenza', 'viruses']\n",
            "    After Lemmatization: ['sequencing', 'virus', 'demonstrated', 'combination', 'human', 'swine', 'gene', 'identified', 'previously', 'circulating', 'influenza', 'virus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9448\n",
            "    After Stopword Removal: [\"'\", 'worth', 'remembering', 'idea', 'actually', 'came', '.']\n",
            "    After Regex: ['worth', 'remembering', 'idea', 'actually', 'came']\n",
            "    After Lemmatization: ['worth', 'remembering', 'idea', 'actually', 'came']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9449\n",
            "    After Stopword Removal: ['developing', 'global', \"'\", 'flat', 'classroom', \"'\", 'projects', '-', 'collaborative', 'lessons', 'use', 'web', '2.0', 'technologies', 'podcasts', ',', 'blogs', 'digital', 'video', '-', 'help', 'every', 'girls', 'learn', 'key', 'twenty', '-', 'first', 'century', 'skills', 'communication', ',', 'collaboration', 'creation', '.']\n",
            "    After Regex: ['developing', 'global', 'flat', 'classroom', 'projects', 'collaborative', 'lessons', 'use', 'web', 'technologies', 'podcasts', 'blogs', 'digital', 'video', 'help', 'every', 'girls', 'learn', 'key', 'twenty', 'first', 'century', 'skills', 'communication', 'collaboration', 'creation']\n",
            "    After Lemmatization: ['developing', 'global', 'flat', 'classroom', 'project', 'collaborative', 'lesson', 'use', 'web', 'technology', 'podcasts', 'blog', 'digital', 'video', 'help', 'every', 'girl', 'learn', 'key', 'twenty', 'first', 'century', 'skill', 'communication', 'collaboration', 'creation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9450\n",
            "    After Stopword Removal: ['digital', 'ports', ',', 'however', ',', 'convert', 'input', 'voltage', 'two', 'output', 'values', ',', 'zero', 'one', '.']\n",
            "    After Regex: ['digital', 'ports', 'however', 'convert', 'input', 'voltage', 'two', 'output', 'values', 'zero', 'one']\n",
            "    After Lemmatization: ['digital', 'port', 'however', 'convert', 'input', 'voltage', 'two', 'output', 'value', 'zero', 'one']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9451\n",
            "    After Stopword Removal: ['provide', 'affordable', ',', 'simple', ',', 'accurate', 'recordation', 'horse', 'lineage', 'without', 'conflict', 'expense', 'breed', 'associations', ',', 'boards', ',', 'politics', '.']\n",
            "    After Regex: ['provide', 'affordable', 'simple', 'accurate', 'recordation', 'horse', 'lineage', 'without', 'conflict', 'expense', 'breed', 'associations', 'boards', 'politics']\n",
            "    After Lemmatization: ['provide', 'affordable', 'simple', 'accurate', 'recordation', 'horse', 'lineage', 'without', 'conflict', 'expense', 'breed', 'association', 'board', 'politics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9452\n",
            "    After Stopword Removal: ['includes', 'ferrous', 'base', 'alloy', 'metals', 'well', 'commonly', 'used', 'metals', ',', 'plastics', ',', 'woods', ',', 'glass', '.']\n",
            "    After Regex: ['includes', 'ferrous', 'base', 'alloy', 'metals', 'well', 'commonly', 'used', 'metals', 'plastics', 'woods', 'glass']\n",
            "    After Lemmatization: ['includes', 'ferrous', 'base', 'alloy', 'metal', 'well', 'commonly', 'used', 'metal', 'plastic', 'wood', 'glass']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9453\n",
            "    After Stopword Removal: ['town', 'dwarka', 'gujarat', 'history', 'dates', 'back', 'centuries', 'mentioned', 'mahabharat', 'epic', 'dwaraka', 'kingdom', '.']\n",
            "    After Regex: ['town', 'dwarka', 'gujarat', 'history', 'dates', 'back', 'centuries', 'mentioned', 'mahabharat', 'epic', 'dwaraka', 'kingdom']\n",
            "    After Lemmatization: ['town', 'dwarka', 'gujarat', 'history', 'date', 'back', 'century', 'mentioned', 'mahabharat', 'epic', 'dwaraka', 'kingdom']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9454\n",
            "    After Stopword Removal: ['costs', 'programs', 'differ', '.']\n",
            "    After Regex: ['costs', 'programs', 'differ']\n",
            "    After Lemmatization: ['cost', 'program', 'differ']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9455\n",
            "    After Stopword Removal: ['sheriff', 'parnell', 'mcnamara', 'says', '47', 'microwaves', 'posed', 'safety', 'risk', 'inmates', 'thrown', 'hot', 'liquids', 'others', '.']\n",
            "    After Regex: ['sheriff', 'parnell', 'mcnamara', 'says', 'microwaves', 'posed', 'safety', 'risk', 'inmates', 'thrown', 'hot', 'liquids', 'others']\n",
            "    After Lemmatization: ['sheriff', 'parnell', 'mcnamara', 'say', 'microwave', 'posed', 'safety', 'risk', 'inmate', 'thrown', 'hot', 'liquid', 'others']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9456\n",
            "    After Stopword Removal: ['facilities', 'cheaper', 'affordable', 'poor', 'families', ',', 'always', '-', 'crowded', 'patients', '.']\n",
            "    After Regex: ['facilities', 'cheaper', 'affordable', 'poor', 'families', 'always', 'crowded', 'patients']\n",
            "    After Lemmatization: ['facility', 'cheaper', 'affordable', 'poor', 'family', 'always', 'crowded', 'patient']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9457\n",
            "    After Stopword Removal: ['cannabinoids', 'look', '?']\n",
            "    After Regex: ['cannabinoids', 'look']\n",
            "    After Lemmatization: ['cannabinoids', 'look']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9458\n",
            "    After Stopword Removal: ['search', 'committee', 'seeks', 'candidate', 'fit', 'particular', 'kind', 'institution', ',', 'uniquely', 'constituted', 'history', 'department', ',', 'special', 'slot', 'within', 'curriculum', '.']\n",
            "    After Regex: ['search', 'committee', 'seeks', 'candidate', 'fit', 'particular', 'kind', 'institution', 'uniquely', 'constituted', 'history', 'department', 'special', 'slot', 'within', 'curriculum']\n",
            "    After Lemmatization: ['search', 'committee', 'seek', 'candidate', 'fit', 'particular', 'kind', 'institution', 'uniquely', 'constituted', 'history', 'department', 'special', 'slot', 'within', 'curriculum']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9459\n",
            "    After Stopword Removal: ['seeds', 'placed', '1.3', '2', 'inches', 'apart', '.']\n",
            "    After Regex: ['seeds', 'placed', 'inches', 'apart']\n",
            "    After Lemmatization: ['seed', 'placed', 'inch', 'apart']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9460\n",
            "    After Stopword Removal: ['officials', 'southern', 'association', 'colleges', 'schools', '(', 'sacs', ')', 'announced', 'friday', 'clayton', 'county', 'school', 'system', 'regained', 'accreditation', '.']\n",
            "    After Regex: ['officials', 'southern', 'association', 'colleges', 'schools', 'sacs', 'announced', 'friday', 'clayton', 'county', 'school', 'system', 'regained', 'accreditation']\n",
            "    After Lemmatization: ['official', 'southern', 'association', 'college', 'school', 'sac', 'announced', 'friday', 'clayton', 'county', 'school', 'system', 'regained', 'accreditation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9461\n",
            "    After Stopword Removal: ['advancements', 'technology', 'past', 'decades', 'impacted', 'dentistry', 'well', 'areas', 'medical', 'care', '.']\n",
            "    After Regex: ['advancements', 'technology', 'past', 'decades', 'impacted', 'dentistry', 'well', 'areas', 'medical', 'care']\n",
            "    After Lemmatization: ['advancement', 'technology', 'past', 'decade', 'impacted', 'dentistry', 'well', 'area', 'medical', 'care']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9462\n",
            "    After Stopword Removal: ['compression', 'shear', 'result', 'creation', \"'\", 'laing', 'cells', \"'\", 'fully', 'tangled', 'initial', 'field', 'distribution', '.']\n",
            "    After Regex: ['compression', 'shear', 'result', 'creation', 'laing', 'cells', 'fully', 'tangled', 'initial', 'field', 'distribution']\n",
            "    After Lemmatization: ['compression', 'shear', 'result', 'creation', 'laing', 'cell', 'fully', 'tangled', 'initial', 'field', 'distribution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9463\n",
            "    After Stopword Removal: ['specialist', 'consultants', 'diagnose', 'treat', 'diseases', 'disorders', 'stomach', ',', 'duodenum', 'small', 'large', 'bowel', '(', 'colon', ').']\n",
            "    After Regex: ['specialist', 'consultants', 'diagnose', 'treat', 'diseases', 'disorders', 'stomach', 'duodenum', 'small', 'large', 'bowel', 'colon']\n",
            "    After Lemmatization: ['specialist', 'consultant', 'diagnose', 'treat', 'disease', 'disorder', 'stomach', 'duodenum', 'small', 'large', 'bowel', 'colon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9464\n",
            "    After Stopword Removal: ['response', 'question', 'whether', 'would', 'visit', 'areas', 'destinations', 'natural', 'disasters', 'occurred', ',78%', 'thais', 'responded', 'barrier', 'travel', 'plans', '.']\n",
            "    After Regex: ['response', 'question', 'whether', 'would', 'visit', 'areas', 'destinations', 'natural', 'disasters', 'occurred', 'thais', 'responded', 'barrier', 'travel', 'plans']\n",
            "    After Lemmatization: ['response', 'question', 'whether', 'would', 'visit', 'area', 'destination', 'natural', 'disaster', 'occurred', 'thai', 'responded', 'barrier', 'travel', 'plan']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9465\n",
            "    After Stopword Removal: ['popular', '18', 'th', 'century', 'style', 'featuring', 'symmetrical', 'fašade', ',', 'prominent', 'front', 'entrance', 'quoins', '(', 'decorative', 'blocks', 'masonry', 'wood', 'set', 'corners', 'house', ').']\n",
            "    After Regex: ['popular', 'th', 'century', 'style', 'featuring', 'symmetrical', 'faade', 'prominent', 'front', 'entrance', 'quoins', 'decorative', 'blocks', 'masonry', 'wood', 'set', 'corners', 'house']\n",
            "    After Lemmatization: ['popular', 'th', 'century', 'style', 'featuring', 'symmetrical', 'faade', 'prominent', 'front', 'entrance', 'quoin', 'decorative', 'block', 'masonry', 'wood', 'set', 'corner', 'house']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9466\n",
            "    After Stopword Removal: ['granite', 'processing', 'flow', 'big', 'granite', 'materials', 'fed', 'jaw', 'crusher', 'evenly', 'gradually', 'vibrating', 'feeder', 'hopper', 'primary', 'crushing', '.']\n",
            "    After Regex: ['granite', 'processing', 'flow', 'big', 'granite', 'materials', 'fed', 'jaw', 'crusher', 'evenly', 'gradually', 'vibrating', 'feeder', 'hopper', 'primary', 'crushing']\n",
            "    After Lemmatization: ['granite', 'processing', 'flow', 'big', 'granite', 'material', 'fed', 'jaw', 'crusher', 'evenly', 'gradually', 'vibrating', 'feeder', 'hopper', 'primary', 'crushing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9467\n",
            "    After Stopword Removal: ['theroux', 'paints', 'portrait', 'man', 'lived', 'fairly', 'solitary', 'existence', 'choice', ',', 'friendly', ',', 'generous', ',', 'apparently', 'comfortable', 'strangers', ',', 'strongly', 'preferring', 'alone', 'time', '.']\n",
            "    After Regex: ['theroux', 'paints', 'portrait', 'man', 'lived', 'fairly', 'solitary', 'existence', 'choice', 'friendly', 'generous', 'apparently', 'comfortable', 'strangers', 'strongly', 'preferring', 'alone', 'time']\n",
            "    After Lemmatization: ['theroux', 'paint', 'portrait', 'man', 'lived', 'fairly', 'solitary', 'existence', 'choice', 'friendly', 'generous', 'apparently', 'comfortable', 'stranger', 'strongly', 'preferring', 'alone', 'time']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9468\n",
            "    After Stopword Removal: ['kepler', 'project', 'typically', 'described', 'terms', 'raw', 'numbers', '.']\n",
            "    After Regex: ['kepler', 'project', 'typically', 'described', 'terms', 'raw', 'numbers']\n",
            "    After Lemmatization: ['kepler', 'project', 'typically', 'described', 'term', 'raw', 'number']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9469\n",
            "    After Stopword Removal: ['adds', 'distance', '.']\n",
            "    After Regex: ['adds', 'distance']\n",
            "    After Lemmatization: ['add', 'distance']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9470\n",
            "    After Stopword Removal: ['two', 'different', 'types', 'water', 'problems', '.']\n",
            "    After Regex: ['two', 'different', 'types', 'water', 'problems']\n",
            "    After Lemmatization: ['two', 'different', 'type', 'water', 'problem']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9471\n",
            "    After Stopword Removal: ['addition', ',', 'women', \"'\", 'blood', 'pressure', 'may', 'rise', 'take', 'hormones', '.']\n",
            "    After Regex: ['addition', 'women', 'blood', 'pressure', 'may', 'rise', 'take', 'hormones']\n",
            "    After Lemmatization: ['addition', 'woman', 'blood', 'pressure', 'may', 'rise', 'take', 'hormone']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9472\n",
            "    After Stopword Removal: ['spider', 'bites', 'occur', 'receives', 'interactions', 'spiders', '.']\n",
            "    After Regex: ['spider', 'bites', 'occur', 'receives', 'interactions', 'spiders']\n",
            "    After Lemmatization: ['spider', 'bite', 'occur', 'receives', 'interaction', 'spider']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9473\n",
            "    After Stopword Removal: ['many', 'instances', ',', 'technological', 'innovation', 'driven', 'u', '.', '.', 'start', '-', 'ups', 'contributes', 'national', 'prosperity', 'also', 'national', 'security', '.']\n",
            "    After Regex: ['many', 'instances', 'technological', 'innovation', 'driven', 'u', 'start', 'ups', 'contributes', 'national', 'prosperity', 'also', 'national', 'security']\n",
            "    After Lemmatization: ['many', 'instance', 'technological', 'innovation', 'driven', 'u', 'start', 'ups', 'contributes', 'national', 'prosperity', 'also', 'national', 'security']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9474\n",
            "    After Stopword Removal: ['israel', 'remains', 'outside', 'npt', ',', 'secretly', 'built', 'sizable', 'arsenal', 'nuclear', 'weapons', ',', 'giving', 'unique', 'status', 'middle', 'eastern', 'country', 'nuclear', 'arms', '.']\n",
            "    After Regex: ['israel', 'remains', 'outside', 'npt', 'secretly', 'built', 'sizable', 'arsenal', 'nuclear', 'weapons', 'giving', 'unique', 'status', 'middle', 'eastern', 'country', 'nuclear', 'arms']\n",
            "    After Lemmatization: ['israel', 'remains', 'outside', 'npt', 'secretly', 'built', 'sizable', 'arsenal', 'nuclear', 'weapon', 'giving', 'unique', 'status', 'middle', 'eastern', 'country', 'nuclear', 'arm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9475\n",
            "    After Stopword Removal: ['ages', 'richly', 'water', 'resistant', '.']\n",
            "    After Regex: ['ages', 'richly', 'water', 'resistant']\n",
            "    After Lemmatization: ['age', 'richly', 'water', 'resistant']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9476\n",
            "    After Stopword Removal: ['week', \"'\", 'flood', ',', 'ashokan', 'reservoir', 'decreased', 'peak', 'flows', 'lower', 'esopus', 'around', '60', 'percent', '.']\n",
            "    After Regex: ['week', 'flood', 'ashokan', 'reservoir', 'decreased', 'peak', 'flows', 'lower', 'esopus', 'around', 'percent']\n",
            "    After Lemmatization: ['week', 'flood', 'ashokan', 'reservoir', 'decreased', 'peak', 'flow', 'lower', 'esopus', 'around', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9477\n",
            "    After Stopword Removal: ['raid', 'britain', \"'\", 'pensioners', ',', 'worth', '£3.3', 'bn', 'next', 'five', 'years', 'represents', 'biggest', 'money', 'spinner', 'treasury', 'budget', '.']\n",
            "    After Regex: ['raid', 'britain', 'pensioners', 'worth', 'bn', 'next', 'five', 'years', 'represents', 'biggest', 'money', 'spinner', 'treasury', 'budget']\n",
            "    After Lemmatization: ['raid', 'britain', 'pensioner', 'worth', 'bn', 'next', 'five', 'year', 'represents', 'biggest', 'money', 'spinner', 'treasury', 'budget']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9478\n",
            "    After Stopword Removal: ['struck', 'first', '2000', 'infecting', '400', 'people', ',', 'nearly', 'half', 'died', '.']\n",
            "    After Regex: ['struck', 'first', 'infecting', 'people', 'nearly', 'half', 'died']\n",
            "    After Lemmatization: ['struck', 'first', 'infecting', 'people', 'nearly', 'half', 'died']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9479\n",
            "    After Stopword Removal: ['ang', '-2', 'implicated', 'cancer', 'development', 'due', 'role', 'angiogenesis', '.']\n",
            "    After Regex: ['ang', 'implicated', 'cancer', 'development', 'due', 'role', 'angiogenesis']\n",
            "    After Lemmatization: ['ang', 'implicated', 'cancer', 'development', 'due', 'role', 'angiogenesis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9480\n",
            "    After Stopword Removal: ['many', 'people', 'technically', 'minded', 'hard', 'time', 'understanding', 'charts', ',', 'graphs', ',', 'plans', 'cad', 'drawings', '.']\n",
            "    After Regex: ['many', 'people', 'technically', 'minded', 'hard', 'time', 'understanding', 'charts', 'graphs', 'plans', 'cad', 'drawings']\n",
            "    After Lemmatization: ['many', 'people', 'technically', 'minded', 'hard', 'time', 'understanding', 'chart', 'graph', 'plan', 'cad', 'drawing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9481\n",
            "    After Stopword Removal: ['study', 'establishes', ',', 'despite', 'introduction', 'defence', 'regulations', 'giving', 'central', 'state', 'control', 'local', 'policing', ',', 'police', 'authority', 'marginalised', 'governance', 'police', '.']\n",
            "    After Regex: ['study', 'establishes', 'despite', 'introduction', 'defence', 'regulations', 'giving', 'central', 'state', 'control', 'local', 'policing', 'police', 'authority', 'marginalised', 'governance', 'police']\n",
            "    After Lemmatization: ['study', 'establishes', 'despite', 'introduction', 'defence', 'regulation', 'giving', 'central', 'state', 'control', 'local', 'policing', 'police', 'authority', 'marginalised', 'governance', 'police']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9482\n",
            "    After Stopword Removal: ['problem', 'monitor', \"'\", 'cope', 'correctly', 'graphics', 'cards', 'bios', 'telling', 'expect', 'picture', '.']\n",
            "    After Regex: ['problem', 'monitor', 'cope', 'correctly', 'graphics', 'cards', 'bios', 'telling', 'expect', 'picture']\n",
            "    After Lemmatization: ['problem', 'monitor', 'cope', 'correctly', 'graphic', 'card', 'bios', 'telling', 'expect', 'picture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9483\n",
            "    After Stopword Removal: ['exists', 'strong', 'correlation', 'percentage', 'fauna', 'planktonics', 'diversity', 'deeper', '-', 'water', 'benthonic', 'association', ',', 'increase', 'upwards', 'section', ',', 'indicating', 'sequence', 'comprises', 'part', 'transgressive', 'systems', 'tract', '.']\n",
            "    After Regex: ['exists', 'strong', 'correlation', 'percentage', 'fauna', 'planktonics', 'diversity', 'deeper', 'water', 'benthonic', 'association', 'increase', 'upwards', 'section', 'indicating', 'sequence', 'comprises', 'part', 'transgressive', 'systems', 'tract']\n",
            "    After Lemmatization: ['exists', 'strong', 'correlation', 'percentage', 'fauna', 'planktonics', 'diversity', 'deeper', 'water', 'benthonic', 'association', 'increase', 'upwards', 'section', 'indicating', 'sequence', 'comprises', 'part', 'transgressive', 'system', 'tract']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9484\n",
            "    After Stopword Removal: ['pediatricians', \"'\", 'like', 'anyway', ',', 'citing', 'concerns', 'might', 'delay', 'babies', \"'\", 'development', '.']\n",
            "    After Regex: ['pediatricians', 'like', 'anyway', 'citing', 'concerns', 'might', 'delay', 'babies', 'development']\n",
            "    After Lemmatization: ['pediatrician', 'like', 'anyway', 'citing', 'concern', 'might', 'delay', 'baby', 'development']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9485\n",
            "    After Stopword Removal: ['ribat', 'monastir', 'religious', 'military', 'monument', 'built', 'end', '8', 'th', 'century', '10', 'th', 'century', 'one', 'largest', 'military', 'constructions', 'islamic', 'tunisia', '.']\n",
            "    After Regex: ['ribat', 'monastir', 'religious', 'military', 'monument', 'built', 'end', 'th', 'century', 'th', 'century', 'one', 'largest', 'military', 'constructions', 'islamic', 'tunisia']\n",
            "    After Lemmatization: ['ribat', 'monastir', 'religious', 'military', 'monument', 'built', 'end', 'th', 'century', 'th', 'century', 'one', 'largest', 'military', 'construction', 'islamic', 'tunisia']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9486\n",
            "    After Stopword Removal: ['historical', 'gold', 'silver', 'bullion', 'charts', ':', 'past', '10', 'years', 'silver', 'prices', 'increased', 'two', 'fold', 'gone', '$10', 'years', '$30', 'ounce', '.']\n",
            "    After Regex: ['historical', 'gold', 'silver', 'bullion', 'charts', 'past', 'years', 'silver', 'prices', 'increased', 'two', 'fold', 'gone', 'years', 'ounce']\n",
            "    After Lemmatization: ['historical', 'gold', 'silver', 'bullion', 'chart', 'past', 'year', 'silver', 'price', 'increased', 'two', 'fold', 'gone', 'year', 'ounce']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9487\n",
            "    After Stopword Removal: ['whether', 'contemplation', 'art', 'via', 'means', ',', 'suspect', 'time', 'spend', 'daydreaming', 'reckoning', 'man', 'nature', ',', 'greater', 'possibility', 'begin', 'deliberate', 'existence', 'means', 'progression', 'earth', \"'\", 'landscapes', '-', 'place', 'among', '.']\n",
            "    After Regex: ['whether', 'contemplation', 'art', 'via', 'means', 'suspect', 'time', 'spend', 'daydreaming', 'reckoning', 'man', 'nature', 'greater', 'possibility', 'begin', 'deliberate', 'existence', 'means', 'progression', 'earth', 'landscapes', 'place', 'among']\n",
            "    After Lemmatization: ['whether', 'contemplation', 'art', 'via', 'mean', 'suspect', 'time', 'spend', 'daydreaming', 'reckoning', 'man', 'nature', 'greater', 'possibility', 'begin', 'deliberate', 'existence', 'mean', 'progression', 'earth', 'landscape', 'place', 'among']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9488\n",
            "    After Stopword Removal: ['scenes', 'found', 'golden', 'legend', ',', 'popular', 'source', 'saints', \"'\", 'lives', 'later', 'middle', 'ages', ',', 'appear', 'artist', \"'\", 'source', '.']\n",
            "    After Regex: ['scenes', 'found', 'golden', 'legend', 'popular', 'source', 'saints', 'lives', 'later', 'middle', 'ages', 'appear', 'artist', 'source']\n",
            "    After Lemmatization: ['scene', 'found', 'golden', 'legend', 'popular', 'source', 'saint', 'life', 'later', 'middle', 'age', 'appear', 'artist', 'source']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9489\n",
            "    After Stopword Removal: ['europe', ',', \"'\", 'secret', 'vast', 'majority', 'people', 'either', 'bilingual', 'trilingual', '.']\n",
            "    After Regex: ['europe', 'secret', 'vast', 'majority', 'people', 'either', 'bilingual', 'trilingual']\n",
            "    After Lemmatization: ['europe', 'secret', 'vast', 'majority', 'people', 'either', 'bilingual', 'trilingual']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9490\n",
            "    After Stopword Removal: ['53%', 'weaving', 'machines', '.']\n",
            "    After Regex: ['weaving', 'machines']\n",
            "    After Lemmatization: ['weaving', 'machine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9491\n",
            "    After Stopword Removal: ['dogmatic', 'divisions', 'shia', 'sunni', 'muslims', 'include', 'differences', 'place', 'prophet', 'mohamed', \"'\", 'companions', 'side', 'favouring', 'others', '.']\n",
            "    After Regex: ['dogmatic', 'divisions', 'shia', 'sunni', 'muslims', 'include', 'differences', 'place', 'prophet', 'mohamed', 'companions', 'side', 'favouring', 'others']\n",
            "    After Lemmatization: ['dogmatic', 'division', 'shia', 'sunni', 'muslim', 'include', 'difference', 'place', 'prophet', 'mohamed', 'companion', 'side', 'favouring', 'others']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9492\n",
            "    After Stopword Removal: ['look', 'apr', '(', 'annual', 'percentage', 'ratio', ').']\n",
            "    After Regex: ['look', 'apr', 'annual', 'percentage', 'ratio']\n",
            "    After Lemmatization: ['look', 'apr', 'annual', 'percentage', 'ratio']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9493\n",
            "    After Stopword Removal: ['betz', ',', 'hans', '-', 'georg', ';', 'radical', 'right', '-', 'wing', 'populism', 'western', 'europe', ';(', 'london', ',1994).']\n",
            "    After Regex: ['betz', 'hans', 'georg', 'radical', 'right', 'wing', 'populism', 'western', 'europe', 'london']\n",
            "    After Lemmatization: ['betz', 'han', 'georg', 'radical', 'right', 'wing', 'populism', 'western', 'europe', 'london']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9494\n",
            "    After Stopword Removal: ['project', 'address', 'key', 'network', 'issues', 'limit', 'internet', 'performance', 'work', 'toward', 'effectively', 'removing', 'barriers', '.']\n",
            "    After Regex: ['project', 'address', 'key', 'network', 'issues', 'limit', 'internet', 'performance', 'work', 'toward', 'effectively', 'removing', 'barriers']\n",
            "    After Lemmatization: ['project', 'address', 'key', 'network', 'issue', 'limit', 'internet', 'performance', 'work', 'toward', 'effectively', 'removing', 'barrier']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9495\n",
            "    After Stopword Removal: ['airborne', 'refers', 'particles', 'float', 'air', '.']\n",
            "    After Regex: ['airborne', 'refers', 'particles', 'float', 'air']\n",
            "    After Lemmatization: ['airborne', 'refers', 'particle', 'float', 'air']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9496\n",
            "    After Stopword Removal: ['tips', 'help', 'know', 'something', 'right', '.']\n",
            "    After Regex: ['tips', 'help', 'know', 'something', 'right']\n",
            "    After Lemmatization: ['tip', 'help', 'know', 'something', 'right']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9497\n",
            "    After Stopword Removal: ['one', 'original', 'battle', 'stars', 'used', 'first', 'cylon', 'war', 'built', 'specifically', 'fighting', 'cylons', 'old', 'mind', '.']\n",
            "    After Regex: ['one', 'original', 'battle', 'stars', 'used', 'first', 'cylon', 'war', 'built', 'specifically', 'fighting', 'cylons', 'old', 'mind']\n",
            "    After Lemmatization: ['one', 'original', 'battle', 'star', 'used', 'first', 'cylon', 'war', 'built', 'specifically', 'fighting', 'cylons', 'old', 'mind']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9498\n",
            "    After Stopword Removal: ['instrument', 'describing', 'curves', ',', 'measuring', ',', 'etc', '.']\n",
            "    After Regex: ['instrument', 'describing', 'curves', 'measuring', 'etc']\n",
            "    After Lemmatization: ['instrument', 'describing', 'curve', 'measuring', 'etc']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9499\n",
            "    After Stopword Removal: ['social', 'technologies', 'bring', 'scope', ',', 'scale', ',', 'economics', 'internet', 'human', 'interactions', ',', 'successful', 'transformation', 'ultimately', 'rest', 'practices', 'culture', '.']\n",
            "    After Regex: ['social', 'technologies', 'bring', 'scope', 'scale', 'economics', 'internet', 'human', 'interactions', 'successful', 'transformation', 'ultimately', 'rest', 'practices', 'culture']\n",
            "    After Lemmatization: ['social', 'technology', 'bring', 'scope', 'scale', 'economics', 'internet', 'human', 'interaction', 'successful', 'transformation', 'ultimately', 'rest', 'practice', 'culture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9500\n",
            "    After Stopword Removal: ['species', 'tolerate', 'frost', 'may', 'die', 'ground', 'winter', '.']\n",
            "    After Regex: ['species', 'tolerate', 'frost', 'may', 'die', 'ground', 'winter']\n",
            "    After Lemmatization: ['specie', 'tolerate', 'frost', 'may', 'die', 'ground', 'winter']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9501\n",
            "    After Stopword Removal: ['beam', 'hits', 'hard', 'ground', ',', 'bounce', 'around', 'environment', '.']\n",
            "    After Regex: ['beam', 'hits', 'hard', 'ground', 'bounce', 'around', 'environment']\n",
            "    After Lemmatization: ['beam', 'hit', 'hard', 'ground', 'bounce', 'around', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9502\n",
            "    After Stopword Removal: ['salt', 'precipitated', 'material', 'examined', 'presence', 'saf', \"'\", \"'\", 'clearly', 'detectable', 'fibrilar', 'structures', 'thus', 'making', 'protocol', 'improper', 'detection', '.']\n",
            "    After Regex: ['salt', 'precipitated', 'material', 'examined', 'presence', 'saf', 'clearly', 'detectable', 'fibrilar', 'structures', 'thus', 'making', 'protocol', 'improper', 'detection']\n",
            "    After Lemmatization: ['salt', 'precipitated', 'material', 'examined', 'presence', 'saf', 'clearly', 'detectable', 'fibrilar', 'structure', 'thus', 'making', 'protocol', 'improper', 'detection']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9503\n",
            "    After Stopword Removal: ['usa', '&', 'china', 'emerged', 'leaders', 'field', 'ai', 'employing', 'comprehensive', 'resources', 'development', 'view', 'commercial', 'interest', 'national', 'security', '.']\n",
            "    After Regex: ['usa', 'china', 'emerged', 'leaders', 'field', 'ai', 'employing', 'comprehensive', 'resources', 'development', 'view', 'commercial', 'interest', 'national', 'security']\n",
            "    After Lemmatization: ['usa', 'china', 'emerged', 'leader', 'field', 'ai', 'employing', 'comprehensive', 'resource', 'development', 'view', 'commercial', 'interest', 'national', 'security']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9504\n",
            "    After Stopword Removal: ['name', \"'\", 'tachograph', \"'\", 'comes', 'graphical', 'recording', 'tachometer', 'engine', 'speed', '.']\n",
            "    After Regex: ['name', 'tachograph', 'comes', 'graphical', 'recording', 'tachometer', 'engine', 'speed']\n",
            "    After Lemmatization: ['name', 'tachograph', 'come', 'graphical', 'recording', 'tachometer', 'engine', 'speed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9505\n",
            "    After Stopword Removal: ['ent', 'specialists', ',', 'dermatologists', ',', 'oral', '&', 'maxillofacial', 'surgeons', 'plastic', 'surgeons', '.']\n",
            "    After Regex: ['ent', 'specialists', 'dermatologists', 'oral', 'maxillofacial', 'surgeons', 'plastic', 'surgeons']\n",
            "    After Lemmatization: ['ent', 'specialist', 'dermatologist', 'oral', 'maxillofacial', 'surgeon', 'plastic', 'surgeon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9506\n",
            "    After Stopword Removal: ['norway', 'small', ',', 'wealthy', 'country', 'takes', 'protection', 'natural', 'environment', 'seriously', 'culturally', 'happy', 'government', 'control', 'industry', '.']\n",
            "    After Regex: ['norway', 'small', 'wealthy', 'country', 'takes', 'protection', 'natural', 'environment', 'seriously', 'culturally', 'happy', 'government', 'control', 'industry']\n",
            "    After Lemmatization: ['norway', 'small', 'wealthy', 'country', 'take', 'protection', 'natural', 'environment', 'seriously', 'culturally', 'happy', 'government', 'control', 'industry']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9507\n",
            "    After Stopword Removal: ['scientists', 'must', 'look', 'land', 'use', 'changes', 'promoting', 'atmospheric', 'carbon', 'capture', 'storage', ':', 'example', 'greening', 'deserts', 'constituting', 'almost', '1/3', 'earth', 'nonpermafrost', 'land', '.']\n",
            "    After Regex: ['scientists', 'must', 'look', 'land', 'use', 'changes', 'promoting', 'atmospheric', 'carbon', 'capture', 'storage', 'example', 'greening', 'deserts', 'constituting', 'almost', 'earth', 'nonpermafrost', 'land']\n",
            "    After Lemmatization: ['scientist', 'must', 'look', 'land', 'use', 'change', 'promoting', 'atmospheric', 'carbon', 'capture', 'storage', 'example', 'greening', 'desert', 'constituting', 'almost', 'earth', 'nonpermafrost', 'land']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9508\n",
            "    After Stopword Removal: ['decision', 'breastfeed', '/', 'formula', 'feed', 'based', 'comfort', 'level', ',', 'lifestyle', ',', 'specific', 'medical', 'considerations', '.']\n",
            "    After Regex: ['decision', 'breastfeed', 'formula', 'feed', 'based', 'comfort', 'level', 'lifestyle', 'specific', 'medical', 'considerations']\n",
            "    After Lemmatization: ['decision', 'breastfeed', 'formula', 'feed', 'based', 'comfort', 'level', 'lifestyle', 'specific', 'medical', 'consideration']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9509\n",
            "    After Stopword Removal: ['project', 'allow', 'astute', 'investors', 'participate', 'agriculture', 'global', 'scale', 'meet', 'increased', 'demand', 'food', ',', 'mitigating', 'effects', 'rising', 'food', 'costs', '.']\n",
            "    After Regex: ['project', 'allow', 'astute', 'investors', 'participate', 'agriculture', 'global', 'scale', 'meet', 'increased', 'demand', 'food', 'mitigating', 'effects', 'rising', 'food', 'costs']\n",
            "    After Lemmatization: ['project', 'allow', 'astute', 'investor', 'participate', 'agriculture', 'global', 'scale', 'meet', 'increased', 'demand', 'food', 'mitigating', 'effect', 'rising', 'food', 'cost']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9510\n",
            "    After Stopword Removal: ['mulching', 'insulate', 'moisture', 'water', 'beautifying', 'garden', '.']\n",
            "    After Regex: ['mulching', 'insulate', 'moisture', 'water', 'beautifying', 'garden']\n",
            "    After Lemmatization: ['mulching', 'insulate', 'moisture', 'water', 'beautifying', 'garden']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9511\n",
            "    After Stopword Removal: [\"'\", 'forget', 'us', 'government', 'could', 'use', 'inflation', 'finance', 'money', 'problems', '.']\n",
            "    After Regex: ['forget', 'us', 'government', 'could', 'use', 'inflation', 'finance', 'money', 'problems']\n",
            "    After Lemmatization: ['forget', 'u', 'government', 'could', 'use', 'inflation', 'finance', 'money', 'problem']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9512\n",
            "    After Stopword Removal: ['grew', 'ate', 'various', 'preparations', 'still', ',', 'usually', 'soups', '.']\n",
            "    After Regex: ['grew', 'ate', 'various', 'preparations', 'still', 'usually', 'soups']\n",
            "    After Lemmatization: ['grew', 'ate', 'various', 'preparation', 'still', 'usually', 'soup']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9513\n",
            "    After Stopword Removal: ['also', 'make', 'difficult', 'get', 'insurance', '.']\n",
            "    After Regex: ['also', 'make', 'difficult', 'get', 'insurance']\n",
            "    After Lemmatization: ['also', 'make', 'difficult', 'get', 'insurance']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9514\n",
            "    After Stopword Removal: [\"'\", 'biggest', 'takeaway', 'suggestion', 'case', 'study', '.']\n",
            "    After Regex: ['biggest', 'takeaway', 'suggestion', 'case', 'study']\n",
            "    After Lemmatization: ['biggest', 'takeaway', 'suggestion', 'case', 'study']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9515\n",
            "    After Stopword Removal: ['citizen', \"'\", 'academy', 'ten', 'week', 'program', 'provides', 'citizens', 'community', 'overview', 'criminal', 'justice', 'system', '.']\n",
            "    After Regex: ['citizen', 'academy', 'ten', 'week', 'program', 'provides', 'citizens', 'community', 'overview', 'criminal', 'justice', 'system']\n",
            "    After Lemmatization: ['citizen', 'academy', 'ten', 'week', 'program', 'provides', 'citizen', 'community', 'overview', 'criminal', 'justice', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9516\n",
            "    After Stopword Removal: ['created', 'less', '-', 'generous', 'pensions', 'newly', 'hired', 'workers', '.']\n",
            "    After Regex: ['created', 'less', 'generous', 'pensions', 'newly', 'hired', 'workers']\n",
            "    After Lemmatization: ['created', 'less', 'generous', 'pension', 'newly', 'hired', 'worker']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9517\n",
            "    After Stopword Removal: ['health', 'problems', 'also', 'increase', 'risk', 'infertility', '.']\n",
            "    After Regex: ['health', 'problems', 'also', 'increase', 'risk', 'infertility']\n",
            "    After Lemmatization: ['health', 'problem', 'also', 'increase', 'risk', 'infertility']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9518\n",
            "    After Stopword Removal: ['southern', 'prairie', 'skink', 'ranges', 'east', '-', 'central', 'texas', 'south', '-', 'central', 'kansas', ',', 'another', 'isolated', 'population', 'far', 'south', 'texas', '(2).', 'top', 'prairie', 'skink', 'habitat', 'primary', 'habitat', 'prairie', 'skink', ',', 'name', 'suggest', ',', 'prairies', 'north', 'america', '(3).']\n",
            "    After Regex: ['southern', 'prairie', 'skink', 'ranges', 'east', 'central', 'texas', 'south', 'central', 'kansas', 'another', 'isolated', 'population', 'far', 'south', 'texas', 'top', 'prairie', 'skink', 'habitat', 'primary', 'habitat', 'prairie', 'skink', 'name', 'suggest', 'prairies', 'north', 'america']\n",
            "    After Lemmatization: ['southern', 'prairie', 'skink', 'range', 'east', 'central', 'texas', 'south', 'central', 'kansa', 'another', 'isolated', 'population', 'far', 'south', 'texas', 'top', 'prairie', 'skink', 'habitat', 'primary', 'habitat', 'prairie', 'skink', 'name', 'suggest', 'prairie', 'north', 'america']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9519\n",
            "    After Stopword Removal: ['previous', 'blogs', ',', 'talked', 'setup', 'node', '.', 'js', 'environment', 'mac', 'describing', 'npm', '.']\n",
            "    After Regex: ['previous', 'blogs', 'talked', 'setup', 'node', 'js', 'environment', 'mac', 'describing', 'npm']\n",
            "    After Lemmatization: ['previous', 'blog', 'talked', 'setup', 'node', 'j', 'environment', 'mac', 'describing', 'npm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9520\n",
            "    After Stopword Removal: ['autophagic', 'circularity', 'works', 'toward', 'self', 'referential', 'reflection', 'nature', 'legitimizing', 'codes', 'contemporary', 'art', '.']\n",
            "    After Regex: ['autophagic', 'circularity', 'works', 'toward', 'self', 'referential', 'reflection', 'nature', 'legitimizing', 'codes', 'contemporary', 'art']\n",
            "    After Lemmatization: ['autophagic', 'circularity', 'work', 'toward', 'self', 'referential', 'reflection', 'nature', 'legitimizing', 'code', 'contemporary', 'art']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9521\n",
            "    After Stopword Removal: ['biological', 'effects', 'produced', 'α', '-', 'tocopheral', 'quinone', '.']\n",
            "    After Regex: ['biological', 'effects', 'produced', 'tocopheral', 'quinone']\n",
            "    After Lemmatization: ['biological', 'effect', 'produced', 'tocopheral', 'quinone']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9522\n",
            "    After Stopword Removal: ['important', 'visit', 'loved', 'one', 'nursing', 'home', 'often', 'possible', 'nursing', 'home', 'staff', 'aware', 'concerned', 'safety', 'loved', 'one', '.']\n",
            "    After Regex: ['important', 'visit', 'loved', 'one', 'nursing', 'home', 'often', 'possible', 'nursing', 'home', 'staff', 'aware', 'concerned', 'safety', 'loved', 'one']\n",
            "    After Lemmatization: ['important', 'visit', 'loved', 'one', 'nursing', 'home', 'often', 'possible', 'nursing', 'home', 'staff', 'aware', 'concerned', 'safety', 'loved', 'one']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9523\n",
            "    After Stopword Removal: ['ret', 'safe', 'natural', 'process', 'designed', 'eliminate', 'virtually', 'form', 'emotional', 'stress', '.']\n",
            "    After Regex: ['ret', 'safe', 'natural', 'process', 'designed', 'eliminate', 'virtually', 'form', 'emotional', 'stress']\n",
            "    After Lemmatization: ['ret', 'safe', 'natural', 'process', 'designed', 'eliminate', 'virtually', 'form', 'emotional', 'stress']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9524\n",
            "    After Stopword Removal: ['committee', 'comprehensive', 'environmental', 'education', 'program', ',', 'composed', 'existing', 'recycling', '&', 'beautification', ',', 'fats', ',', 'oils', '&', 'grease', ',', 'stormwater', 'education', 'water', 'awareness', 'programs', '.']\n",
            "    After Regex: ['committee', 'comprehensive', 'environmental', 'education', 'program', 'composed', 'existing', 'recycling', 'beautification', 'fats', 'oils', 'grease', 'stormwater', 'education', 'water', 'awareness', 'programs']\n",
            "    After Lemmatization: ['committee', 'comprehensive', 'environmental', 'education', 'program', 'composed', 'existing', 'recycling', 'beautification', 'fat', 'oil', 'grease', 'stormwater', 'education', 'water', 'awareness', 'program']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9525\n",
            "    After Stopword Removal: ['bunny', 'vaccinated', '?']\n",
            "    After Regex: ['bunny', 'vaccinated']\n",
            "    After Lemmatization: ['bunny', 'vaccinated']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9526\n",
            "    After Stopword Removal: ['inverters', 'complex', 'part', 'system', 'replaced', 'panels', 'lose', '15%', 'output', 'capacity', '.']\n",
            "    After Regex: ['inverters', 'complex', 'part', 'system', 'replaced', 'panels', 'lose', 'output', 'capacity']\n",
            "    After Lemmatization: ['inverter', 'complex', 'part', 'system', 'replaced', 'panel', 'lose', 'output', 'capacity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9527\n",
            "    After Stopword Removal: ['silvia', 'lindtner', \"'\", 'research', 'fascinating', ',', 'research', 'suggests', 'innovation', 'china', 'may', 'come', 'computer', 'industry', 'know', ',', 'may', 'come', 'loose', 'forms', 'transnational', 'chinese', 'breathe', 'design', ',', 'art', ',', 'tech', '.']\n",
            "    After Regex: ['silvia', 'lindtner', 'research', 'fascinating', 'research', 'suggests', 'innovation', 'china', 'may', 'come', 'computer', 'industry', 'know', 'may', 'come', 'loose', 'forms', 'transnational', 'chinese', 'breathe', 'design', 'art', 'tech']\n",
            "    After Lemmatization: ['silvia', 'lindtner', 'research', 'fascinating', 'research', 'suggests', 'innovation', 'china', 'may', 'come', 'computer', 'industry', 'know', 'may', 'come', 'loose', 'form', 'transnational', 'chinese', 'breathe', 'design', 'art', 'tech']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9528\n",
            "    After Stopword Removal: ['laparoscopy', ',', 'surgeon', 'makes', 'one', 'small', 'incisions', 'slender', 'surgical', 'instruments', 'passed', '.']\n",
            "    After Regex: ['laparoscopy', 'surgeon', 'makes', 'one', 'small', 'incisions', 'slender', 'surgical', 'instruments', 'passed']\n",
            "    After Lemmatization: ['laparoscopy', 'surgeon', 'make', 'one', 'small', 'incision', 'slender', 'surgical', 'instrument', 'passed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9529\n",
            "    After Stopword Removal: ['60', 'long', '-', 'term', 'usgs', 'streamgages', '(>30', 'years', 'record', ')', 'nebraska', ',12(', '20', 'percent', ')', 'flow', '(', 'slight', 'decrease', 'last', 'week', ').']\n",
            "    After Regex: ['long', 'term', 'usgs', 'streamgages', 'years', 'record', 'nebraska', 'percent', 'flow', 'slight', 'decrease', 'last', 'week']\n",
            "    After Lemmatization: ['long', 'term', 'usgs', 'streamgages', 'year', 'record', 'nebraska', 'percent', 'flow', 'slight', 'decrease', 'last', 'week']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9530\n",
            "    After Stopword Removal: ['valuable', 'work', 'done', 'bletchley', 'park', 'second', 'world', 'war', 'breaking', 'enigma', 'code', 'kept', 'secret', '1974,', 'time', 'many', 'staff', 'could', 'told', 'story', 'died', '.']\n",
            "    After Regex: ['valuable', 'work', 'done', 'bletchley', 'park', 'second', 'world', 'war', 'breaking', 'enigma', 'code', 'kept', 'secret', 'time', 'many', 'staff', 'could', 'told', 'story', 'died']\n",
            "    After Lemmatization: ['valuable', 'work', 'done', 'bletchley', 'park', 'second', 'world', 'war', 'breaking', 'enigma', 'code', 'kept', 'secret', 'time', 'many', 'staff', 'could', 'told', 'story', 'died']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9531\n",
            "    After Stopword Removal: ['rings', 'rise', 'abruptly', 'platform', 'bank', ',', 'placed', ';', 'outer', 'margin', 'invariably', 'bordered', 'living', 'coral', ',*', 'within', 'flat', 'surface', 'coral', '-', 'rock', ';', 'flat', ',', 'sand', 'fragments', 'many', 'cases', 'accumulated', 'converted', 'islets', ',', 'clothed', 'vegetation', '.']\n",
            "    After Regex: ['rings', 'rise', 'abruptly', 'platform', 'bank', 'placed', 'outer', 'margin', 'invariably', 'bordered', 'living', 'coral', 'within', 'flat', 'surface', 'coral', 'rock', 'flat', 'sand', 'fragments', 'many', 'cases', 'accumulated', 'converted', 'islets', 'clothed', 'vegetation']\n",
            "    After Lemmatization: ['ring', 'rise', 'abruptly', 'platform', 'bank', 'placed', 'outer', 'margin', 'invariably', 'bordered', 'living', 'coral', 'within', 'flat', 'surface', 'coral', 'rock', 'flat', 'sand', 'fragment', 'many', 'case', 'accumulated', 'converted', 'islet', 'clothed', 'vegetation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9532\n",
            "    After Stopword Removal: ['tubes', 'used', 'make', 'potable', 'water', 'seawater', 'facility', 'independent', 'water', 'power', 'project', '(', 'iwpp', ')', 'qatar', '.']\n",
            "    After Regex: ['tubes', 'used', 'make', 'potable', 'water', 'seawater', 'facility', 'independent', 'water', 'power', 'project', 'iwpp', 'qatar']\n",
            "    After Lemmatization: ['tube', 'used', 'make', 'potable', 'water', 'seawater', 'facility', 'independent', 'water', 'power', 'project', 'iwpp', 'qatar']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9533\n",
            "    After Stopword Removal: ['body', 'prone', 'problems', 'getting', 'unwell', 'one', '.']\n",
            "    After Regex: ['body', 'prone', 'problems', 'getting', 'unwell', 'one']\n",
            "    After Lemmatization: ['body', 'prone', 'problem', 'getting', 'unwell', 'one']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9534\n",
            "    After Stopword Removal: ['cats', 'carry', 'disease', 'called', 'toxoplasmosis', '.']\n",
            "    After Regex: ['cats', 'carry', 'disease', 'called', 'toxoplasmosis']\n",
            "    After Lemmatization: ['cat', 'carry', 'disease', 'called', 'toxoplasmosis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9535\n",
            "    After Stopword Removal: ['course', 'look', 'expectations', ',', 'appropriate', 'behaviour', ',', 'including', 'self', 'help', 'skills', 'go', 'outline', 'strategies', 'focusing', 'wanted', 'behaviour', '.']\n",
            "    After Regex: ['course', 'look', 'expectations', 'appropriate', 'behaviour', 'including', 'self', 'help', 'skills', 'go', 'outline', 'strategies', 'focusing', 'wanted', 'behaviour']\n",
            "    After Lemmatization: ['course', 'look', 'expectation', 'appropriate', 'behaviour', 'including', 'self', 'help', 'skill', 'go', 'outline', 'strategy', 'focusing', 'wanted', 'behaviour']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9536\n",
            "    After Stopword Removal: ['also', 'found', 'eight', 'per', 'cent', 'patients', 'swat', 'valley', 'infected', 'versions', 'virus', '.']\n",
            "    After Regex: ['also', 'found', 'eight', 'per', 'cent', 'patients', 'swat', 'valley', 'infected', 'versions', 'virus']\n",
            "    After Lemmatization: ['also', 'found', 'eight', 'per', 'cent', 'patient', 'swat', 'valley', 'infected', 'version', 'virus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9537\n",
            "    After Stopword Removal: ['fact', 'filling', 'station', ',', \"'\", 'type', 'gas', 'americans', 'typically', 'use', '.']\n",
            "    After Regex: ['fact', 'filling', 'station', 'type', 'gas', 'americans', 'typically', 'use']\n",
            "    After Lemmatization: ['fact', 'filling', 'station', 'type', 'gas', 'american', 'typically', 'use']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9538\n",
            "    After Stopword Removal: ['computer', 'scientists', '—', 'even', 'programming', 'theorists', '—', 'would', 'deny', 'life', 'computational', 'system', 'utmost', 'importance', '.']\n",
            "    After Regex: ['computer', 'scientists', 'even', 'programming', 'theorists', 'would', 'deny', 'life', 'computational', 'system', 'utmost', 'importance']\n",
            "    After Lemmatization: ['computer', 'scientist', 'even', 'programming', 'theorist', 'would', 'deny', 'life', 'computational', 'system', 'utmost', 'importance']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9539\n",
            "    After Stopword Removal: ['list', '3', 'essential', 'skills', 'nurses', 'need', 'better', 'nursing', 'prevent', 'burnout', '.']\n",
            "    After Regex: ['list', 'essential', 'skills', 'nurses', 'need', 'better', 'nursing', 'prevent', 'burnout']\n",
            "    After Lemmatization: ['list', 'essential', 'skill', 'nurse', 'need', 'better', 'nursing', 'prevent', 'burnout']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9540\n",
            "    After Stopword Removal: ['direct', 'impacts', 'variability', 'large', '-', 'scale', 'physical', 'environment', ',', 'changes', 'ocean', 'circulation', ',', 'suggested', 'main', 'factor', 'generating', 'observed', 'fluctuations', '.']\n",
            "    After Regex: ['direct', 'impacts', 'variability', 'large', 'scale', 'physical', 'environment', 'changes', 'ocean', 'circulation', 'suggested', 'main', 'factor', 'generating', 'observed', 'fluctuations']\n",
            "    After Lemmatization: ['direct', 'impact', 'variability', 'large', 'scale', 'physical', 'environment', 'change', 'ocean', 'circulation', 'suggested', 'main', 'factor', 'generating', 'observed', 'fluctuation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9541\n",
            "    After Stopword Removal: ['essays', '-', 'examining', 'legend', 'good', 'women', ',', 'placing', 'cultural', 'historical', 'context', '.']\n",
            "    After Regex: ['essays', 'examining', 'legend', 'good', 'women', 'placing', 'cultural', 'historical', 'context']\n",
            "    After Lemmatization: ['essay', 'examining', 'legend', 'good', 'woman', 'placing', 'cultural', 'historical', 'context']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9542\n",
            "    After Stopword Removal: ['20', 'million', 'americans', '____(', 'celebrate', ')', 'first', 'earth', 'day', '1970.']\n",
            "    After Regex: ['million', 'americans', 'celebrate', 'first', 'earth', 'day']\n",
            "    After Lemmatization: ['million', 'american', 'celebrate', 'first', 'earth', 'day']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9543\n",
            "    After Stopword Removal: ['televisions', 'equipped', 'satellite', 'channels', '.']\n",
            "    After Regex: ['televisions', 'equipped', 'satellite', 'channels']\n",
            "    After Lemmatization: ['television', 'equipped', 'satellite', 'channel']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9544\n",
            "    After Stopword Removal: ['swype', 'technology', 'calculates', 'word', \"'\", 'intending', 'type', '(', 'learn', ').']\n",
            "    After Regex: ['swype', 'technology', 'calculates', 'word', 'intending', 'type', 'learn']\n",
            "    After Lemmatization: ['swype', 'technology', 'calculates', 'word', 'intending', 'type', 'learn']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9545\n",
            "    After Stopword Removal: ['another', 'issue', 'postal', 'service', 'fund', 'future', 'retiree', 'health', 'benefits', ',', '$11.1', 'billion', 'hit', 'two', '-', 'year', 'installment', 'last', 'year', '.']\n",
            "    After Regex: ['another', 'issue', 'postal', 'service', 'fund', 'future', 'retiree', 'health', 'benefits', 'billion', 'hit', 'two', 'year', 'installment', 'last', 'year']\n",
            "    After Lemmatization: ['another', 'issue', 'postal', 'service', 'fund', 'future', 'retiree', 'health', 'benefit', 'billion', 'hit', 'two', 'year', 'installment', 'last', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9546\n",
            "    After Stopword Removal: ['plaque', 'tartar', 'removed', 'regularly', ',', 'bacteria', 'increase', 'risk', 'gum', 'disease', '.']\n",
            "    After Regex: ['plaque', 'tartar', 'removed', 'regularly', 'bacteria', 'increase', 'risk', 'gum', 'disease']\n",
            "    After Lemmatization: ['plaque', 'tartar', 'removed', 'regularly', 'bacteria', 'increase', 'risk', 'gum', 'disease']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9547\n",
            "    After Stopword Removal: ['new', 'year', ',', 'mark', 'passage', 'time', 'celebration', '.']\n",
            "    After Regex: ['new', 'year', 'mark', 'passage', 'time', 'celebration']\n",
            "    After Lemmatization: ['new', 'year', 'mark', 'passage', 'time', 'celebration']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9548\n",
            "    After Stopword Removal: ['actually', 'means', '15', 'bis', '(', ')', '15', 'bis', '24(', 'b', ')', '.']\n",
            "    After Regex: ['actually', 'means', 'bis', 'bis', 'b']\n",
            "    After Lemmatization: ['actually', 'mean', 'bi', 'bi', 'b']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9549\n",
            "    After Stopword Removal: ['one', 'goals', 'able', 'connect', 'emrs', 'using', 'connectivity', 'standards', '.']\n",
            "    After Regex: ['one', 'goals', 'able', 'connect', 'emrs', 'using', 'connectivity', 'standards']\n",
            "    After Lemmatization: ['one', 'goal', 'able', 'connect', 'emrs', 'using', 'connectivity', 'standard']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9550\n",
            "    After Stopword Removal: ['materials', 'used', 'mattresses', 'include', 'natural', 'talalay', 'latex', ',', 'individual', 'pocket', 'coil', 'springs', ',', 'organic', 'cotton', ',', 'organic', 'wool', '.']\n",
            "    After Regex: ['materials', 'used', 'mattresses', 'include', 'natural', 'talalay', 'latex', 'individual', 'pocket', 'coil', 'springs', 'organic', 'cotton', 'organic', 'wool']\n",
            "    After Lemmatization: ['material', 'used', 'mattress', 'include', 'natural', 'talalay', 'latex', 'individual', 'pocket', 'coil', 'spring', 'organic', 'cotton', 'organic', 'wool']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9551\n",
            "    After Stopword Removal: ['tell', 'doctor', 'pregnant', 'plan', 'become', 'pregnant', '.']\n",
            "    After Regex: ['tell', 'doctor', 'pregnant', 'plan', 'become', 'pregnant']\n",
            "    After Lemmatization: ['tell', 'doctor', 'pregnant', 'plan', 'become', 'pregnant']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9552\n",
            "    After Stopword Removal: ['formula', \"'\", 'even', 'call', 'small', 'amount', 'instant', 'yeast', '(', 'often', 'used', 'sourdough', 'breads', ',', 'kick', '-', 'start', 'fermentation', 'process', ').']\n",
            "    After Regex: ['formula', 'even', 'call', 'small', 'amount', 'instant', 'yeast', 'often', 'used', 'sourdough', 'breads', 'kick', 'start', 'fermentation', 'process']\n",
            "    After Lemmatization: ['formula', 'even', 'call', 'small', 'amount', 'instant', 'yeast', 'often', 'used', 'sourdough', 'bread', 'kick', 'start', 'fermentation', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9553\n",
            "    After Stopword Removal: ['pull', 'string', 'hear', 'authentic', 'sounds', 'education', '.']\n",
            "    After Regex: ['pull', 'string', 'hear', 'authentic', 'sounds', 'education']\n",
            "    After Lemmatization: ['pull', 'string', 'hear', 'authentic', 'sound', 'education']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9554\n",
            "    After Stopword Removal: ['projects', 'may', 'based', 'subject', 'area', 'either', 'teaching', 'research', 'focus', ',', 'may', 'range', 'size', 'provision', 'advice', 'planning', 'project', 'design', 'development', 'whole', 'system', '.']\n",
            "    After Regex: ['projects', 'may', 'based', 'subject', 'area', 'either', 'teaching', 'research', 'focus', 'may', 'range', 'size', 'provision', 'advice', 'planning', 'project', 'design', 'development', 'whole', 'system']\n",
            "    After Lemmatization: ['project', 'may', 'based', 'subject', 'area', 'either', 'teaching', 'research', 'focus', 'may', 'range', 'size', 'provision', 'advice', 'planning', 'project', 'design', 'development', 'whole', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9555\n",
            "    After Stopword Removal: ['severe', 'chromium', 'deficiency', 'seen', 'hospitalized', 'individuals', 'receiving', 'nutrition', 'intravenously', '.']\n",
            "    After Regex: ['severe', 'chromium', 'deficiency', 'seen', 'hospitalized', 'individuals', 'receiving', 'nutrition', 'intravenously']\n",
            "    After Lemmatization: ['severe', 'chromium', 'deficiency', 'seen', 'hospitalized', 'individual', 'receiving', 'nutrition', 'intravenously']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9556\n",
            "    After Stopword Removal: ['knowledge', 'transfer', 'capacity', 'building', 'core', 'part', 'fao', \"'\", 'work', 'field', 'providing', 'technical', 'assistance', 'training', 'build', 'capacity', 'support', 'action', 'country', 'level', '.']\n",
            "    After Regex: ['knowledge', 'transfer', 'capacity', 'building', 'core', 'part', 'fao', 'work', 'field', 'providing', 'technical', 'assistance', 'training', 'build', 'capacity', 'support', 'action', 'country', 'level']\n",
            "    After Lemmatization: ['knowledge', 'transfer', 'capacity', 'building', 'core', 'part', 'fao', 'work', 'field', 'providing', 'technical', 'assistance', 'training', 'build', 'capacity', 'support', 'action', 'country', 'level']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9557\n",
            "    After Stopword Removal: ['soldier', 'attached', 'electrodes', 'palestinian', \"'\", 'neck', 'shocked', ',', 'increasing', 'voltage', 'man', 'begged', 'soldier', 'stop', '.']\n",
            "    After Regex: ['soldier', 'attached', 'electrodes', 'palestinian', 'neck', 'shocked', 'increasing', 'voltage', 'man', 'begged', 'soldier', 'stop']\n",
            "    After Lemmatization: ['soldier', 'attached', 'electrode', 'palestinian', 'neck', 'shocked', 'increasing', 'voltage', 'man', 'begged', 'soldier', 'stop']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9558\n",
            "    After Stopword Removal: ['electrical', 'codes', 'vary', 'place', 'place', '.']\n",
            "    After Regex: ['electrical', 'codes', 'vary', 'place', 'place']\n",
            "    After Lemmatization: ['electrical', 'code', 'vary', 'place', 'place']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9559\n",
            "    After Stopword Removal: ['southern', 'maine', 'liberal', 'northern', 'half', 'state', '.']\n",
            "    After Regex: ['southern', 'maine', 'liberal', 'northern', 'half', 'state']\n",
            "    After Lemmatization: ['southern', 'maine', 'liberal', 'northern', 'half', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9560\n",
            "    After Stopword Removal: ['therefore', ',', 'fields', 'compared', 'terms', 'respective', 'shares', 'degrees', ',', 'percentages', 'add', '100%.']\n",
            "    After Regex: ['therefore', 'fields', 'compared', 'terms', 'respective', 'shares', 'degrees', 'percentages', 'add']\n",
            "    After Lemmatization: ['therefore', 'field', 'compared', 'term', 'respective', 'share', 'degree', 'percentage', 'add']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9561\n",
            "    After Stopword Removal: ['february', '13,', 'year', ',', 'deputy', 'minister', 'education', ',', 'dr', 'yaw', 'osei', 'adutwum', ',', 'announced', 'government', 'started', 'disbursement', '$500', 'million', '(', 'gh', '¢2.45', 'billion', ')', 'realised', 'back', 'inflows', 'ghana', 'education', 'trust', 'fund', '(', 'getfund', '),', 'completion', 'classroom', 'blocks', 'senior', 'high', 'schools', '(', 'shss', ').']\n",
            "    After Regex: ['february', 'year', 'deputy', 'minister', 'education', 'dr', 'yaw', 'osei', 'adutwum', 'announced', 'government', 'started', 'disbursement', 'million', 'gh', 'billion', 'realised', 'back', 'inflows', 'ghana', 'education', 'trust', 'fund', 'getfund', 'completion', 'classroom', 'blocks', 'senior', 'high', 'schools', 'shss']\n",
            "    After Lemmatization: ['february', 'year', 'deputy', 'minister', 'education', 'dr', 'yaw', 'osei', 'adutwum', 'announced', 'government', 'started', 'disbursement', 'million', 'gh', 'billion', 'realised', 'back', 'inflow', 'ghana', 'education', 'trust', 'fund', 'getfund', 'completion', 'classroom', 'block', 'senior', 'high', 'school', 'shss']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9562\n",
            "    After Stopword Removal: ['opportunities', 'giving', '?']\n",
            "    After Regex: ['opportunities', 'giving']\n",
            "    After Lemmatization: ['opportunity', 'giving']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9563\n",
            "    After Stopword Removal: ['80/20', 'rule', 'useful', 'initial', 'assumption', ':', 'confronting', 'problem', ',', 'start', 'assuming', 'problem', 'created', 'individuals', ',', 'places', ',', 'events', '.']\n",
            "    After Regex: ['rule', 'useful', 'initial', 'assumption', 'confronting', 'problem', 'start', 'assuming', 'problem', 'created', 'individuals', 'places', 'events']\n",
            "    After Lemmatization: ['rule', 'useful', 'initial', 'assumption', 'confronting', 'problem', 'start', 'assuming', 'problem', 'created', 'individual', 'place', 'event']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9564\n",
            "    After Stopword Removal: ['great', 'anti', '-', 'war', 'statement', ',', 'benjamin', 'britten', \"'\", 'war', 'requiem', ',', 'written', '1962', '-', 'consecration', 'coventry', 'cathedral', ',', 'destroyed', 'german', 'bombs', '1940.']\n",
            "    After Regex: ['great', 'anti', 'war', 'statement', 'benjamin', 'britten', 'war', 'requiem', 'written', 'consecration', 'coventry', 'cathedral', 'destroyed', 'german', 'bombs']\n",
            "    After Lemmatization: ['great', 'anti', 'war', 'statement', 'benjamin', 'britten', 'war', 'requiem', 'written', 'consecration', 'coventry', 'cathedral', 'destroyed', 'german', 'bomb']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9565\n",
            "    After Stopword Removal: ['environmental', 'ministry', 'workers', 'collected', '300', 'dead', 'birds', 'lake', 'yurria', 'queretero', 'state', ',', 'daily', 'la', 'jornada', 'said', '.']\n",
            "    After Regex: ['environmental', 'ministry', 'workers', 'collected', 'dead', 'birds', 'lake', 'yurria', 'queretero', 'state', 'daily', 'la', 'jornada', 'said']\n",
            "    After Lemmatization: ['environmental', 'ministry', 'worker', 'collected', 'dead', 'bird', 'lake', 'yurria', 'queretero', 'state', 'daily', 'la', 'jornada', 'said']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9566\n",
            "    After Stopword Removal: ['specifically', ',', 'author', 'argues', 'services', 'aimed', 'primary', 'secondary', 'users', 'represent', 'revelatory', 'case', 'digital', 'reference', 'three', 'reasons', ';', 'digital', 'reference', 'services', 'education', ':', 'current', 'state', 'art', 'order', 'present', 'picture', 'digital', 'reference', 'education', ',', 'author', 'first', 'presents', 'two', 'major', 'types', 'services', 'education', 'domain', '.']\n",
            "    After Regex: ['specifically', 'author', 'argues', 'services', 'aimed', 'primary', 'secondary', 'users', 'represent', 'revelatory', 'case', 'digital', 'reference', 'three', 'reasons', 'digital', 'reference', 'services', 'education', 'current', 'state', 'art', 'order', 'present', 'picture', 'digital', 'reference', 'education', 'author', 'first', 'presents', 'two', 'major', 'types', 'services', 'education', 'domain']\n",
            "    After Lemmatization: ['specifically', 'author', 'argues', 'service', 'aimed', 'primary', 'secondary', 'user', 'represent', 'revelatory', 'case', 'digital', 'reference', 'three', 'reason', 'digital', 'reference', 'service', 'education', 'current', 'state', 'art', 'order', 'present', 'picture', 'digital', 'reference', 'education', 'author', 'first', 'present', 'two', 'major', 'type', 'service', 'education', 'domain']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9567\n",
            "    After Stopword Removal: ['makes', 'feasible', 'sanitation', 'professionals', 'eliminate', 'garbage', 'junk', 'multiple', 'residences', 'companies', 'region', 'affordable', 'prices', '.']\n",
            "    After Regex: ['makes', 'feasible', 'sanitation', 'professionals', 'eliminate', 'garbage', 'junk', 'multiple', 'residences', 'companies', 'region', 'affordable', 'prices']\n",
            "    After Lemmatization: ['make', 'feasible', 'sanitation', 'professional', 'eliminate', 'garbage', 'junk', 'multiple', 'residence', 'company', 'region', 'affordable', 'price']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9568\n",
            "    After Stopword Removal: ['china', '-', 'us', 'collaboration', 'infrastructure', 'projects', 'asia', 'could', 'reduce', 'extremism', ',', 'improve', 'relations', 'help', 'companies', '.']\n",
            "    After Regex: ['china', 'us', 'collaboration', 'infrastructure', 'projects', 'asia', 'could', 'reduce', 'extremism', 'improve', 'relations', 'help', 'companies']\n",
            "    After Lemmatization: ['china', 'u', 'collaboration', 'infrastructure', 'project', 'asia', 'could', 'reduce', 'extremism', 'improve', 'relation', 'help', 'company']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9569\n",
            "    After Stopword Removal: ['nowadays', ',', 'topics', 'doctoral', 'dissertations', 'field', 'often', 'multidisciplinary', 'relate', ',', 'example', ',', 'measurement', 'neurotransmitters', 'brain', 'tissue', ',', 'development', 'characterisation', 'new', 'piezoelectric', 'films', 'design', 'new', 'types', 'carbon', 'hybrid', 'materials', '.']\n",
            "    After Regex: ['nowadays', 'topics', 'doctoral', 'dissertations', 'field', 'often', 'multidisciplinary', 'relate', 'example', 'measurement', 'neurotransmitters', 'brain', 'tissue', 'development', 'characterisation', 'new', 'piezoelectric', 'films', 'design', 'new', 'types', 'carbon', 'hybrid', 'materials']\n",
            "    After Lemmatization: ['nowadays', 'topic', 'doctoral', 'dissertation', 'field', 'often', 'multidisciplinary', 'relate', 'example', 'measurement', 'neurotransmitter', 'brain', 'tissue', 'development', 'characterisation', 'new', 'piezoelectric', 'film', 'design', 'new', 'type', 'carbon', 'hybrid', 'material']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9570\n",
            "    After Stopword Removal: ['efficient', 'apart', 'providing', 'information', 'aggregate', 'current', 'draw', 'across', 'entire', 'pdu', ',', 'also', 'used', 'evaluate', 'power', 'draw', 'individual', 'devices', 'connected', '.']\n",
            "    After Regex: ['efficient', 'apart', 'providing', 'information', 'aggregate', 'current', 'draw', 'across', 'entire', 'pdu', 'also', 'used', 'evaluate', 'power', 'draw', 'individual', 'devices', 'connected']\n",
            "    After Lemmatization: ['efficient', 'apart', 'providing', 'information', 'aggregate', 'current', 'draw', 'across', 'entire', 'pdu', 'also', 'used', 'evaluate', 'power', 'draw', 'individual', 'device', 'connected']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9571\n",
            "    After Stopword Removal: ['key', 'making', 'sure', 'parents', 'keep', 'prescription', 'drugs', 'hands', 'children', ',', 'abusing', 'illegal', 'drug', 'except', 'marijuana', '.']\n",
            "    After Regex: ['key', 'making', 'sure', 'parents', 'keep', 'prescription', 'drugs', 'hands', 'children', 'abusing', 'illegal', 'drug', 'except', 'marijuana']\n",
            "    After Lemmatization: ['key', 'making', 'sure', 'parent', 'keep', 'prescription', 'drug', 'hand', 'child', 'abusing', 'illegal', 'drug', 'except', 'marijuana']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9572\n",
            "    After Stopword Removal: ['distributed', 'sensor', 'system', '(~100', 'sensor', 'nodes', ')', 'measures', 'temperature', ',', 'humidity', ',', '3', 'airflow', ',', 'transmits', 'information', 'wireless', 'zigbee', 'protocol', '.']\n",
            "    After Regex: ['distributed', 'sensor', 'system', 'sensor', 'nodes', 'measures', 'temperature', 'humidity', 'airflow', 'transmits', 'information', 'wireless', 'zigbee', 'protocol']\n",
            "    After Lemmatization: ['distributed', 'sensor', 'system', 'sensor', 'node', 'measure', 'temperature', 'humidity', 'airflow', 'transmits', 'information', 'wireless', 'zigbee', 'protocol']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9573\n",
            "    After Stopword Removal: ['open', 'day', 'day', ',', 'word', 'god', 'penetrates', 'inner', ',', 'revealing', 'areas', 'life', \"('\", 'thoughts', 'attitudes', 'heart', \"')\", 'need', 'sort', '.']\n",
            "    After Regex: ['open', 'day', 'day', 'word', 'god', 'penetrates', 'inner', 'revealing', 'areas', 'life', 'thoughts', 'attitudes', 'heart', 'need', 'sort']\n",
            "    After Lemmatization: ['open', 'day', 'day', 'word', 'god', 'penetrates', 'inner', 'revealing', 'area', 'life', 'thought', 'attitude', 'heart', 'need', 'sort']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9574\n",
            "    After Stopword Removal: ['average', 'household', 'size', '2.42', 'people', '.']\n",
            "    After Regex: ['average', 'household', 'size', 'people']\n",
            "    After Lemmatization: ['average', 'household', 'size', 'people']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9575\n",
            "    After Stopword Removal: ['comparing', 'cable', 'vs', 'satellite', 'internet', 'even', 'worse', ',', 'since', 'cable', 'connections', 'need', 'strong', 'connection', 'nearby', 'service', 'center', 'order', 'give', 'good', 'internet', 'signal', 'jackson', ',', 'california', 'keep', 'speeds', 'agonizingly', 'slow', '.']\n",
            "    After Regex: ['comparing', 'cable', 'vs', 'satellite', 'internet', 'even', 'worse', 'since', 'cable', 'connections', 'need', 'strong', 'connection', 'nearby', 'service', 'center', 'order', 'give', 'good', 'internet', 'signal', 'jackson', 'california', 'keep', 'speeds', 'agonizingly', 'slow']\n",
            "    After Lemmatization: ['comparing', 'cable', 'v', 'satellite', 'internet', 'even', 'worse', 'since', 'cable', 'connection', 'need', 'strong', 'connection', 'nearby', 'service', 'center', 'order', 'give', 'good', 'internet', 'signal', 'jackson', 'california', 'keep', 'speed', 'agonizingly', 'slow']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9576\n",
            "    After Stopword Removal: ['kids', \"'\", 'table', ',', 'kids', 'practice', 'basic', 'culinary', 'skills', ',', 'try', 'new', 'foods', ',', 'explore', 'flavors', 'make', 'new', 'friends', 'apple', 'seeds', 'teaching', 'farm', '.']\n",
            "    After Regex: ['kids', 'table', 'kids', 'practice', 'basic', 'culinary', 'skills', 'try', 'new', 'foods', 'explore', 'flavors', 'make', 'new', 'friends', 'apple', 'seeds', 'teaching', 'farm']\n",
            "    After Lemmatization: ['kid', 'table', 'kid', 'practice', 'basic', 'culinary', 'skill', 'try', 'new', 'food', 'explore', 'flavor', 'make', 'new', 'friend', 'apple', 'seed', 'teaching', 'farm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9577\n",
            "    After Stopword Removal: ['students', 'study', 'analyze', 'work', 'experts', ',', 'given', 'opportunity', 'gain', 'knowledge', 'subject', 'area', ',', 'also', 'provided', 'chance', 'voice', 'opinions', ',', 'formed', 'study', 'expert', \"'\", 'work', '.']\n",
            "    After Regex: ['students', 'study', 'analyze', 'work', 'experts', 'given', 'opportunity', 'gain', 'knowledge', 'subject', 'area', 'also', 'provided', 'chance', 'voice', 'opinions', 'formed', 'study', 'expert', 'work']\n",
            "    After Lemmatization: ['student', 'study', 'analyze', 'work', 'expert', 'given', 'opportunity', 'gain', 'knowledge', 'subject', 'area', 'also', 'provided', 'chance', 'voice', 'opinion', 'formed', 'study', 'expert', 'work']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9578\n",
            "    After Stopword Removal: ['coalition', ',', 'founded', '2006', 'texas', 'medical', 'association', ',', 'made', '30', 'organizations', 'dedicated', 'advancing', 'disease', 'prevention', 'health', 'education', '.']\n",
            "    After Regex: ['coalition', 'founded', 'texas', 'medical', 'association', 'made', 'organizations', 'dedicated', 'advancing', 'disease', 'prevention', 'health', 'education']\n",
            "    After Lemmatization: ['coalition', 'founded', 'texas', 'medical', 'association', 'made', 'organization', 'dedicated', 'advancing', 'disease', 'prevention', 'health', 'education']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9579\n",
            "    After Stopword Removal: ['along', 'new', 'construction', 'come', 'new', 'schools', ',', 'supplementing', 'already', 'established', '.']\n",
            "    After Regex: ['along', 'new', 'construction', 'come', 'new', 'schools', 'supplementing', 'already', 'established']\n",
            "    After Lemmatization: ['along', 'new', 'construction', 'come', 'new', 'school', 'supplementing', 'already', 'established']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9580\n",
            "    After Stopword Removal: ['senate', 'medicaid', 'task', 'force', 'targets', 'waste', ',', 'fraud', 'abuse', 'new', 'york', 'spends', 'total', '$52.5', 'billion', 'taxpayers', \"'\", 'money', 'year', 'medicaid', '.']\n",
            "    After Regex: ['senate', 'medicaid', 'task', 'force', 'targets', 'waste', 'fraud', 'abuse', 'new', 'york', 'spends', 'total', 'billion', 'taxpayers', 'money', 'year', 'medicaid']\n",
            "    After Lemmatization: ['senate', 'medicaid', 'task', 'force', 'target', 'waste', 'fraud', 'abuse', 'new', 'york', 'spends', 'total', 'billion', 'taxpayer', 'money', 'year', 'medicaid']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9581\n",
            "    After Stopword Removal: ['last', 'week', 'cancun', ',', 'aquarium', 'broke', 'record', 'dolphins', 'born', 'captivity', '.']\n",
            "    After Regex: ['last', 'week', 'cancun', 'aquarium', 'broke', 'record', 'dolphins', 'born', 'captivity']\n",
            "    After Lemmatization: ['last', 'week', 'cancun', 'aquarium', 'broke', 'record', 'dolphin', 'born', 'captivity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9582\n",
            "    After Stopword Removal: ['department', 'defense', 'uses', 'petroleum', '(', 'energy', ')', 'organization', 'planet', '—$13', 'billion', '$18', 'billion', 'worth', 'year', ',', 'depending', 'math', '.']\n",
            "    After Regex: ['department', 'defense', 'uses', 'petroleum', 'energy', 'organization', 'planet', 'billion', 'billion', 'worth', 'year', 'depending', 'math']\n",
            "    After Lemmatization: ['department', 'defense', 'us', 'petroleum', 'energy', 'organization', 'planet', 'billion', 'billion', 'worth', 'year', 'depending', 'math']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9583\n",
            "    After Stopword Removal: ['sand', 'often', 'used', 'manufacturing', ',', 'example', 'abrasive', '.']\n",
            "    After Regex: ['sand', 'often', 'used', 'manufacturing', 'example', 'abrasive']\n",
            "    After Lemmatization: ['sand', 'often', 'used', 'manufacturing', 'example', 'abrasive']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9584\n",
            "    After Stopword Removal: ['magellanic', 'penguin', 'named', ',', 'first', 'european', 'note', '.']\n",
            "    After Regex: ['magellanic', 'penguin', 'named', 'first', 'european', 'note']\n",
            "    After Lemmatization: ['magellanic', 'penguin', 'named', 'first', 'european', 'note']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9585\n",
            "    After Stopword Removal: ['traces', 'sage', '(', 'pictorial', 'biography', 'confucius', '),', 'ming', 'dynasty', 'album', ',', 'illustrates', 'events', 'confuciuss', 'life', ';', 'attributes', 'supernatural', 'qualities', ',', 'including', 'ability', 'communicate', 'cosmic', 'forces', '.']\n",
            "    After Regex: ['traces', 'sage', 'pictorial', 'biography', 'confucius', 'ming', 'dynasty', 'album', 'illustrates', 'events', 'confuciuss', 'life', 'attributes', 'supernatural', 'qualities', 'including', 'ability', 'communicate', 'cosmic', 'forces']\n",
            "    After Lemmatization: ['trace', 'sage', 'pictorial', 'biography', 'confucius', 'ming', 'dynasty', 'album', 'illustrates', 'event', 'confucius', 'life', 'attribute', 'supernatural', 'quality', 'including', 'ability', 'communicate', 'cosmic', 'force']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9586\n",
            "    After Stopword Removal: ['weakness', 'human', 'trait', ',', 'gender', 'trait', '.']\n",
            "    After Regex: ['weakness', 'human', 'trait', 'gender', 'trait']\n",
            "    After Lemmatization: ['weakness', 'human', 'trait', 'gender', 'trait']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9587\n",
            "    After Stopword Removal: ['deflect', 'discussion', 'away', 'core', 'aspects', 'scientology', \"'\", 'cosmological', '/', 'theological', 'beliefs', '.']\n",
            "    After Regex: ['deflect', 'discussion', 'away', 'core', 'aspects', 'scientology', 'cosmological', 'theological', 'beliefs']\n",
            "    After Lemmatization: ['deflect', 'discussion', 'away', 'core', 'aspect', 'scientology', 'cosmological', 'theological', 'belief']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9588\n",
            "    After Stopword Removal: ['also', 'overlap', 'lead', 'one', 'another', '.']\n",
            "    After Regex: ['also', 'overlap', 'lead', 'one', 'another']\n",
            "    After Lemmatization: ['also', 'overlap', 'lead', 'one', 'another']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9589\n",
            "    After Stopword Removal: ['farm', 'first', 'subdivided', '1950', 'size', 'lot', 'reduced', '.']\n",
            "    After Regex: ['farm', 'first', 'subdivided', 'size', 'lot', 'reduced']\n",
            "    After Lemmatization: ['farm', 'first', 'subdivided', 'size', 'lot', 'reduced']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9590\n",
            "    After Stopword Removal: ['goal', 'federal', 'consistency', 'process', 'maintain', 'balance', 'state', 'coastal', 'zone', 'management', 'programs', 'federal', 'activities', '.']\n",
            "    After Regex: ['goal', 'federal', 'consistency', 'process', 'maintain', 'balance', 'state', 'coastal', 'zone', 'management', 'programs', 'federal', 'activities']\n",
            "    After Lemmatization: ['goal', 'federal', 'consistency', 'process', 'maintain', 'balance', 'state', 'coastal', 'zone', 'management', 'program', 'federal', 'activity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9591\n",
            "    After Stopword Removal: ['onguard', 'online', 'great', 'resource', 'lot', 'ideas', 'parents', ',', 'students', ',', 'educators', 'wide', 'range', 'topics', 'online', 'safety', 'way', 'tips', 'protect', 'personal', 'information', ',', 'securing', 'computer', 'viruses', '.']\n",
            "    After Regex: ['onguard', 'online', 'great', 'resource', 'lot', 'ideas', 'parents', 'students', 'educators', 'wide', 'range', 'topics', 'online', 'safety', 'way', 'tips', 'protect', 'personal', 'information', 'securing', 'computer', 'viruses']\n",
            "    After Lemmatization: ['onguard', 'online', 'great', 'resource', 'lot', 'idea', 'parent', 'student', 'educator', 'wide', 'range', 'topic', 'online', 'safety', 'way', 'tip', 'protect', 'personal', 'information', 'securing', 'computer', 'virus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9592\n",
            "    After Stopword Removal: ['fall', 'two', 'major', 'categories', ':', 'behavioral', 'tests', 'electrophysiologic', 'tests', '.']\n",
            "    After Regex: ['fall', 'two', 'major', 'categories', 'behavioral', 'tests', 'electrophysiologic', 'tests']\n",
            "    After Lemmatization: ['fall', 'two', 'major', 'category', 'behavioral', 'test', 'electrophysiologic', 'test']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9593\n",
            "    After Stopword Removal: ['100', 'known', ',', '88', 'occur', 'naturally', '.']\n",
            "    After Regex: ['known', 'occur', 'naturally']\n",
            "    After Lemmatization: ['known', 'occur', 'naturally']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9594\n",
            "    After Stopword Removal: ['store', 'upright', 'lay', 'back', 'angled', 'section', 'brush', 'head', 'air', 'dry', 'reduce', 'bacterial', 'growth', '.']\n",
            "    After Regex: ['store', 'upright', 'lay', 'back', 'angled', 'section', 'brush', 'head', 'air', 'dry', 'reduce', 'bacterial', 'growth']\n",
            "    After Lemmatization: ['store', 'upright', 'lay', 'back', 'angled', 'section', 'brush', 'head', 'air', 'dry', 'reduce', 'bacterial', 'growth']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9595\n",
            "    After Stopword Removal: ['kindergarten', 'reading', 'heather', 'taylor', 'think', 'good', 'project', 'children', 'able', 'share', 'experiences', 'reading', 'talking', 'books', 'home', '.']\n",
            "    After Regex: ['kindergarten', 'reading', 'heather', 'taylor', 'think', 'good', 'project', 'children', 'able', 'share', 'experiences', 'reading', 'talking', 'books', 'home']\n",
            "    After Lemmatization: ['kindergarten', 'reading', 'heather', 'taylor', 'think', 'good', 'project', 'child', 'able', 'share', 'experience', 'reading', 'talking', 'book', 'home']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9596\n",
            "    After Stopword Removal: ['cellulite', 'cause', 'fat', 'cells', 'instead', 'laying', 'smooth', 'skin', ',', 'lay', 'top', 'one', 'another', 'form', 'dimple', '.']\n",
            "    After Regex: ['cellulite', 'cause', 'fat', 'cells', 'instead', 'laying', 'smooth', 'skin', 'lay', 'top', 'one', 'another', 'form', 'dimple']\n",
            "    After Lemmatization: ['cellulite', 'cause', 'fat', 'cell', 'instead', 'laying', 'smooth', 'skin', 'lay', 'top', 'one', 'another', 'form', 'dimple']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9597\n",
            "    After Stopword Removal: ['frequently', 'found', 'mexico', 'northern', 'parts', 'south', 'america', '.']\n",
            "    After Regex: ['frequently', 'found', 'mexico', 'northern', 'parts', 'south', 'america']\n",
            "    After Lemmatization: ['frequently', 'found', 'mexico', 'northern', 'part', 'south', 'america']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9598\n",
            "    After Stopword Removal: ['interfaces', 'communication', 'protocols', 'supported', '?']\n",
            "    After Regex: ['interfaces', 'communication', 'protocols', 'supported']\n",
            "    After Lemmatization: ['interface', 'communication', 'protocol', 'supported']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9599\n",
            "    After Stopword Removal: ['tree', 'stands', 'used', 'bow', 'rifle', 'hunting', '.']\n",
            "    After Regex: ['tree', 'stands', 'used', 'bow', 'rifle', 'hunting']\n",
            "    After Lemmatization: ['tree', 'stand', 'used', 'bow', 'rifle', 'hunting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9600\n",
            "    After Stopword Removal: ['mcginn', 'urged', 'test', 'takers', 'begin', 'homework', 'using', 'authentic', 'materials', 'available', 'internet', 'sources', ',', 'encouraged', 'supervisors', 'reinforce', 'need', 'prepare', '.']\n",
            "    After Regex: ['mcginn', 'urged', 'test', 'takers', 'begin', 'homework', 'using', 'authentic', 'materials', 'available', 'internet', 'sources', 'encouraged', 'supervisors', 'reinforce', 'need', 'prepare']\n",
            "    After Lemmatization: ['mcginn', 'urged', 'test', 'taker', 'begin', 'homework', 'using', 'authentic', 'material', 'available', 'internet', 'source', 'encouraged', 'supervisor', 'reinforce', 'need', 'prepare']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9601\n",
            "    After Stopword Removal: ['design', 'equations', 'classic', '2-', 'way', ',', 'n', '-', 'way', 'star', 'configuration', ',', 'unequal', 'split', 'wilkinson', 'consistent', 'equations', 'presented', '.']\n",
            "    After Regex: ['design', 'equations', 'classic', 'way', 'n', 'way', 'star', 'configuration', 'unequal', 'split', 'wilkinson', 'consistent', 'equations', 'presented']\n",
            "    After Lemmatization: ['design', 'equation', 'classic', 'way', 'n', 'way', 'star', 'configuration', 'unequal', 'split', 'wilkinson', 'consistent', 'equation', 'presented']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9602\n",
            "    After Stopword Removal: ['year', ',', 'parliamentary', 'authorities', 'resisted', 'efforts', 'campaigners', 'force', 'full', 'disclosure', 'claims', '.']\n",
            "    After Regex: ['year', 'parliamentary', 'authorities', 'resisted', 'efforts', 'campaigners', 'force', 'full', 'disclosure', 'claims']\n",
            "    After Lemmatization: ['year', 'parliamentary', 'authority', 'resisted', 'effort', 'campaigner', 'force', 'full', 'disclosure', 'claim']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9603\n",
            "    After Stopword Removal: ['white', 'drinking', 'men', 'drove', 'would', '31,285', 'fewer', 'traffic', 'fatalities', 'white', 'non', '-', 'drinking', 'women', 'drove', 'would', '15,904', '.']\n",
            "    After Regex: ['white', 'drinking', 'men', 'drove', 'would', 'fewer', 'traffic', 'fatalities', 'white', 'non', 'drinking', 'women', 'drove', 'would']\n",
            "    After Lemmatization: ['white', 'drinking', 'men', 'drove', 'would', 'fewer', 'traffic', 'fatality', 'white', 'non', 'drinking', 'woman', 'drove', 'would']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9604\n",
            "    After Stopword Removal: ['guilt', 'past', ',', 'fear', 'future', 'guilt', '.']\n",
            "    After Regex: ['guilt', 'past', 'fear', 'future', 'guilt']\n",
            "    After Lemmatization: ['guilt', 'past', 'fear', 'future', 'guilt']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9605\n",
            "    After Stopword Removal: ['commission', 'thinks', 'irregularities', 'occurred', 'member', 'state', ',', 'launch', 'administrative', 'inquiry', 'member', 'state', '.']\n",
            "    After Regex: ['commission', 'thinks', 'irregularities', 'occurred', 'member', 'state', 'launch', 'administrative', 'inquiry', 'member', 'state']\n",
            "    After Lemmatization: ['commission', 'think', 'irregularity', 'occurred', 'member', 'state', 'launch', 'administrative', 'inquiry', 'member', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9606\n",
            "    After Stopword Removal: ['september', '1919', 'dr', '.', 'cleven', 'moved', 'fayetteville', 'accepting', 'assistant', 'professorship', 'dr', '.', 'david', 'yancey', 'thomas', ',', 'head', 'department', 'history', 'political', 'science', 'university', 'arkansas', '.']\n",
            "    After Regex: ['september', 'dr', 'cleven', 'moved', 'fayetteville', 'accepting', 'assistant', 'professorship', 'dr', 'david', 'yancey', 'thomas', 'head', 'department', 'history', 'political', 'science', 'university', 'arkansas']\n",
            "    After Lemmatization: ['september', 'dr', 'cleven', 'moved', 'fayetteville', 'accepting', 'assistant', 'professorship', 'dr', 'david', 'yancey', 'thomas', 'head', 'department', 'history', 'political', 'science', 'university', 'arkansas']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9607\n",
            "    After Stopword Removal: ['addition', ',', '-', 'day', 'exhibit', 'invasive', 'species', '.']\n",
            "    After Regex: ['addition', 'day', 'exhibit', 'invasive', 'species']\n",
            "    After Lemmatization: ['addition', 'day', 'exhibit', 'invasive', 'specie']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9608\n",
            "    After Stopword Removal: ['account', ',', 'speakers', 'bear', 'relationship', 'linguistic', 'authority', 'quantified', 'statements', ',', 'use', 'statements', 'express', 'certain', 'kinds', 'thoughts', '.']\n",
            "    After Regex: ['account', 'speakers', 'bear', 'relationship', 'linguistic', 'authority', 'quantified', 'statements', 'use', 'statements', 'express', 'certain', 'kinds', 'thoughts']\n",
            "    After Lemmatization: ['account', 'speaker', 'bear', 'relationship', 'linguistic', 'authority', 'quantified', 'statement', 'use', 'statement', 'express', 'certain', 'kind', 'thought']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9609\n",
            "    After Stopword Removal: ['u', '.', '.', 'spent', 'trillions', 'dollars', ',', 'spilled', 'blood', 'millions', 'american', 'soldiers', 'innocent', 'civilians', ',', 'fighting', 'al', '-', 'qaeda', 'iraq', 'afghanistan', '.']\n",
            "    After Regex: ['u', 'spent', 'trillions', 'dollars', 'spilled', 'blood', 'millions', 'american', 'soldiers', 'innocent', 'civilians', 'fighting', 'al', 'qaeda', 'iraq', 'afghanistan']\n",
            "    After Lemmatization: ['u', 'spent', 'trillion', 'dollar', 'spilled', 'blood', 'million', 'american', 'soldier', 'innocent', 'civilian', 'fighting', 'al', 'qaeda', 'iraq', 'afghanistan']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9610\n",
            "    After Stopword Removal: ['traditionally', ',', 'summer', 'dry', 'period', 'donations', ',', 'children', 'school', 'families', 'vacation', '.']\n",
            "    After Regex: ['traditionally', 'summer', 'dry', 'period', 'donations', 'children', 'school', 'families', 'vacation']\n",
            "    After Lemmatization: ['traditionally', 'summer', 'dry', 'period', 'donation', 'child', 'school', 'family', 'vacation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9611\n",
            "    After Stopword Removal: ['many', 'discussions', '(', 'plenary', ',', 'informal', ')', 'recent', 'rounds', 'fla', 'meetings', 'touched', 'kinds', 'tools', 'systems', 'might', 'valuable', 'helping', 'stem', 'flow', 'wood', 'products', 'illegal', 'origin', ',', 'moving', 'forestry', 'sector', 'producer', 'countries', 'better', 'standard', 'governance', ',', 'monitoring', ',', 'enforcement', '.']\n",
            "    After Regex: ['many', 'discussions', 'plenary', 'informal', 'recent', 'rounds', 'fla', 'meetings', 'touched', 'kinds', 'tools', 'systems', 'might', 'valuable', 'helping', 'stem', 'flow', 'wood', 'products', 'illegal', 'origin', 'moving', 'forestry', 'sector', 'producer', 'countries', 'better', 'standard', 'governance', 'monitoring', 'enforcement']\n",
            "    After Lemmatization: ['many', 'discussion', 'plenary', 'informal', 'recent', 'round', 'fla', 'meeting', 'touched', 'kind', 'tool', 'system', 'might', 'valuable', 'helping', 'stem', 'flow', 'wood', 'product', 'illegal', 'origin', 'moving', 'forestry', 'sector', 'producer', 'country', 'better', 'standard', 'governance', 'monitoring', 'enforcement']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9612\n",
            "    After Stopword Removal: ['edwards', 'picked', 'cotton', 'pulled', 'corn', 'mississippi', 'delta', 'plantations', 'age', '9,', 'living', 'hard', 'life', 'blues', 'created', 'address', '.']\n",
            "    After Regex: ['edwards', 'picked', 'cotton', 'pulled', 'corn', 'mississippi', 'delta', 'plantations', 'age', 'living', 'hard', 'life', 'blues', 'created', 'address']\n",
            "    After Lemmatization: ['edward', 'picked', 'cotton', 'pulled', 'corn', 'mississippi', 'delta', 'plantation', 'age', 'living', 'hard', 'life', 'blue', 'created', 'address']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9613\n",
            "    After Stopword Removal: ['activation', 'continues', ',', 'conductance', 'membrane', 'sodium', 'less', 'greater', 'membrane', 'conductance', 'potassium', '?']\n",
            "    After Regex: ['activation', 'continues', 'conductance', 'membrane', 'sodium', 'less', 'greater', 'membrane', 'conductance', 'potassium']\n",
            "    After Lemmatization: ['activation', 'continues', 'conductance', 'membrane', 'sodium', 'less', 'greater', 'membrane', 'conductance', 'potassium']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9614\n",
            "    After Stopword Removal: ['asbestos', 'also', 'insulates', 'well', \"'\", 'break', 'easily', '.']\n",
            "    After Regex: ['asbestos', 'also', 'insulates', 'well', 'break', 'easily']\n",
            "    After Lemmatization: ['asbestos', 'also', 'insulates', 'well', 'break', 'easily']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9615\n",
            "    After Stopword Removal: ['appears', 'though', 'american', 'definitions', 'liberty', 'democracy', 'contrary', 'rest', 'world', '.']\n",
            "    After Regex: ['appears', 'though', 'american', 'definitions', 'liberty', 'democracy', 'contrary', 'rest', 'world']\n",
            "    After Lemmatization: ['appears', 'though', 'american', 'definition', 'liberty', 'democracy', 'contrary', 'rest', 'world']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9616\n",
            "    After Stopword Removal: ['showing', 'support', 'belief', 'mission', 'child', \"'\", 'school', 'important', '.']\n",
            "    After Regex: ['showing', 'support', 'belief', 'mission', 'child', 'school', 'important']\n",
            "    After Lemmatization: ['showing', 'support', 'belief', 'mission', 'child', 'school', 'important']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9617\n",
            "    After Stopword Removal: ['married', 'mary', 'elizabeth', 'mathis', 'dec', '.18,1938.']\n",
            "    After Regex: ['married', 'mary', 'elizabeth', 'mathis', 'dec']\n",
            "    After Lemmatization: ['married', 'mary', 'elizabeth', 'mathis', 'dec']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9618\n",
            "    After Stopword Removal: ['accountability', 'important', 'stakeholders', '.']\n",
            "    After Regex: ['accountability', 'important', 'stakeholders']\n",
            "    After Lemmatization: ['accountability', 'important', 'stakeholder']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9619\n",
            "    After Stopword Removal: ['leaky', 'faucets', ',', 'hose', 'bibs', 'water', '-', 'using', 'appliances', 'waste', 'surprising', 'amount', 'water', '.']\n",
            "    After Regex: ['leaky', 'faucets', 'hose', 'bibs', 'water', 'using', 'appliances', 'waste', 'surprising', 'amount', 'water']\n",
            "    After Lemmatization: ['leaky', 'faucet', 'hose', 'bib', 'water', 'using', 'appliance', 'waste', 'surprising', 'amount', 'water']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9620\n",
            "    After Stopword Removal: ['search', 'gas', 'revived', '1886', 'discovery', 'trenton', 'gas', 'ohio', 'indiana', '.']\n",
            "    After Regex: ['search', 'gas', 'revived', 'discovery', 'trenton', 'gas', 'ohio', 'indiana']\n",
            "    After Lemmatization: ['search', 'gas', 'revived', 'discovery', 'trenton', 'gas', 'ohio', 'indiana']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9621\n",
            "    After Stopword Removal: ['advocates', 'estate', 'tax', 'see', 'crucial', 'tool', 'raising', 'revenue', 'buffer', 'sharp', ',', 'nearly', 'inexorable', 'rise', 'inequality', 'past', 'four', 'decades', '.']\n",
            "    After Regex: ['advocates', 'estate', 'tax', 'see', 'crucial', 'tool', 'raising', 'revenue', 'buffer', 'sharp', 'nearly', 'inexorable', 'rise', 'inequality', 'past', 'four', 'decades']\n",
            "    After Lemmatization: ['advocate', 'estate', 'tax', 'see', 'crucial', 'tool', 'raising', 'revenue', 'buffer', 'sharp', 'nearly', 'inexorable', 'rise', 'inequality', 'past', 'four', 'decade']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9622\n",
            "    After Stopword Removal: ['terms', 'using', 'modern', 'biomedical', 'methods', 'investigating', 'clinical', 'effects', 'acupuncture', ',', 'first', 'rcts', 'acupuncture', 'conducted', 'japan', '1960', 'kinoshita', 'okabe', '(', 'shichido', '1996),', 'earliest', 'attempts', \"'\", 'inter', '-', 'rater', \"'\", 'reliability', 'studies', 'also', 'japanese', '(', 'debata', '1968,', 'matsumoto', '1968,', 'ogawa', '1978).']\n",
            "    After Regex: ['terms', 'using', 'modern', 'biomedical', 'methods', 'investigating', 'clinical', 'effects', 'acupuncture', 'first', 'rcts', 'acupuncture', 'conducted', 'japan', 'kinoshita', 'okabe', 'shichido', 'earliest', 'attempts', 'inter', 'rater', 'reliability', 'studies', 'also', 'japanese', 'debata', 'matsumoto', 'ogawa']\n",
            "    After Lemmatization: ['term', 'using', 'modern', 'biomedical', 'method', 'investigating', 'clinical', 'effect', 'acupuncture', 'first', 'rcts', 'acupuncture', 'conducted', 'japan', 'kinoshita', 'okabe', 'shichido', 'earliest', 'attempt', 'inter', 'rater', 'reliability', 'study', 'also', 'japanese', 'debata', 'matsumoto', 'ogawa']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9623\n",
            "    After Stopword Removal: ['timely', 'recommendation', 'coincides', 'new', 'federal', 'rules', 'require', 'insurance', 'companies', 'fully', 'cover', 'annual', 'physician', 'checkups', 'women', '.']\n",
            "    After Regex: ['timely', 'recommendation', 'coincides', 'new', 'federal', 'rules', 'require', 'insurance', 'companies', 'fully', 'cover', 'annual', 'physician', 'checkups', 'women']\n",
            "    After Lemmatization: ['timely', 'recommendation', 'coincides', 'new', 'federal', 'rule', 'require', 'insurance', 'company', 'fully', 'cover', 'annual', 'physician', 'checkup', 'woman']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9624\n",
            "    After Stopword Removal: ['looks', 'characteristics', 'agile', 'organisations', 'might', 'translate', 'public', 'sector', 'environment', '.']\n",
            "    After Regex: ['looks', 'characteristics', 'agile', 'organisations', 'might', 'translate', 'public', 'sector', 'environment']\n",
            "    After Lemmatization: ['look', 'characteristic', 'agile', 'organisation', 'might', 'translate', 'public', 'sector', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9625\n",
            "    After Stopword Removal: ['dens', 'relatively', 'permanent', 'colony', ',', 'used', '20', 'years', '.']\n",
            "    After Regex: ['dens', 'relatively', 'permanent', 'colony', 'used', 'years']\n",
            "    After Lemmatization: ['den', 'relatively', 'permanent', 'colony', 'used', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9626\n",
            "    After Stopword Removal: ['example', ',', 'taxation', ',', 'one', 'obliged', 'transfer', 'one', \"'\", 'wealth', 'government', 'sake', 'common', 'good', '.']\n",
            "    After Regex: ['example', 'taxation', 'one', 'obliged', 'transfer', 'one', 'wealth', 'government', 'sake', 'common', 'good']\n",
            "    After Lemmatization: ['example', 'taxation', 'one', 'obliged', 'transfer', 'one', 'wealth', 'government', 'sake', 'common', 'good']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9627\n",
            "    After Stopword Removal: ['back', '2011,', 'friends', 'peta', 'u', '.', 'k', '.', 'urged', 'agency', 'head', 'dr', '.', 'hamid', 'fazeli', 'ground', 'misguided', 'mission', ',', 'pointing', 'nonhuman', 'primates', 'longer', 'sent', 'space', 'american', 'european', 'appears', 'iran', 'repeating', 'wasteful', 'cruel', 'mistakes', 'marked', 'darkest', 'days', 'space', 'race', '.']\n",
            "    After Regex: ['back', 'friends', 'peta', 'u', 'k', 'urged', 'agency', 'head', 'dr', 'hamid', 'fazeli', 'ground', 'misguided', 'mission', 'pointing', 'nonhuman', 'primates', 'longer', 'sent', 'space', 'american', 'european', 'appears', 'iran', 'repeating', 'wasteful', 'cruel', 'mistakes', 'marked', 'darkest', 'days', 'space', 'race']\n",
            "    After Lemmatization: ['back', 'friend', 'peta', 'u', 'k', 'urged', 'agency', 'head', 'dr', 'hamid', 'fazeli', 'ground', 'misguided', 'mission', 'pointing', 'nonhuman', 'primate', 'longer', 'sent', 'space', 'american', 'european', 'appears', 'iran', 'repeating', 'wasteful', 'cruel', 'mistake', 'marked', 'darkest', 'day', 'space', 'race']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9628\n",
            "    After Stopword Removal: ['peel', ',', 'core', 'chop', 'apple', 'add', 'pieces', 'blender', '.']\n",
            "    After Regex: ['peel', 'core', 'chop', 'apple', 'add', 'pieces', 'blender']\n",
            "    After Lemmatization: ['peel', 'core', 'chop', 'apple', 'add', 'piece', 'blender']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9629\n",
            "    After Stopword Removal: ['epidural', ',', 'anyway', '?']\n",
            "    After Regex: ['epidural', 'anyway']\n",
            "    After Lemmatization: ['epidural', 'anyway']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9630\n",
            "    After Stopword Removal: ['relationship', 'impose', 'obligations', 'parties', '.']\n",
            "    After Regex: ['relationship', 'impose', 'obligations', 'parties']\n",
            "    After Lemmatization: ['relationship', 'impose', 'obligation', 'party']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9631\n",
            "    After Stopword Removal: ['installing', 'solar', 'panels', 'perth', 'help', 'household', 'budget', 'also', 'help', 'planet', 'well', '.']\n",
            "    After Regex: ['installing', 'solar', 'panels', 'perth', 'help', 'household', 'budget', 'also', 'help', 'planet', 'well']\n",
            "    After Lemmatization: ['installing', 'solar', 'panel', 'perth', 'help', 'household', 'budget', 'also', 'help', 'planet', 'well']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9632\n",
            "    After Stopword Removal: ['lester', 'r', '.', 'brown', 'claimed', ',', 'since', 'converting', 'entire', 'grain', 'harvest', 'us', 'would', 'produce', '16%', 'auto', 'fuel', 'needs', ',', 'energy', 'markets', 'effectively', 'placed', 'competition', 'food', 'markets', 'scarce', 'arable', 'land', ',', 'resulting', 'higher', 'food', 'prices', '.']\n",
            "    After Regex: ['lester', 'r', 'brown', 'claimed', 'since', 'converting', 'entire', 'grain', 'harvest', 'us', 'would', 'produce', 'auto', 'fuel', 'needs', 'energy', 'markets', 'effectively', 'placed', 'competition', 'food', 'markets', 'scarce', 'arable', 'land', 'resulting', 'higher', 'food', 'prices']\n",
            "    After Lemmatization: ['lester', 'r', 'brown', 'claimed', 'since', 'converting', 'entire', 'grain', 'harvest', 'u', 'would', 'produce', 'auto', 'fuel', 'need', 'energy', 'market', 'effectively', 'placed', 'competition', 'food', 'market', 'scarce', 'arable', 'land', 'resulting', 'higher', 'food', 'price']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9633\n",
            "    After Stopword Removal: ['distinction', 'deeper', 'typical', 'american', 'understanding', 'separation', 'church', 'state', '.']\n",
            "    After Regex: ['distinction', 'deeper', 'typical', 'american', 'understanding', 'separation', 'church', 'state']\n",
            "    After Lemmatization: ['distinction', 'deeper', 'typical', 'american', 'understanding', 'separation', 'church', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9634\n",
            "    After Stopword Removal: ['ladies', 'gentlemen', ',', 'corruption', 'obstacle', 'democracy', 'rule', 'law', '.']\n",
            "    After Regex: ['ladies', 'gentlemen', 'corruption', 'obstacle', 'democracy', 'rule', 'law']\n",
            "    After Lemmatization: ['lady', 'gentleman', 'corruption', 'obstacle', 'democracy', 'rule', 'law']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9635\n",
            "    After Stopword Removal: ['per', 'economic', 'social', 'commission', 'asia', 'pacific', 'report', '2016,', 'around', '12.4%', 'population', 'asia', 'pacific', '60', 'years', '.']\n",
            "    After Regex: ['per', 'economic', 'social', 'commission', 'asia', 'pacific', 'report', 'around', 'population', 'asia', 'pacific', 'years']\n",
            "    After Lemmatization: ['per', 'economic', 'social', 'commission', 'asia', 'pacific', 'report', 'around', 'population', 'asia', 'pacific', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9636\n",
            "    After Stopword Removal: ['onset', 'symptoms', 'may', 'occur', 'within', 'minutes', 'weeks', 'often', 'presents', 'flu', '-', 'like', 'symptoms', ',', 'ill', 'person', 'may', 'experience', 'symptoms', 'nausea', ',', 'vomiting', ',', 'diarrhea', ',', 'fever', '.']\n",
            "    After Regex: ['onset', 'symptoms', 'may', 'occur', 'within', 'minutes', 'weeks', 'often', 'presents', 'flu', 'like', 'symptoms', 'ill', 'person', 'may', 'experience', 'symptoms', 'nausea', 'vomiting', 'diarrhea', 'fever']\n",
            "    After Lemmatization: ['onset', 'symptom', 'may', 'occur', 'within', 'minute', 'week', 'often', 'present', 'flu', 'like', 'symptom', 'ill', 'person', 'may', 'experience', 'symptom', 'nausea', 'vomiting', 'diarrhea', 'fever']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9637\n",
            "    After Stopword Removal: ['propose', 'mediated', 'model', 'considering', 'individuals', 'engage', 'surrogate', 'behavior', '(', 'promoting', 'leader', ',', 'defending', 'leader', ',', 'modeling', 'followership', ')', 'increases', 'charismatic', 'percep', '-', 'tions', 'among', 'distant', 'followers', '.']\n",
            "    After Regex: ['propose', 'mediated', 'model', 'considering', 'individuals', 'engage', 'surrogate', 'behavior', 'promoting', 'leader', 'defending', 'leader', 'modeling', 'followership', 'increases', 'charismatic', 'percep', 'tions', 'among', 'distant', 'followers']\n",
            "    After Lemmatization: ['propose', 'mediated', 'model', 'considering', 'individual', 'engage', 'surrogate', 'behavior', 'promoting', 'leader', 'defending', 'leader', 'modeling', 'followership', 'increase', 'charismatic', 'percep', 'tions', 'among', 'distant', 'follower']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9638\n",
            "    After Stopword Removal: ['great', 'way', 'get', 'started', '!']\n",
            "    After Regex: ['great', 'way', 'get', 'started']\n",
            "    After Lemmatization: ['great', 'way', 'get', 'started']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9639\n",
            "    After Stopword Removal: ['wyoming', 'legislature', \"'\", 'select', 'committee', 'education', 'accountability', 'given', 'public', 'instruction', 'superintendent', 'cindy', 'hill', 'monday', 'present', 'written', 'arguments', '.']\n",
            "    After Regex: ['wyoming', 'legislature', 'select', 'committee', 'education', 'accountability', 'given', 'public', 'instruction', 'superintendent', 'cindy', 'hill', 'monday', 'present', 'written', 'arguments']\n",
            "    After Lemmatization: ['wyoming', 'legislature', 'select', 'committee', 'education', 'accountability', 'given', 'public', 'instruction', 'superintendent', 'cindy', 'hill', 'monday', 'present', 'written', 'argument']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9640\n",
            "    After Stopword Removal: ['also', 'significant', 'regional', 'disparities', 'urban', 'rural', 'differences', '.']\n",
            "    After Regex: ['also', 'significant', 'regional', 'disparities', 'urban', 'rural', 'differences']\n",
            "    After Lemmatization: ['also', 'significant', 'regional', 'disparity', 'urban', 'rural', 'difference']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9641\n",
            "    After Stopword Removal: ['properties', 'physics', 'objects', 'restitution', ',', 'friction', ',', 'mass', ',', 'well', 'definitions', 'restitution', '/', 'friction', 'combination', 'modes', '.']\n",
            "    After Regex: ['properties', 'physics', 'objects', 'restitution', 'friction', 'mass', 'well', 'definitions', 'restitution', 'friction', 'combination', 'modes']\n",
            "    After Lemmatization: ['property', 'physic', 'object', 'restitution', 'friction', 'mass', 'well', 'definition', 'restitution', 'friction', 'combination', 'mode']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9642\n",
            "    After Stopword Removal: ['dry', 'food', 'cheaper', ',', 'wet', 'foods', 'benefits', '.']\n",
            "    After Regex: ['dry', 'food', 'cheaper', 'wet', 'foods', 'benefits']\n",
            "    After Lemmatization: ['dry', 'food', 'cheaper', 'wet', 'food', 'benefit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9643\n",
            "    After Stopword Removal: ['variety', 'reasons', ',', 'persons', 'homosexual', 'may', 'choose', 'identify', 'work', 'site', 'public', 'settings', '.']\n",
            "    After Regex: ['variety', 'reasons', 'persons', 'homosexual', 'may', 'choose', 'identify', 'work', 'site', 'public', 'settings']\n",
            "    After Lemmatization: ['variety', 'reason', 'person', 'homosexual', 'may', 'choose', 'identify', 'work', 'site', 'public', 'setting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9644\n",
            "    After Stopword Removal: ['library', 'institute', 'text', 'books', 'required', 'courses', 'along', 'numerous', 'reference', 'books', ',', 'research', 'journals', 'subscription', 'online', 'databases', 'web', 'science', 'scopus', '.']\n",
            "    After Regex: ['library', 'institute', 'text', 'books', 'required', 'courses', 'along', 'numerous', 'reference', 'books', 'research', 'journals', 'subscription', 'online', 'databases', 'web', 'science', 'scopus']\n",
            "    After Lemmatization: ['library', 'institute', 'text', 'book', 'required', 'course', 'along', 'numerous', 'reference', 'book', 'research', 'journal', 'subscription', 'online', 'database', 'web', 'science', 'scopus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9645\n",
            "    After Stopword Removal: ['drove', 'interest', 'rates', ':', 'volcker', 'decided', 'economy', 'needed', 'pick', '-', '-', ',', 'quickly', 'able', 'drive', 'interest', 'rate', 'treasury', 'bills', '13', 'percent', 'eight', 'percent', '.']\n",
            "    After Regex: ['drove', 'interest', 'rates', 'volcker', 'decided', 'economy', 'needed', 'pick', 'quickly', 'able', 'drive', 'interest', 'rate', 'treasury', 'bills', 'percent', 'eight', 'percent']\n",
            "    After Lemmatization: ['drove', 'interest', 'rate', 'volcker', 'decided', 'economy', 'needed', 'pick', 'quickly', 'able', 'drive', 'interest', 'rate', 'treasury', 'bill', 'percent', 'eight', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9646\n",
            "    After Stopword Removal: ['spectacular', 'road', 'built', ',', 'backwards', ',', 'using', 'enormous', 'digger', 'used', 'creating', 'reservoirs', 'vertical', 'landscape', '.']\n",
            "    After Regex: ['spectacular', 'road', 'built', 'backwards', 'using', 'enormous', 'digger', 'used', 'creating', 'reservoirs', 'vertical', 'landscape']\n",
            "    After Lemmatization: ['spectacular', 'road', 'built', 'backwards', 'using', 'enormous', 'digger', 'used', 'creating', 'reservoir', 'vertical', 'landscape']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9647\n",
            "    After Stopword Removal: ['national', 'authority', 'responsible', 'producer', 'registration', 'related', 'issues', 'centre', 'economic', 'development', ',', 'transport', 'environment', 'pirkanmaa', '.']\n",
            "    After Regex: ['national', 'authority', 'responsible', 'producer', 'registration', 'related', 'issues', 'centre', 'economic', 'development', 'transport', 'environment', 'pirkanmaa']\n",
            "    After Lemmatization: ['national', 'authority', 'responsible', 'producer', 'registration', 'related', 'issue', 'centre', 'economic', 'development', 'transport', 'environment', 'pirkanmaa']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9648\n",
            "    After Stopword Removal: ['warrior', 'society', '.']\n",
            "    After Regex: ['warrior', 'society']\n",
            "    After Lemmatization: ['warrior', 'society']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9649\n",
            "    After Stopword Removal: ['jambha', 'stone', '(', 'laterite', ')', 'otherwise', 'used', 'porous', '.']\n",
            "    After Regex: ['jambha', 'stone', 'laterite', 'otherwise', 'used', 'porous']\n",
            "    After Lemmatization: ['jambha', 'stone', 'laterite', 'otherwise', 'used', 'porous']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9650\n",
            "    After Stopword Removal: ['learned', 'teaching', '?']\n",
            "    After Regex: ['learned', 'teaching']\n",
            "    After Lemmatization: ['learned', 'teaching']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9651\n",
            "    After Stopword Removal: ['use', 'information', 'collected', 'information', 'collect', 'enables', 'us', 'offer', 'better', 'products', 'service', ',', 'may', 'use', 'data', 'purposes', '.']\n",
            "    After Regex: ['use', 'information', 'collected', 'information', 'collect', 'enables', 'us', 'offer', 'better', 'products', 'service', 'may', 'use', 'data', 'purposes']\n",
            "    After Lemmatization: ['use', 'information', 'collected', 'information', 'collect', 'enables', 'u', 'offer', 'better', 'product', 'service', 'may', 'use', 'data', 'purpose']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9652\n",
            "    After Stopword Removal: ['wire', '36', 'may', ',', 'example', ',30', 'gauge', 'copper', 'wire', '.']\n",
            "    After Regex: ['wire', 'may', 'example', 'gauge', 'copper', 'wire']\n",
            "    After Lemmatization: ['wire', 'may', 'example', 'gauge', 'copper', 'wire']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9653\n",
            "    After Stopword Removal: ['find', 'organisation', 'reduce', 'costs', ',', 'save', 'money', 'learn', 'roi', 'investing', 'lone', 'worker', 'safety', 'monitoring', 'solutions', '.']\n",
            "    After Regex: ['find', 'organisation', 'reduce', 'costs', 'save', 'money', 'learn', 'roi', 'investing', 'lone', 'worker', 'safety', 'monitoring', 'solutions']\n",
            "    After Lemmatization: ['find', 'organisation', 'reduce', 'cost', 'save', 'money', 'learn', 'roi', 'investing', 'lone', 'worker', 'safety', 'monitoring', 'solution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9654\n",
            "    After Stopword Removal: ['-', '-', 'profits', 'targeted', 'scams', 'fraud', ',', \"'\", 'important', 'aware', 'threats', 'protect', '.']\n",
            "    After Regex: ['profits', 'targeted', 'scams', 'fraud', 'important', 'aware', 'threats', 'protect']\n",
            "    After Lemmatization: ['profit', 'targeted', 'scam', 'fraud', 'important', 'aware', 'threat', 'protect']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9655\n",
            "    After Stopword Removal: ['may', 'case', 'chauncey', 'barnes', ',', 'managed', 'operations', 'baltimore', 'mobile', 'mid', '-1840', '.']\n",
            "    After Regex: ['may', 'case', 'chauncey', 'barnes', 'managed', 'operations', 'baltimore', 'mobile', 'mid']\n",
            "    After Lemmatization: ['may', 'case', 'chauncey', 'barnes', 'managed', 'operation', 'baltimore', 'mobile', 'mid']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9656\n",
            "    After Stopword Removal: ['condition', 'road', 'poor', ',', 'ageing', 'highway', 'localised', 'surface', 'defects', 'caused', 'potholes', ',', 'deep', 'ruts', 'heavy', 'degrading', 'white', 'lining', '.']\n",
            "    After Regex: ['condition', 'road', 'poor', 'ageing', 'highway', 'localised', 'surface', 'defects', 'caused', 'potholes', 'deep', 'ruts', 'heavy', 'degrading', 'white', 'lining']\n",
            "    After Lemmatization: ['condition', 'road', 'poor', 'ageing', 'highway', 'localised', 'surface', 'defect', 'caused', 'pothole', 'deep', 'rut', 'heavy', 'degrading', 'white', 'lining']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9657\n",
            "    After Stopword Removal: ['see', '[3.6]', 'reference', 'concerns', 'expressed', ',', 'amongst', 'others', ',', 'us', 'government', 'accountability', 'office', '.']\n",
            "    After Regex: ['see', 'reference', 'concerns', 'expressed', 'amongst', 'others', 'us', 'government', 'accountability', 'office']\n",
            "    After Lemmatization: ['see', 'reference', 'concern', 'expressed', 'amongst', 'others', 'u', 'government', 'accountability', 'office']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9658\n",
            "    After Stopword Removal: ['parents', 'provide', 'genetic', 'blueprints', 'birth', ',', 'raw', 'genetic', 'material', 'appears', 'malleable', 'environmental', 'influences', ',', 'including', 'toxic', 'chemicals', 'vaccines', '.']\n",
            "    After Regex: ['parents', 'provide', 'genetic', 'blueprints', 'birth', 'raw', 'genetic', 'material', 'appears', 'malleable', 'environmental', 'influences', 'including', 'toxic', 'chemicals', 'vaccines']\n",
            "    After Lemmatization: ['parent', 'provide', 'genetic', 'blueprint', 'birth', 'raw', 'genetic', 'material', 'appears', 'malleable', 'environmental', 'influence', 'including', 'toxic', 'chemical', 'vaccine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9659\n",
            "    After Stopword Removal: ['problem', 'authors', 'assumed', ',', 'without', 'justification', ',', 'black', 'sea', 'flood', 'one', ',', 'world', '-', 'wide', 'flood', 'thousands', 'years', 'earlier', 'never', 'took', 'place', '.']\n",
            "    After Regex: ['problem', 'authors', 'assumed', 'without', 'justification', 'black', 'sea', 'flood', 'one', 'world', 'wide', 'flood', 'thousands', 'years', 'earlier', 'never', 'took', 'place']\n",
            "    After Lemmatization: ['problem', 'author', 'assumed', 'without', 'justification', 'black', 'sea', 'flood', 'one', 'world', 'wide', 'flood', 'thousand', 'year', 'earlier', 'never', 'took', 'place']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9660\n",
            "    After Stopword Removal: ['experimental', 'results', 'obtained', 'students', 'term', 'project', 'also', 'presented', 'compared', 'theoretical', 'calculations', 'made', 'classroom', '.']\n",
            "    After Regex: ['experimental', 'results', 'obtained', 'students', 'term', 'project', 'also', 'presented', 'compared', 'theoretical', 'calculations', 'made', 'classroom']\n",
            "    After Lemmatization: ['experimental', 'result', 'obtained', 'student', 'term', 'project', 'also', 'presented', 'compared', 'theoretical', 'calculation', 'made', 'classroom']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9661\n",
            "    After Stopword Removal: ['joint', 'school', 'nanoscience', 'nanoengineering', '(', 'jsnn', ')', 'joint', 'school', 'nanoscience', 'nanoengineering', 'joint', 'collaboration', 'two', 'major', 'universities', 'complementary', 'strengths', 'histories', '-', 'north', 'carolina', '&', 'state', 'university', '(', 'nc', '&', ')', 'university', 'north', 'carolina', 'greensboro', '(', 'uncg', ').']\n",
            "    After Regex: ['joint', 'school', 'nanoscience', 'nanoengineering', 'jsnn', 'joint', 'school', 'nanoscience', 'nanoengineering', 'joint', 'collaboration', 'two', 'major', 'universities', 'complementary', 'strengths', 'histories', 'north', 'carolina', 'state', 'university', 'nc', 'university', 'north', 'carolina', 'greensboro', 'uncg']\n",
            "    After Lemmatization: ['joint', 'school', 'nanoscience', 'nanoengineering', 'jsnn', 'joint', 'school', 'nanoscience', 'nanoengineering', 'joint', 'collaboration', 'two', 'major', 'university', 'complementary', 'strength', 'history', 'north', 'carolina', 'state', 'university', 'nc', 'university', 'north', 'carolina', 'greensboro', 'uncg']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9662\n",
            "    After Stopword Removal: ['cities', 'ordinance', 'implement', 'enforce', 'wellhead', 'protection', 'program', 'portion', 'area', '.']\n",
            "    After Regex: ['cities', 'ordinance', 'implement', 'enforce', 'wellhead', 'protection', 'program', 'portion', 'area']\n",
            "    After Lemmatization: ['city', 'ordinance', 'implement', 'enforce', 'wellhead', 'protection', 'program', 'portion', 'area']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9663\n",
            "    After Stopword Removal: ['heat', 'melt', 'wax', 'fuse', 'tissue', 'canvas', '.']\n",
            "    After Regex: ['heat', 'melt', 'wax', 'fuse', 'tissue', 'canvas']\n",
            "    After Lemmatization: ['heat', 'melt', 'wax', 'fuse', 'tissue', 'canvas']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9664\n",
            "    After Stopword Removal: ['another', 'way', 'companies', 'adjusted', 'practices', 'reduction', 'hours', 'workforce', ';42%', 'companies', 'reduced', 'plan', 'reduce', 'hours', 'worked', 'restaurant', 'employees', '.']\n",
            "    After Regex: ['another', 'way', 'companies', 'adjusted', 'practices', 'reduction', 'hours', 'workforce', 'companies', 'reduced', 'plan', 'reduce', 'hours', 'worked', 'restaurant', 'employees']\n",
            "    After Lemmatization: ['another', 'way', 'company', 'adjusted', 'practice', 'reduction', 'hour', 'workforce', 'company', 'reduced', 'plan', 'reduce', 'hour', 'worked', 'restaurant', 'employee']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9665\n",
            "    After Stopword Removal: ['area', 'known', 'ruins', 'silvermoon', 'makes', 'western', 'half', 'silvermoon', 'city', ',', 'capital', 'city', 'blood', 'elves', 'former', 'kingdom', 'quel', \"'\", 'thalas', '.']\n",
            "    After Regex: ['area', 'known', 'ruins', 'silvermoon', 'makes', 'western', 'half', 'silvermoon', 'city', 'capital', 'city', 'blood', 'elves', 'former', 'kingdom', 'quel', 'thalas']\n",
            "    After Lemmatization: ['area', 'known', 'ruin', 'silvermoon', 'make', 'western', 'half', 'silvermoon', 'city', 'capital', 'city', 'blood', 'elf', 'former', 'kingdom', 'quel', 'thalas']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9666\n",
            "    After Stopword Removal: ['prevention', ',', 'public', 'health', 'broadest', 'sense', ',', 'equity', 'looking', 'minority', 'majority', 'imposed', '.']\n",
            "    After Regex: ['prevention', 'public', 'health', 'broadest', 'sense', 'equity', 'looking', 'minority', 'majority', 'imposed']\n",
            "    After Lemmatization: ['prevention', 'public', 'health', 'broadest', 'sense', 'equity', 'looking', 'minority', 'majority', 'imposed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9667\n",
            "    After Stopword Removal: ['explanations', 'russian', 'revolution', ',', 'mentioned', 'first', 'time', 'bloodshed', 'would', 'encountered', 'revolution', '.']\n",
            "    After Regex: ['explanations', 'russian', 'revolution', 'mentioned', 'first', 'time', 'bloodshed', 'would', 'encountered', 'revolution']\n",
            "    After Lemmatization: ['explanation', 'russian', 'revolution', 'mentioned', 'first', 'time', 'bloodshed', 'would', 'encountered', 'revolution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9668\n",
            "    After Stopword Removal: ['states', 'rely', 'high', '-', 'tech', 'means', 'retrieving', 'information', 'known', 'automatic', 'license', 'plate', 'recognition', '.']\n",
            "    After Regex: ['states', 'rely', 'high', 'tech', 'means', 'retrieving', 'information', 'known', 'automatic', 'license', 'plate', 'recognition']\n",
            "    After Lemmatization: ['state', 'rely', 'high', 'tech', 'mean', 'retrieving', 'information', 'known', 'automatic', 'license', 'plate', 'recognition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9669\n",
            "    After Stopword Removal: ['thomas', 'spent', 'summer', '2003', 'caida', 'developing', 'methodologies', 'identify', 'measure', 'peer', '-', '-', 'peer', 'file', '-', 'sharing', 'traffic', 'internet', 'core', '.']\n",
            "    After Regex: ['thomas', 'spent', 'summer', 'caida', 'developing', 'methodologies', 'identify', 'measure', 'peer', 'peer', 'file', 'sharing', 'traffic', 'internet', 'core']\n",
            "    After Lemmatization: ['thomas', 'spent', 'summer', 'caida', 'developing', 'methodology', 'identify', 'measure', 'peer', 'peer', 'file', 'sharing', 'traffic', 'internet', 'core']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9670\n",
            "    After Stopword Removal: ['global', 'collaborative', 'network', 'connecting', 'stakeholders', 'secure', ',', 'regulated', ',', 'private', 'system', 'common', 'adjudication', 'process', 'ensuring', 'identity', 'healthcare', 'professionals', 'protect', 'ensure', 'safety', 'data', 'exchange', '.']\n",
            "    After Regex: ['global', 'collaborative', 'network', 'connecting', 'stakeholders', 'secure', 'regulated', 'private', 'system', 'common', 'adjudication', 'process', 'ensuring', 'identity', 'healthcare', 'professionals', 'protect', 'ensure', 'safety', 'data', 'exchange']\n",
            "    After Lemmatization: ['global', 'collaborative', 'network', 'connecting', 'stakeholder', 'secure', 'regulated', 'private', 'system', 'common', 'adjudication', 'process', 'ensuring', 'identity', 'healthcare', 'professional', 'protect', 'ensure', 'safety', 'data', 'exchange']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9671\n",
            "    After Stopword Removal: ['projects', 'range', 'straightforward', 'flat', 'patterns', ',', 'complex', 'assemblies', 'require', 'secondary', 'processes', '(', 'deburring', ',', 'welding', ',', 'forming', ')', 'completed', 'house', '.']\n",
            "    After Regex: ['projects', 'range', 'straightforward', 'flat', 'patterns', 'complex', 'assemblies', 'require', 'secondary', 'processes', 'deburring', 'welding', 'forming', 'completed', 'house']\n",
            "    After Lemmatization: ['project', 'range', 'straightforward', 'flat', 'pattern', 'complex', 'assembly', 'require', 'secondary', 'process', 'deburring', 'welding', 'forming', 'completed', 'house']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9672\n",
            "    After Stopword Removal: ['neuroscience', '208,58-68.']\n",
            "    After Regex: ['neuroscience']\n",
            "    After Lemmatization: ['neuroscience']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9673\n",
            "    After Stopword Removal: ['far', 'beijing', 'stood', 'firm', 'resisting', 'pressures', 'local', 'governments', 'relax', 'administrative', 'restrictions', 'house', 'purchases', '.']\n",
            "    After Regex: ['far', 'beijing', 'stood', 'firm', 'resisting', 'pressures', 'local', 'governments', 'relax', 'administrative', 'restrictions', 'house', 'purchases']\n",
            "    After Lemmatization: ['far', 'beijing', 'stood', 'firm', 'resisting', 'pressure', 'local', 'government', 'relax', 'administrative', 'restriction', 'house', 'purchase']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9674\n",
            "    After Stopword Removal: ['particular', ',', 'finite', 'representation', 'type', 'every', 'module', 'moda', 'almost', 'projective', 'injective', '.']\n",
            "    After Regex: ['particular', 'finite', 'representation', 'type', 'every', 'module', 'moda', 'almost', 'projective', 'injective']\n",
            "    After Lemmatization: ['particular', 'finite', 'representation', 'type', 'every', 'module', 'moda', 'almost', 'projective', 'injective']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9675\n",
            "    After Stopword Removal: ['particular', 'use', 'may', 'psychology', 'department', \"'\", 'faq', '.']\n",
            "    After Regex: ['particular', 'use', 'may', 'psychology', 'department', 'faq']\n",
            "    After Lemmatization: ['particular', 'use', 'may', 'psychology', 'department', 'faq']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9676\n",
            "    After Stopword Removal: ['san', 'marcos', 'area', 'southernmost', 'occurrence', 'mortoniceras', 'ammonites', 'macraster', 'echinoids', 'encountered', 'author', '.']\n",
            "    After Regex: ['san', 'marcos', 'area', 'southernmost', 'occurrence', 'mortoniceras', 'ammonites', 'macraster', 'echinoids', 'encountered', 'author']\n",
            "    After Lemmatization: ['san', 'marcos', 'area', 'southernmost', 'occurrence', 'mortoniceras', 'ammonite', 'macraster', 'echinoids', 'encountered', 'author']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9677\n",
            "    After Stopword Removal: ['first', 'man', ',', 'known', 'bridgeless', 'baron', 'magnus', 'vonbiddlewort', '—', 'usually', 'spoken', 'hushed', 'tones', 'sideways', 'glances', 'sure', \"'\", 'standing', 'nearby', ',', 'nearly', 'always', '—', 'made', 'fortune', ',', 'made', 'name', ',', 'generally', 'made', 'rather', 'large', 'unnecessary', 'presence', 'bridgeless', '.']\n",
            "    After Regex: ['first', 'man', 'known', 'bridgeless', 'baron', 'magnus', 'vonbiddlewort', 'usually', 'spoken', 'hushed', 'tones', 'sideways', 'glances', 'sure', 'standing', 'nearby', 'nearly', 'always', 'made', 'fortune', 'made', 'name', 'generally', 'made', 'rather', 'large', 'unnecessary', 'presence', 'bridgeless']\n",
            "    After Lemmatization: ['first', 'man', 'known', 'bridgeless', 'baron', 'magnus', 'vonbiddlewort', 'usually', 'spoken', 'hushed', 'tone', 'sideways', 'glance', 'sure', 'standing', 'nearby', 'nearly', 'always', 'made', 'fortune', 'made', 'name', 'generally', 'made', 'rather', 'large', 'unnecessary', 'presence', 'bridgeless']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9678\n",
            "    After Stopword Removal: ['benefit', 'winter', 'mulch', 'keep', 'vulnerable', 'trees', 'shrubs', 'heaving', 'thaws', ',', 'plus', 'allowing', 'roots', 'absorb', 'moisture', '.']\n",
            "    After Regex: ['benefit', 'winter', 'mulch', 'keep', 'vulnerable', 'trees', 'shrubs', 'heaving', 'thaws', 'plus', 'allowing', 'roots', 'absorb', 'moisture']\n",
            "    After Lemmatization: ['benefit', 'winter', 'mulch', 'keep', 'vulnerable', 'tree', 'shrub', 'heaving', 'thaw', 'plus', 'allowing', 'root', 'absorb', 'moisture']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9679\n",
            "    After Stopword Removal: ['together', ',', 'techniques', 'could', 'hopefully', 'make', 'lot', 'progress', 'problem', 'staring', 'five', 'packages', 'linear', 'algebra', 'haskell', 'wondering', 'exactly', 'differ', ',', 'one', 'use', '.']\n",
            "    After Regex: ['together', 'techniques', 'could', 'hopefully', 'make', 'lot', 'progress', 'problem', 'staring', 'five', 'packages', 'linear', 'algebra', 'haskell', 'wondering', 'exactly', 'differ', 'one', 'use']\n",
            "    After Lemmatization: ['together', 'technique', 'could', 'hopefully', 'make', 'lot', 'progress', 'problem', 'staring', 'five', 'package', 'linear', 'algebra', 'haskell', 'wondering', 'exactly', 'differ', 'one', 'use']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9680\n",
            "    After Stopword Removal: ['second', ',', 'epa', 'ought', 'ban', 'cancer', '-', 'causing', 'nerve', '-', 'damaging', 'pesticides', '.']\n",
            "    After Regex: ['second', 'epa', 'ought', 'ban', 'cancer', 'causing', 'nerve', 'damaging', 'pesticides']\n",
            "    After Lemmatization: ['second', 'epa', 'ought', 'ban', 'cancer', 'causing', 'nerve', 'damaging', 'pesticide']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9681\n",
            "    After Stopword Removal: ['obtain', '?']\n",
            "    After Regex: ['obtain']\n",
            "    After Lemmatization: ['obtain']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9682\n",
            "    After Stopword Removal: ['freedom', 'pluralism', 'media', 'guaranteed', '.']\n",
            "    After Regex: ['freedom', 'pluralism', 'media', 'guaranteed']\n",
            "    After Lemmatization: ['freedom', 'pluralism', 'medium', 'guaranteed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9683\n",
            "    After Stopword Removal: ['ought', 'know', \"'\", 'negatively', 'affecting', 'indoor', 'air', 'quality', 'make', 'better', '.']\n",
            "    After Regex: ['ought', 'know', 'negatively', 'affecting', 'indoor', 'air', 'quality', 'make', 'better']\n",
            "    After Lemmatization: ['ought', 'know', 'negatively', 'affecting', 'indoor', 'air', 'quality', 'make', 'better']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9684\n",
            "    After Stopword Removal: [\"'\", 'reilly', 'wrote', 'senior', 'thesis', 'history', 'development', 'isles', 'nonprofit', 'community', 'environmental', 'organization', '.']\n",
            "    After Regex: ['reilly', 'wrote', 'senior', 'thesis', 'history', 'development', 'isles', 'nonprofit', 'community', 'environmental', 'organization']\n",
            "    After Lemmatization: ['reilly', 'wrote', 'senior', 'thesis', 'history', 'development', 'isle', 'nonprofit', 'community', 'environmental', 'organization']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9685\n",
            "    After Stopword Removal: ['could', 'done', 'partnering', 'various', 'community', 'organizations', 'disseminate', 'information', 'caregivers', 'presenting', 'information', 'events', 'locations', 'community', 'older', 'adults', 'family', 'caregivers', 'go', 'physicians', \"'\", 'offices', ',', 'places', 'worship', ',', 'senior', 'centers', '.']\n",
            "    After Regex: ['could', 'done', 'partnering', 'various', 'community', 'organizations', 'disseminate', 'information', 'caregivers', 'presenting', 'information', 'events', 'locations', 'community', 'older', 'adults', 'family', 'caregivers', 'go', 'physicians', 'offices', 'places', 'worship', 'senior', 'centers']\n",
            "    After Lemmatization: ['could', 'done', 'partnering', 'various', 'community', 'organization', 'disseminate', 'information', 'caregiver', 'presenting', 'information', 'event', 'location', 'community', 'older', 'adult', 'family', 'caregiver', 'go', 'physician', 'office', 'place', 'worship', 'senior', 'center']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9686\n",
            "    After Stopword Removal: ['located', '118', 'feet', 'ground', ',', 'antelope', 'canyon', 'earned', \"'\", 'name', 'antelope', 'called', 'home', '.']\n",
            "    After Regex: ['located', 'feet', 'ground', 'antelope', 'canyon', 'earned', 'name', 'antelope', 'called', 'home']\n",
            "    After Lemmatization: ['located', 'foot', 'ground', 'antelope', 'canyon', 'earned', 'name', 'antelope', 'called', 'home']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9687\n",
            "    After Stopword Removal: ['stress', 'also', 'cause', 'tmj', '.']\n",
            "    After Regex: ['stress', 'also', 'cause', 'tmj']\n",
            "    After Lemmatization: ['stress', 'also', 'cause', 'tmj']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9688\n",
            "    After Stopword Removal: ['jonah', 'goldberg', 'aside', ',', \"'\", 'basic', 'definition', 'legitimate', 'scholars', 'field', 'agree', ',', 'one', \"'\", 'referring', '.']\n",
            "    After Regex: ['jonah', 'goldberg', 'aside', 'basic', 'definition', 'legitimate', 'scholars', 'field', 'agree', 'one', 'referring']\n",
            "    After Lemmatization: ['jonah', 'goldberg', 'aside', 'basic', 'definition', 'legitimate', 'scholar', 'field', 'agree', 'one', 'referring']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9689\n",
            "    After Stopword Removal: [\"'\", 'also', 'find', 'help', 'study', 'skills', ',', 'broad', 'list', 'topics', ',', 'solid', 'search', 'mechanism', '.']\n",
            "    After Regex: ['also', 'find', 'help', 'study', 'skills', 'broad', 'list', 'topics', 'solid', 'search', 'mechanism']\n",
            "    After Lemmatization: ['also', 'find', 'help', 'study', 'skill', 'broad', 'list', 'topic', 'solid', 'search', 'mechanism']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9690\n",
            "    After Stopword Removal: ['sti', 'caused', 'virus', ',', 'cure', '.']\n",
            "    After Regex: ['sti', 'caused', 'virus', 'cure']\n",
            "    After Lemmatization: ['sti', 'caused', 'virus', 'cure']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9691\n",
            "    After Stopword Removal: ['world', \"'\", 'largest', 'food', 'manufacturers', ',', 'including', 'kraft', ',', 'tyson', 'nestle', ',', 'also', 'indicated', 'pass', 'increase', 'consumers', '.']\n",
            "    After Regex: ['world', 'largest', 'food', 'manufacturers', 'including', 'kraft', 'tyson', 'nestle', 'also', 'indicated', 'pass', 'increase', 'consumers']\n",
            "    After Lemmatization: ['world', 'largest', 'food', 'manufacturer', 'including', 'kraft', 'tyson', 'nestle', 'also', 'indicated', 'pas', 'increase', 'consumer']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9692\n",
            "    After Stopword Removal: ['2012', 'jobs', 'act', 'amended', 'federal', 'securities', 'law', 'benefit', 'small', 'emerging', 'businesses', 'easing', 'rules', 'public', 'offerings', 'small', 'businesses', 'broadening', 'base', 'people', 'allowed', 'buy', 'equity', 'companies', '.']\n",
            "    After Regex: ['jobs', 'act', 'amended', 'federal', 'securities', 'law', 'benefit', 'small', 'emerging', 'businesses', 'easing', 'rules', 'public', 'offerings', 'small', 'businesses', 'broadening', 'base', 'people', 'allowed', 'buy', 'equity', 'companies']\n",
            "    After Lemmatization: ['job', 'act', 'amended', 'federal', 'security', 'law', 'benefit', 'small', 'emerging', 'business', 'easing', 'rule', 'public', 'offering', 'small', 'business', 'broadening', 'base', 'people', 'allowed', 'buy', 'equity', 'company']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9693\n",
            "    After Stopword Removal: ['period', 'time', ',', 'efforts', 'underway', 'united', 'states', 'europe', 'build', 'supersonic', 'commercial', 'aircraft', '.']\n",
            "    After Regex: ['period', 'time', 'efforts', 'underway', 'united', 'states', 'europe', 'build', 'supersonic', 'commercial', 'aircraft']\n",
            "    After Lemmatization: ['period', 'time', 'effort', 'underway', 'united', 'state', 'europe', 'build', 'supersonic', 'commercial', 'aircraft']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9694\n",
            "    After Stopword Removal: ['reason', 'common', 'sense', 'tell', 'risks', 'far', 'outweigh', 'potential', '(', 'cern', 'physicists', 'say', ')', 'benefits', '.']\n",
            "    After Regex: ['reason', 'common', 'sense', 'tell', 'risks', 'far', 'outweigh', 'potential', 'cern', 'physicists', 'say', 'benefits']\n",
            "    After Lemmatization: ['reason', 'common', 'sense', 'tell', 'risk', 'far', 'outweigh', 'potential', 'cern', 'physicist', 'say', 'benefit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9695\n",
            "    After Stopword Removal: ['career', 'evolved', 'became', 'respected', 'portrait', 'painter', 'important', 'sitters', 'tsar', 'alexander', 'ii', ',', 'prince', 'demidoff', '-', 'san', 'domato', 'ivan', 'tourguéneff', '.']\n",
            "    After Regex: ['career', 'evolved', 'became', 'respected', 'portrait', 'painter', 'important', 'sitters', 'tsar', 'alexander', 'ii', 'prince', 'demidoff', 'san', 'domato', 'ivan', 'tourguneff']\n",
            "    After Lemmatization: ['career', 'evolved', 'became', 'respected', 'portrait', 'painter', 'important', 'sitter', 'tsar', 'alexander', 'ii', 'prince', 'demidoff', 'san', 'domato', 'ivan', 'tourguneff']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9696\n",
            "    After Stopword Removal: ['meanwhile', ',', 'nab', 'working', 'ngos', 'universities', 'develop', 'adaptation', 'financing', 'solutions', 'share', 'ideas', '.']\n",
            "    After Regex: ['meanwhile', 'nab', 'working', 'ngos', 'universities', 'develop', 'adaptation', 'financing', 'solutions', 'share', 'ideas']\n",
            "    After Lemmatization: ['meanwhile', 'nab', 'working', 'ngo', 'university', 'develop', 'adaptation', 'financing', 'solution', 'share', 'idea']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9697\n",
            "    After Stopword Removal: ['sandoval', 'became', 'first', 'republican', 'governor', 'accept', 'expanding', 'medicaid', 'eligibility', 'called', 'federal', 'patient', 'protection', 'affordable', 'care', 'act', '.']\n",
            "    After Regex: ['sandoval', 'became', 'first', 'republican', 'governor', 'accept', 'expanding', 'medicaid', 'eligibility', 'called', 'federal', 'patient', 'protection', 'affordable', 'care', 'act']\n",
            "    After Lemmatization: ['sandoval', 'became', 'first', 'republican', 'governor', 'accept', 'expanding', 'medicaid', 'eligibility', 'called', 'federal', 'patient', 'protection', 'affordable', 'care', 'act']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9698\n",
            "    After Stopword Removal: ['key', 'genres', 'history', ',', 'science', 'natural', 'history', '.']\n",
            "    After Regex: ['key', 'genres', 'history', 'science', 'natural', 'history']\n",
            "    After Lemmatization: ['key', 'genre', 'history', 'science', 'natural', 'history']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9699\n",
            "    After Stopword Removal: ['dippers', 'catch', 'food', 'water', ',', 'jump', 'dive', 'frigid', 'water', 'forage', '.']\n",
            "    After Regex: ['dippers', 'catch', 'food', 'water', 'jump', 'dive', 'frigid', 'water', 'forage']\n",
            "    After Lemmatization: ['dipper', 'catch', 'food', 'water', 'jump', 'dive', 'frigid', 'water', 'forage']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9700\n",
            "    After Stopword Removal: ['quite', 'yet', ',', 'headway', 'made', 'making', 'graphene', 'successor', 'silicon', 'semiconductor', 'electronics', '.']\n",
            "    After Regex: ['quite', 'yet', 'headway', 'made', 'making', 'graphene', 'successor', 'silicon', 'semiconductor', 'electronics']\n",
            "    After Lemmatization: ['quite', 'yet', 'headway', 'made', 'making', 'graphene', 'successor', 'silicon', 'semiconductor', 'electronics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9701\n",
            "    After Stopword Removal: ['despite', 'fact', 'growing', 'consensus', '—', 'among', 'even', 'conservative', 'economists', 'policy', 'makers', '—', 'tax', 'cuts', 'little', 'nothing', 'stimulate', 'job', 'creation', 'country', 'lost', 'almost', '600,000', 'positions', 'january', 'alone', '.']\n",
            "    After Regex: ['despite', 'fact', 'growing', 'consensus', 'among', 'even', 'conservative', 'economists', 'policy', 'makers', 'tax', 'cuts', 'little', 'nothing', 'stimulate', 'job', 'creation', 'country', 'lost', 'almost', 'positions', 'january', 'alone']\n",
            "    After Lemmatization: ['despite', 'fact', 'growing', 'consensus', 'among', 'even', 'conservative', 'economist', 'policy', 'maker', 'tax', 'cut', 'little', 'nothing', 'stimulate', 'job', 'creation', 'country', 'lost', 'almost', 'position', 'january', 'alone']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9702\n",
            "    After Stopword Removal: ['martin', '(2003)', 'says', \"'\", 'culturally', ',', 'birth', 'become', 'real', 'outsider', 'gaze', 'lived', 'bodily', 'experience', \"'(2003:64).\"]\n",
            "    After Regex: ['martin', 'says', 'culturally', 'birth', 'become', 'real', 'outsider', 'gaze', 'lived', 'bodily', 'experience']\n",
            "    After Lemmatization: ['martin', 'say', 'culturally', 'birth', 'become', 'real', 'outsider', 'gaze', 'lived', 'bodily', 'experience']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9703\n",
            "    After Stopword Removal: ['compare', 'number', 'number', 'valence', 'electrons', 'one', 'n', 'four', 'h', \"'\", '.']\n",
            "    After Regex: ['compare', 'number', 'number', 'valence', 'electrons', 'one', 'n', 'four', 'h']\n",
            "    After Lemmatization: ['compare', 'number', 'number', 'valence', 'electron', 'one', 'n', 'four', 'h']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9704\n",
            "    After Stopword Removal: ['hispanic', 'women', ',', 'figure', '0.4', 'percent', '.']\n",
            "    After Regex: ['hispanic', 'women', 'figure', 'percent']\n",
            "    After Lemmatization: ['hispanic', 'woman', 'figure', 'percent']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9705\n",
            "    After Stopword Removal: ['small', 'bowel', 'obstruction', ':', 'causes', 'management', '.']\n",
            "    After Regex: ['small', 'bowel', 'obstruction', 'causes', 'management']\n",
            "    After Lemmatization: ['small', 'bowel', 'obstruction', 'cause', 'management']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9706\n",
            "    After Stopword Removal: ['however', ',', 'detailed', 'information', 'direct', 'measurements', 'exact', 'time', 'energy', 'structure', 'x', '-', 'ray', 'pulse', ',', 'x', '-', 'ray', 'science', 'enter', 'new', 'era', 'time', 'resolved', 'coherence', 'dependent', 'experiments', '.']\n",
            "    After Regex: ['however', 'detailed', 'information', 'direct', 'measurements', 'exact', 'time', 'energy', 'structure', 'x', 'ray', 'pulse', 'x', 'ray', 'science', 'enter', 'new', 'era', 'time', 'resolved', 'coherence', 'dependent', 'experiments']\n",
            "    After Lemmatization: ['however', 'detailed', 'information', 'direct', 'measurement', 'exact', 'time', 'energy', 'structure', 'x', 'ray', 'pulse', 'x', 'ray', 'science', 'enter', 'new', 'era', 'time', 'resolved', 'coherence', 'dependent', 'experiment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9707\n",
            "    After Stopword Removal: ['fighting', 'ensued', 'chinese', 'japanese', 'forces', 'ended', 'easy', 'victory', 'modern', 'japanese', 'army', '.']\n",
            "    After Regex: ['fighting', 'ensued', 'chinese', 'japanese', 'forces', 'ended', 'easy', 'victory', 'modern', 'japanese', 'army']\n",
            "    After Lemmatization: ['fighting', 'ensued', 'chinese', 'japanese', 'force', 'ended', 'easy', 'victory', 'modern', 'japanese', 'army']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9708\n",
            "    After Stopword Removal: ['network', 'supports', 'global', 'alliance', 'practitioners', ',', 'researchers', ',', 'advocates', ',', 'people', 'living', 'hiv', ',', 'donors', 'providing', 'one', '-', 'stop', 'shop', 'sharing', 'exchanging', 'information', 'best', 'practices', ',', 'knowledge', ',', 'resources', ',', 'research', '.']\n",
            "    After Regex: ['network', 'supports', 'global', 'alliance', 'practitioners', 'researchers', 'advocates', 'people', 'living', 'hiv', 'donors', 'providing', 'one', 'stop', 'shop', 'sharing', 'exchanging', 'information', 'best', 'practices', 'knowledge', 'resources', 'research']\n",
            "    After Lemmatization: ['network', 'support', 'global', 'alliance', 'practitioner', 'researcher', 'advocate', 'people', 'living', 'hiv', 'donor', 'providing', 'one', 'stop', 'shop', 'sharing', 'exchanging', 'information', 'best', 'practice', 'knowledge', 'resource', 'research']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9709\n",
            "    After Stopword Removal: ['pensions', '(', 'retirees', ')', 'increase', 'year', 'inflation', 'guaranteed', 'taxpayers', 'forever', '—', 'regardless', 'happens', 'economy', 'whether', 'state', \"'\", 'pensions', 'funds', 'fully', 'funded', '(', \"'\", ').', '2008', 'state', 'commission', 'pegged', 'california', \"'\", 'unfunded', 'pension', 'liability', '$63.5', 'billion', ',', 'amortized', 'several', 'decades', '.']\n",
            "    After Regex: ['pensions', 'retirees', 'increase', 'year', 'inflation', 'guaranteed', 'taxpayers', 'forever', 'regardless', 'happens', 'economy', 'whether', 'state', 'pensions', 'funds', 'fully', 'funded', 'state', 'commission', 'pegged', 'california', 'unfunded', 'pension', 'liability', 'billion', 'amortized', 'several', 'decades']\n",
            "    After Lemmatization: ['pension', 'retiree', 'increase', 'year', 'inflation', 'guaranteed', 'taxpayer', 'forever', 'regardless', 'happens', 'economy', 'whether', 'state', 'pension', 'fund', 'fully', 'funded', 'state', 'commission', 'pegged', 'california', 'unfunded', 'pension', 'liability', 'billion', 'amortized', 'several', 'decade']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9710\n",
            "    After Stopword Removal: ['michigan', 'fast', 'facts', 'fast', 'facts', 'web', 'site', 'source', 'information', 'recent', 'employment', 'trends', ',', 'wages', ',', 'detailed', 'industries', ',', 'jobs', 'demand', '.']\n",
            "    After Regex: ['michigan', 'fast', 'facts', 'fast', 'facts', 'web', 'site', 'source', 'information', 'recent', 'employment', 'trends', 'wages', 'detailed', 'industries', 'jobs', 'demand']\n",
            "    After Lemmatization: ['michigan', 'fast', 'fact', 'fast', 'fact', 'web', 'site', 'source', 'information', 'recent', 'employment', 'trend', 'wage', 'detailed', 'industry', 'job', 'demand']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9711\n",
            "    After Stopword Removal: ['changing', 'role', 'teachers', 'development', 'learner', 'autonomy', 'based', 'survey', 'english', 'dorm', 'activity', '.']\n",
            "    After Regex: ['changing', 'role', 'teachers', 'development', 'learner', 'autonomy', 'based', 'survey', 'english', 'dorm', 'activity']\n",
            "    After Lemmatization: ['changing', 'role', 'teacher', 'development', 'learner', 'autonomy', 'based', 'survey', 'english', 'dorm', 'activity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9712\n",
            "    After Stopword Removal: ['cardiac', 'stable', '–', 'minimal', 'changes', 'heart', 'rate', ',', 'stroke', 'volume', '(', 'sv', '),', 'cardiac', 'output', '(', 'co', ').']\n",
            "    After Regex: ['cardiac', 'stable', 'minimal', 'changes', 'heart', 'rate', 'stroke', 'volume', 'sv', 'cardiac', 'output', 'co']\n",
            "    After Lemmatization: ['cardiac', 'stable', 'minimal', 'change', 'heart', 'rate', 'stroke', 'volume', 'sv', 'cardiac', 'output', 'co']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9713\n",
            "    After Stopword Removal: ['mammals', 'smooth', '-', 'surfaced', 'nonconvoluted', 'brains', 'called', 'lissencephalics', 'folded', 'convoluted', 'brains', 'gyrencephalics', '.']\n",
            "    After Regex: ['mammals', 'smooth', 'surfaced', 'nonconvoluted', 'brains', 'called', 'lissencephalics', 'folded', 'convoluted', 'brains', 'gyrencephalics']\n",
            "    After Lemmatization: ['mammal', 'smooth', 'surfaced', 'nonconvoluted', 'brain', 'called', 'lissencephalics', 'folded', 'convoluted', 'brain', 'gyrencephalics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9714\n",
            "    After Stopword Removal: ['fixing', 'cracks', 'relieve', 'pressure', 'beneath', 'home', '.']\n",
            "    After Regex: ['fixing', 'cracks', 'relieve', 'pressure', 'beneath', 'home']\n",
            "    After Lemmatization: ['fixing', 'crack', 'relieve', 'pressure', 'beneath', 'home']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9715\n",
            "    After Stopword Removal: ['individuals', 'prepare', 'areas', 'specialty', 'taking', 'classes', 'support', 'college', 'studies', '.']\n",
            "    After Regex: ['individuals', 'prepare', 'areas', 'specialty', 'taking', 'classes', 'support', 'college', 'studies']\n",
            "    After Lemmatization: ['individual', 'prepare', 'area', 'specialty', 'taking', 'class', 'support', 'college', 'study']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9716\n",
            "    After Stopword Removal: ['different', 'therapies', 'may', 'also', 'used', 'combination', '.']\n",
            "    After Regex: ['different', 'therapies', 'may', 'also', 'used', 'combination']\n",
            "    After Lemmatization: ['different', 'therapy', 'may', 'also', 'used', 'combination']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9717\n",
            "    After Stopword Removal: ['war', 'iraq', 'end', 'road', '.']\n",
            "    After Regex: ['war', 'iraq', 'end', 'road']\n",
            "    After Lemmatization: ['war', 'iraq', 'end', 'road']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9718\n",
            "    After Stopword Removal: ['rural', 'factors', 'impact', 'rural', 'battered', 'woman', \"'\", 'isolation', 'changes', 'safe', 'shelter', '.']\n",
            "    After Regex: ['rural', 'factors', 'impact', 'rural', 'battered', 'woman', 'isolation', 'changes', 'safe', 'shelter']\n",
            "    After Lemmatization: ['rural', 'factor', 'impact', 'rural', 'battered', 'woman', 'isolation', 'change', 'safe', 'shelter']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9719\n",
            "    After Stopword Removal: ['comparing', 'manifests', 'different', 'systems', ',', 'determine', 'systems', 'installed', 'identically', 'upgraded', 'synch', '.']\n",
            "    After Regex: ['comparing', 'manifests', 'different', 'systems', 'determine', 'systems', 'installed', 'identically', 'upgraded', 'synch']\n",
            "    After Lemmatization: ['comparing', 'manifest', 'different', 'system', 'determine', 'system', 'installed', 'identically', 'upgraded', 'synch']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9720\n",
            "    After Stopword Removal: ['unlike', 'amniote', 'ifn', 'genes', ',', 'piscine', 'type', 'ifn', 'genes', 'contain', 'introns', ',', 'similar', 'positions', 'orthologs', ',', 'certain', 'interleukins', '.']\n",
            "    After Regex: ['unlike', 'amniote', 'ifn', 'genes', 'piscine', 'type', 'ifn', 'genes', 'contain', 'introns', 'similar', 'positions', 'orthologs', 'certain', 'interleukins']\n",
            "    After Lemmatization: ['unlike', 'amniote', 'ifn', 'gene', 'piscine', 'type', 'ifn', 'gene', 'contain', 'intron', 'similar', 'position', 'orthologs', 'certain', 'interleukin']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9721\n",
            "    After Stopword Removal: ['founded', '2002,', 'solaria', 'energía', 'medio', 'ambiente', '(', 'hereafter', 'solaria', '),', 'dedicated', 'renewables', 'specifically', 'solar', 'photovoltaic', '(', 'hereafter', 'pv', ')', 'power', 'generation', '.']\n",
            "    After Regex: ['founded', 'solaria', 'energa', 'medio', 'ambiente', 'hereafter', 'solaria', 'dedicated', 'renewables', 'specifically', 'solar', 'photovoltaic', 'hereafter', 'pv', 'power', 'generation']\n",
            "    After Lemmatization: ['founded', 'solarium', 'energa', 'medio', 'ambiente', 'hereafter', 'solarium', 'dedicated', 'renewables', 'specifically', 'solar', 'photovoltaic', 'hereafter', 'pv', 'power', 'generation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9722\n",
            "    After Stopword Removal: ['bending', 'process', ',', 'outer', 'layer', 'subjected', 'tensile', 'stress', 'inner', 'layer', 'subjected', 'compressive', 'stress', '.']\n",
            "    After Regex: ['bending', 'process', 'outer', 'layer', 'subjected', 'tensile', 'stress', 'inner', 'layer', 'subjected', 'compressive', 'stress']\n",
            "    After Lemmatization: ['bending', 'process', 'outer', 'layer', 'subjected', 'tensile', 'stress', 'inner', 'layer', 'subjected', 'compressive', 'stress']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9723\n",
            "    After Stopword Removal: ['density', 'stabilizes', 'mean', '1.09,', 'less', 'compacted', 'recent', 'sediments', 'less', 'dense', '.']\n",
            "    After Regex: ['density', 'stabilizes', 'mean', 'less', 'compacted', 'recent', 'sediments', 'less', 'dense']\n",
            "    After Lemmatization: ['density', 'stabilizes', 'mean', 'less', 'compacted', 'recent', 'sediment', 'less', 'dense']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9724\n",
            "    After Stopword Removal: ['straightforward', 'construction', ',', 'means', 'roll', 'weld', 'techniques', '.']\n",
            "    After Regex: ['straightforward', 'construction', 'means', 'roll', 'weld', 'techniques']\n",
            "    After Lemmatization: ['straightforward', 'construction', 'mean', 'roll', 'weld', 'technique']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9725\n",
            "    After Stopword Removal: ['magnesium', 'test', 'done', 'assess', 'magnesium', 'levels', 'blood', '.']\n",
            "    After Regex: ['magnesium', 'test', 'done', 'assess', 'magnesium', 'levels', 'blood']\n",
            "    After Lemmatization: ['magnesium', 'test', 'done', 'assess', 'magnesium', 'level', 'blood']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9726\n",
            "    After Stopword Removal: ['manufacturers', 'able', 'improve', 'keyboard', 'controllers', 'offer', 'new', 'means', 'musical', 'expression', 'simple', 'player', '-', 'friendly', 'way', ',', 'general', 'interest', 'follow', '.']\n",
            "    After Regex: ['manufacturers', 'able', 'improve', 'keyboard', 'controllers', 'offer', 'new', 'means', 'musical', 'expression', 'simple', 'player', 'friendly', 'way', 'general', 'interest', 'follow']\n",
            "    After Lemmatization: ['manufacturer', 'able', 'improve', 'keyboard', 'controller', 'offer', 'new', 'mean', 'musical', 'expression', 'simple', 'player', 'friendly', 'way', 'general', 'interest', 'follow']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9727\n",
            "    After Stopword Removal: ['halloran', 'bp', ',', 'deluca', 'hf', ',1980', 'effect', 'vitamin', 'deficiency', 'fertility', 'reproductive', 'capacity', 'female', 'rat', '.']\n",
            "    After Regex: ['halloran', 'bp', 'deluca', 'hf', 'effect', 'vitamin', 'deficiency', 'fertility', 'reproductive', 'capacity', 'female', 'rat']\n",
            "    After Lemmatization: ['halloran', 'bp', 'deluca', 'hf', 'effect', 'vitamin', 'deficiency', 'fertility', 'reproductive', 'capacity', 'female', 'rat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9728\n",
            "    After Stopword Removal: ['technology', 'adopted', 'chinese', 'medical', 'researchers', 'national', 'rehabilitation', 'aids', 'research', 'beijing', 'treat', 'patients', 'suffering', 'mild', 'moderate', 'scoliosis', '.']\n",
            "    After Regex: ['technology', 'adopted', 'chinese', 'medical', 'researchers', 'national', 'rehabilitation', 'aids', 'research', 'beijing', 'treat', 'patients', 'suffering', 'mild', 'moderate', 'scoliosis']\n",
            "    After Lemmatization: ['technology', 'adopted', 'chinese', 'medical', 'researcher', 'national', 'rehabilitation', 'aid', 'research', 'beijing', 'treat', 'patient', 'suffering', 'mild', 'moderate', 'scoliosis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9729\n",
            "    After Stopword Removal: ['health', 'plan', 'primarily', 'furnishes', 'medical', 'care', 'hospitals', 'clinics', 'administered', 'department', 'veterans', 'affairs', '(', 'va', ')', 'veterans', 'enrolled', 'va', 'health', 'care', 'system', '.']\n",
            "    After Regex: ['health', 'plan', 'primarily', 'furnishes', 'medical', 'care', 'hospitals', 'clinics', 'administered', 'department', 'veterans', 'affairs', 'va', 'veterans', 'enrolled', 'va', 'health', 'care', 'system']\n",
            "    After Lemmatization: ['health', 'plan', 'primarily', 'furnishes', 'medical', 'care', 'hospital', 'clinic', 'administered', 'department', 'veteran', 'affair', 'va', 'veteran', 'enrolled', 'va', 'health', 'care', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9730\n",
            "    After Stopword Removal: ['clayton', ',', 'david', '.', 'vinograd', ',', 'jerome', '(1969)', 'complex', 'mitochondrial', 'dna', 'leukemic', 'normal', 'human', 'myeloid', 'cells', '.']\n",
            "    After Regex: ['clayton', 'david', 'vinograd', 'jerome', 'complex', 'mitochondrial', 'dna', 'leukemic', 'normal', 'human', 'myeloid', 'cells']\n",
            "    After Lemmatization: ['clayton', 'david', 'vinograd', 'jerome', 'complex', 'mitochondrial', 'dna', 'leukemic', 'normal', 'human', 'myeloid', 'cell']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9731\n",
            "    After Stopword Removal: ['soldering', 'alloy', 'tin', 'lead', ',', 'used', 'fusing', 'metals', 'relatively', 'low', 'temperature', '260', 'uk', '315', 'uk', '.', 'joint', 'two', 'metal', 'conductors', 'fused', 'heated', 'solder', 'applied', 'melt', 'cover', 'connection', '.']\n",
            "    After Regex: ['soldering', 'alloy', 'tin', 'lead', 'used', 'fusing', 'metals', 'relatively', 'low', 'temperature', 'uk', 'uk', 'joint', 'two', 'metal', 'conductors', 'fused', 'heated', 'solder', 'applied', 'melt', 'cover', 'connection']\n",
            "    After Lemmatization: ['soldering', 'alloy', 'tin', 'lead', 'used', 'fusing', 'metal', 'relatively', 'low', 'temperature', 'uk', 'uk', 'joint', 'two', 'metal', 'conductor', 'fused', 'heated', 'solder', 'applied', 'melt', 'cover', 'connection']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9732\n",
            "    After Stopword Removal: ['wrong', 'assume', 'prices', 'goods', 'equal', 'countries', '.']\n",
            "    After Regex: ['wrong', 'assume', 'prices', 'goods', 'equal', 'countries']\n",
            "    After Lemmatization: ['wrong', 'assume', 'price', 'good', 'equal', 'country']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9733\n",
            "    After Stopword Removal: ['without', 'false', 'alerts', ',', 'users', 'likely', 'keep', 'security', 'alerts', 'enabled', ',', 'keeping', '-', '-', 'date', 'home', 'enhancing', 'safety', '.']\n",
            "    After Regex: ['without', 'false', 'alerts', 'users', 'likely', 'keep', 'security', 'alerts', 'enabled', 'keeping', 'date', 'home', 'enhancing', 'safety']\n",
            "    After Lemmatization: ['without', 'false', 'alert', 'user', 'likely', 'keep', 'security', 'alert', 'enabled', 'keeping', 'date', 'home', 'enhancing', 'safety']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9734\n",
            "    After Stopword Removal: ['perc', 'issue', 'invalidation', '–', 'invalidation', 'occurs', 'root', 'cause', 'object', 'within', 'issue', 'longer', 'present', '.']\n",
            "    After Regex: ['perc', 'issue', 'invalidation', 'invalidation', 'occurs', 'root', 'cause', 'object', 'within', 'issue', 'longer', 'present']\n",
            "    After Lemmatization: ['perc', 'issue', 'invalidation', 'invalidation', 'occurs', 'root', 'cause', 'object', 'within', 'issue', 'longer', 'present']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9735\n",
            "    After Stopword Removal: ['general', 'outrage', 'often', 'reaction', 'modern', 'archaeologists', 'scholars', 'discussing', '.']\n",
            "    After Regex: ['general', 'outrage', 'often', 'reaction', 'modern', 'archaeologists', 'scholars', 'discussing']\n",
            "    After Lemmatization: ['general', 'outrage', 'often', 'reaction', 'modern', 'archaeologist', 'scholar', 'discussing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9736\n",
            "    After Stopword Removal: ['find', 'full', 'document', 'location', ':', 'additional', 'reading', 'period', 'utah', 'history', 'bushman', ',', 'claudia', 'l', '.,', 'ed', '.,', 'mormon', 'sisters', ':', 'women', 'early', 'utah', '.']\n",
            "    After Regex: ['find', 'full', 'document', 'location', 'additional', 'reading', 'period', 'utah', 'history', 'bushman', 'claudia', 'l', 'ed', 'mormon', 'sisters', 'women', 'early', 'utah']\n",
            "    After Lemmatization: ['find', 'full', 'document', 'location', 'additional', 'reading', 'period', 'utah', 'history', 'bushman', 'claudia', 'l', 'ed', 'mormon', 'sister', 'woman', 'early', 'utah']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9737\n",
            "    After Stopword Removal: ['blend', 'sand', 'often', 'mixed', 'cement', 'water', 'lay', 'paving', 'slabs', 'create', 'floor', 'screed', '.']\n",
            "    After Regex: ['blend', 'sand', 'often', 'mixed', 'cement', 'water', 'lay', 'paving', 'slabs', 'create', 'floor', 'screed']\n",
            "    After Lemmatization: ['blend', 'sand', 'often', 'mixed', 'cement', 'water', 'lay', 'paving', 'slab', 'create', 'floor', 'screed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9738\n",
            "    After Stopword Removal: ['put', 'behind', 'supporting', 'epa', 'restriction', 'carbon', 'emissions', '.']\n",
            "    After Regex: ['put', 'behind', 'supporting', 'epa', 'restriction', 'carbon', 'emissions']\n",
            "    After Lemmatization: ['put', 'behind', 'supporting', 'epa', 'restriction', 'carbon', 'emission']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9739\n",
            "    After Stopword Removal: ['anxiety', 'disorders', 'often', 'see', 'long', 'list', 'physicians', 'finally', 'realizing', 'problem', 'anxiety', ',', 'find', 'therapist', 'help', '.']\n",
            "    After Regex: ['anxiety', 'disorders', 'often', 'see', 'long', 'list', 'physicians', 'finally', 'realizing', 'problem', 'anxiety', 'find', 'therapist', 'help']\n",
            "    After Lemmatization: ['anxiety', 'disorder', 'often', 'see', 'long', 'list', 'physician', 'finally', 'realizing', 'problem', 'anxiety', 'find', 'therapist', 'help']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9740\n",
            "    After Stopword Removal: ['large', 'randomized', 'controlled', 'trials', 'least', '6', 'months', 'duration', 'carbohydrate', 'restriction', 'appear', 'superior', 'improving', 'lipid', 'markers', 'compared', 'low', '-', 'fat', 'diets', '.']\n",
            "    After Regex: ['large', 'randomized', 'controlled', 'trials', 'least', 'months', 'duration', 'carbohydrate', 'restriction', 'appear', 'superior', 'improving', 'lipid', 'markers', 'compared', 'low', 'fat', 'diets']\n",
            "    After Lemmatization: ['large', 'randomized', 'controlled', 'trial', 'least', 'month', 'duration', 'carbohydrate', 'restriction', 'appear', 'superior', 'improving', 'lipid', 'marker', 'compared', 'low', 'fat', 'diet']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9741\n",
            "    After Stopword Removal: ['based', 'ability', 'detect', 'track', 'multiple', 'moving', 'objects', 'across', 'camera', ',', 'classify', 'objects', ',', 'extract', 'various', 'object', 'attributes', 'like', 'color', ',', 'shape', 'size', '.']\n",
            "    After Regex: ['based', 'ability', 'detect', 'track', 'multiple', 'moving', 'objects', 'across', 'camera', 'classify', 'objects', 'extract', 'various', 'object', 'attributes', 'like', 'color', 'shape', 'size']\n",
            "    After Lemmatization: ['based', 'ability', 'detect', 'track', 'multiple', 'moving', 'object', 'across', 'camera', 'classify', 'object', 'extract', 'various', 'object', 'attribute', 'like', 'color', 'shape', 'size']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9742\n",
            "    After Stopword Removal: ['high', 'school', 'football', 'flying', '?']\n",
            "    After Regex: ['high', 'school', 'football', 'flying']\n",
            "    After Lemmatization: ['high', 'school', 'football', 'flying']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9743\n",
            "    After Stopword Removal: ['summer', 'exposure', 'elements', ',', 'parched', 'hair', 'requires', 'hydration', 'infusion', '.']\n",
            "    After Regex: ['summer', 'exposure', 'elements', 'parched', 'hair', 'requires', 'hydration', 'infusion']\n",
            "    After Lemmatization: ['summer', 'exposure', 'element', 'parched', 'hair', 'requires', 'hydration', 'infusion']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9744\n",
            "    After Stopword Removal: ['yet', 'know', '70%', 'kids', 'quit', 'sports', 'teens', 'citing', 'number', 'one', 'reason', ':', 'longer', 'fun', '.']\n",
            "    After Regex: ['yet', 'know', 'kids', 'quit', 'sports', 'teens', 'citing', 'number', 'one', 'reason', 'longer', 'fun']\n",
            "    After Lemmatization: ['yet', 'know', 'kid', 'quit', 'sport', 'teen', 'citing', 'number', 'one', 'reason', 'longer', 'fun']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9745\n",
            "    After Stopword Removal: ['online', 'resources', 'extend', 'capabilities', 'collaboration', 'across', 'distance', ',', 'new', 'ways', 'cooperating', 'emerging', 'work', 'play', '.']\n",
            "    After Regex: ['online', 'resources', 'extend', 'capabilities', 'collaboration', 'across', 'distance', 'new', 'ways', 'cooperating', 'emerging', 'work', 'play']\n",
            "    After Lemmatization: ['online', 'resource', 'extend', 'capability', 'collaboration', 'across', 'distance', 'new', 'way', 'cooperating', 'emerging', 'work', 'play']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9746\n",
            "    After Stopword Removal: ['currently', 'management', 'prostate', 'cancer', 'whole', 'prostate', 'extracapsular', 'extension', 'treated', 'though', 'mostly', 'cancer', 'confined', 'peripheral', 'zones', '.']\n",
            "    After Regex: ['currently', 'management', 'prostate', 'cancer', 'whole', 'prostate', 'extracapsular', 'extension', 'treated', 'though', 'mostly', 'cancer', 'confined', 'peripheral', 'zones']\n",
            "    After Lemmatization: ['currently', 'management', 'prostate', 'cancer', 'whole', 'prostate', 'extracapsular', 'extension', 'treated', 'though', 'mostly', 'cancer', 'confined', 'peripheral', 'zone']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9747\n",
            "    After Stopword Removal: ['restos', 'recordatorios', 'de', 'lo', 'divino', '.']\n",
            "    After Regex: ['restos', 'recordatorios', 'de', 'lo', 'divino']\n",
            "    After Lemmatization: ['restos', 'recordatorios', 'de', 'lo', 'divino']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9748\n",
            "    After Stopword Removal: ['consequently', ',', 'makes', 'cities', 'towns', 'overly', 'dependent', 'local', 'sales', 'tax', 'fund', 'operations', ',', 'including', 'police', ',', 'fire', ',', 'libraries', ',', 'parks', ',', 'streets', ',', 'drinking', 'wastewater', 'needs', 'myriad', 'services', '.']\n",
            "    After Regex: ['consequently', 'makes', 'cities', 'towns', 'overly', 'dependent', 'local', 'sales', 'tax', 'fund', 'operations', 'including', 'police', 'fire', 'libraries', 'parks', 'streets', 'drinking', 'wastewater', 'needs', 'myriad', 'services']\n",
            "    After Lemmatization: ['consequently', 'make', 'city', 'town', 'overly', 'dependent', 'local', 'sale', 'tax', 'fund', 'operation', 'including', 'police', 'fire', 'library', 'park', 'street', 'drinking', 'wastewater', 'need', 'myriad', 'service']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9749\n",
            "    After Stopword Removal: ['important', 'support', 'body', \"'\", 'natural', 'healing', 'processes', '.']\n",
            "    After Regex: ['important', 'support', 'body', 'natural', 'healing', 'processes']\n",
            "    After Lemmatization: ['important', 'support', 'body', 'natural', 'healing', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9750\n",
            "    After Stopword Removal: ['terminal', 'part', 'human', 'arm', 'located', 'forearm', ',', 'used', 'grasping', 'holding', 'consisting', 'wrist', ',', 'palm', ',', 'four', 'fingers', ',', 'opposable', 'thumb', '.']\n",
            "    After Regex: ['terminal', 'part', 'human', 'arm', 'located', 'forearm', 'used', 'grasping', 'holding', 'consisting', 'wrist', 'palm', 'four', 'fingers', 'opposable', 'thumb']\n",
            "    After Lemmatization: ['terminal', 'part', 'human', 'arm', 'located', 'forearm', 'used', 'grasping', 'holding', 'consisting', 'wrist', 'palm', 'four', 'finger', 'opposable', 'thumb']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9751\n",
            "    After Stopword Removal: ['consider', ',', 'instance', ',', 'one', 'alternative', 'myth', 'comes', 'highly', 'recommended', 'jesus', ',', 'buddha', ',', 'mohammad', ',', 'abraham', ',', 'rama', 'good', 'old', '-', 'fashioned', 'common', 'sense', ':', 'cooperation', 'lodestone', 'human', 'society', '—', 'huge', 'individual', 'profit', 'cost', 'everything', 'else', '.']\n",
            "    After Regex: ['consider', 'instance', 'one', 'alternative', 'myth', 'comes', 'highly', 'recommended', 'jesus', 'buddha', 'mohammad', 'abraham', 'rama', 'good', 'old', 'fashioned', 'common', 'sense', 'cooperation', 'lodestone', 'human', 'society', 'huge', 'individual', 'profit', 'cost', 'everything', 'else']\n",
            "    After Lemmatization: ['consider', 'instance', 'one', 'alternative', 'myth', 'come', 'highly', 'recommended', 'jesus', 'buddha', 'mohammad', 'abraham', 'rama', 'good', 'old', 'fashioned', 'common', 'sense', 'cooperation', 'lodestone', 'human', 'society', 'huge', 'individual', 'profit', 'cost', 'everything', 'else']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9752\n",
            "    After Stopword Removal: ['increase', 'liquor', 'purchases', 'could', 'submitted', 'evidence', 'individual', 'may', 'abusive', 'tendencies', 'regarding', 'alcohol', ',', 'would', 'best', 'interest', 'child', '.']\n",
            "    After Regex: ['increase', 'liquor', 'purchases', 'could', 'submitted', 'evidence', 'individual', 'may', 'abusive', 'tendencies', 'regarding', 'alcohol', 'would', 'best', 'interest', 'child']\n",
            "    After Lemmatization: ['increase', 'liquor', 'purchase', 'could', 'submitted', 'evidence', 'individual', 'may', 'abusive', 'tendency', 'regarding', 'alcohol', 'would', 'best', 'interest', 'child']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9753\n",
            "    After Stopword Removal: ['fridays', 'busiest', 'days', ',', 'largely', 'people', 'make', 'social', 'recreational', 'trips', 'addition', 'regular', 'work', 'school', 'trips', '.']\n",
            "    After Regex: ['fridays', 'busiest', 'days', 'largely', 'people', 'make', 'social', 'recreational', 'trips', 'addition', 'regular', 'work', 'school', 'trips']\n",
            "    After Lemmatization: ['friday', 'busiest', 'day', 'largely', 'people', 'make', 'social', 'recreational', 'trip', 'addition', 'regular', 'work', 'school', 'trip']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9754\n",
            "    After Stopword Removal: ['mining', 'graph', 'databases', 'molecular', 'data', 'sets', 'computer', '-', 'aided', 'analysis', 'molecular', 'databases', 'plays', 'increasingly', 'important', 'role', 'drug', 'discovery', 'well', 'compound', 'synthesis', 'prediction', '.']\n",
            "    After Regex: ['mining', 'graph', 'databases', 'molecular', 'data', 'sets', 'computer', 'aided', 'analysis', 'molecular', 'databases', 'plays', 'increasingly', 'important', 'role', 'drug', 'discovery', 'well', 'compound', 'synthesis', 'prediction']\n",
            "    After Lemmatization: ['mining', 'graph', 'database', 'molecular', 'data', 'set', 'computer', 'aided', 'analysis', 'molecular', 'database', 'play', 'increasingly', 'important', 'role', 'drug', 'discovery', 'well', 'compound', 'synthesis', 'prediction']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9755\n",
            "    After Stopword Removal: ['western', 'front', 'first', 'naval', 'air', 'raid', 'occurred', 'december', '25,1914', 'twelve', 'seaplanes', 'hms', 'engadine', ',', 'riviera', 'empress', '(', 'cross', '-', 'channel', 'steamers', 'converted', 'seaplane', 'carriers', ')', 'attacked', 'zeppelin', 'base', 'cuxhaven', '.']\n",
            "    After Regex: ['western', 'front', 'first', 'naval', 'air', 'raid', 'occurred', 'december', 'twelve', 'seaplanes', 'hms', 'engadine', 'riviera', 'empress', 'cross', 'channel', 'steamers', 'converted', 'seaplane', 'carriers', 'attacked', 'zeppelin', 'base', 'cuxhaven']\n",
            "    After Lemmatization: ['western', 'front', 'first', 'naval', 'air', 'raid', 'occurred', 'december', 'twelve', 'seaplane', 'hm', 'engadine', 'riviera', 'empress', 'cross', 'channel', 'steamer', 'converted', 'seaplane', 'carrier', 'attacked', 'zeppelin', 'base', 'cuxhaven']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9756\n",
            "    After Stopword Removal: ['rats', 'study', 'administered', 'cbd', 'thc', 'experienced', 'increased', 'callous', 'formation', '(', 'size', ')', 'rats', 'receive', 'cannabis', 'experience', 'increase', '.']\n",
            "    After Regex: ['rats', 'study', 'administered', 'cbd', 'thc', 'experienced', 'increased', 'callous', 'formation', 'size', 'rats', 'receive', 'cannabis', 'experience', 'increase']\n",
            "    After Lemmatization: ['rat', 'study', 'administered', 'cbd', 'thc', 'experienced', 'increased', 'callous', 'formation', 'size', 'rat', 'receive', 'cannabis', 'experience', 'increase']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9757\n",
            "    After Stopword Removal: ['technical', 'side', ',', 'passwords', 'hashed', 'using', 'sha', '-1', 'salt', '.']\n",
            "    After Regex: ['technical', 'side', 'passwords', 'hashed', 'using', 'sha', 'salt']\n",
            "    After Lemmatization: ['technical', 'side', 'password', 'hashed', 'using', 'sha', 'salt']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9758\n",
            "    After Stopword Removal: ['typical', 'global', 'integrity', 'assessments', 'provide', 'detailed', 'data', 'reporting', 'mechanisms', 'place', 'prevent', 'abuses', 'power', 'promote', 'public', 'integrity', 'national', ',', 'sub', '-', 'national', 'sector', 'levels', ',', 'u', '.', '.', 'abroad', '.']\n",
            "    After Regex: ['typical', 'global', 'integrity', 'assessments', 'provide', 'detailed', 'data', 'reporting', 'mechanisms', 'place', 'prevent', 'abuses', 'power', 'promote', 'public', 'integrity', 'national', 'sub', 'national', 'sector', 'levels', 'u', 'abroad']\n",
            "    After Lemmatization: ['typical', 'global', 'integrity', 'assessment', 'provide', 'detailed', 'data', 'reporting', 'mechanism', 'place', 'prevent', 'abuse', 'power', 'promote', 'public', 'integrity', 'national', 'sub', 'national', 'sector', 'level', 'u', 'abroad']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9759\n",
            "    After Stopword Removal: ['help', 'generate', 'ideas', ',', ',', 'improvements', 'made', '.']\n",
            "    After Regex: ['help', 'generate', 'ideas', 'improvements', 'made']\n",
            "    After Lemmatization: ['help', 'generate', 'idea', 'improvement', 'made']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9760\n",
            "    After Stopword Removal: ['book', 'formed', 'foundational', 'values', 'every', 'major', 'american', 'university', '.']\n",
            "    After Regex: ['book', 'formed', 'foundational', 'values', 'every', 'major', 'american', 'university']\n",
            "    After Lemmatization: ['book', 'formed', 'foundational', 'value', 'every', 'major', 'american', 'university']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9761\n",
            "    After Stopword Removal: ['congruence', 'studies', 'neotropical', 'forests', ',', 'seems', 'also', 'anthropogenic', 'disturbances', 'type', 'forest', 'structure', 'affect', 'occurrence', 'myxomycetes', 'philippines', '.']\n",
            "    After Regex: ['congruence', 'studies', 'neotropical', 'forests', 'seems', 'also', 'anthropogenic', 'disturbances', 'type', 'forest', 'structure', 'affect', 'occurrence', 'myxomycetes', 'philippines']\n",
            "    After Lemmatization: ['congruence', 'study', 'neotropical', 'forest', 'seems', 'also', 'anthropogenic', 'disturbance', 'type', 'forest', 'structure', 'affect', 'occurrence', 'myxomycete', 'philippine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9762\n",
            "    After Stopword Removal: ['exact', 'identifiable', 'cause', 'sad', 'found', '.']\n",
            "    After Regex: ['exact', 'identifiable', 'cause', 'sad', 'found']\n",
            "    After Lemmatization: ['exact', 'identifiable', 'cause', 'sad', 'found']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9763\n",
            "    After Stopword Removal: ['group', 'volunteer', 'workdays', 'consist', 'individual', 'seasonal', 'gardening', 'tasks', 'larger', 'projects', 'require', 'teamwork', '.']\n",
            "    After Regex: ['group', 'volunteer', 'workdays', 'consist', 'individual', 'seasonal', 'gardening', 'tasks', 'larger', 'projects', 'require', 'teamwork']\n",
            "    After Lemmatization: ['group', 'volunteer', 'workday', 'consist', 'individual', 'seasonal', 'gardening', 'task', 'larger', 'project', 'require', 'teamwork']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9764\n",
            "    After Stopword Removal: ['32,000', 'msu', 'students', 'participated', 'community', '-', 'engaged', 'learning', 'service', 'opportunities', '2016–17', 'academic', 'year', '.']\n",
            "    After Regex: ['msu', 'students', 'participated', 'community', 'engaged', 'learning', 'service', 'opportunities', 'academic', 'year']\n",
            "    After Lemmatization: ['msu', 'student', 'participated', 'community', 'engaged', 'learning', 'service', 'opportunity', 'academic', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9765\n",
            "    After Stopword Removal: ['fellows', 'may', 'come', 'educational', 'institutions', ',', 'direct', 'college', 'graduate', 'school', ',', 'non', '-', 'academic', 'settings', '.']\n",
            "    After Regex: ['fellows', 'may', 'come', 'educational', 'institutions', 'direct', 'college', 'graduate', 'school', 'non', 'academic', 'settings']\n",
            "    After Lemmatization: ['fellow', 'may', 'come', 'educational', 'institution', 'direct', 'college', 'graduate', 'school', 'non', 'academic', 'setting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9766\n",
            "    After Stopword Removal: ['postsecondary', 'institutions', 'may', 'disclose', 'alleged', 'victim', 'crime', 'violence', '(', 'defined', 'u', '.', '.', 'code', 'title', '18,§16)', 'results', 'disciplinary', 'proceeding', 'conducted', 'institution', 'alleged', 'perpetrator', 'crime', ',', 'regardless', 'outcome', 'proceeding', '.']\n",
            "    After Regex: ['postsecondary', 'institutions', 'may', 'disclose', 'alleged', 'victim', 'crime', 'violence', 'defined', 'u', 'code', 'title', 'results', 'disciplinary', 'proceeding', 'conducted', 'institution', 'alleged', 'perpetrator', 'crime', 'regardless', 'outcome', 'proceeding']\n",
            "    After Lemmatization: ['postsecondary', 'institution', 'may', 'disclose', 'alleged', 'victim', 'crime', 'violence', 'defined', 'u', 'code', 'title', 'result', 'disciplinary', 'proceeding', 'conducted', 'institution', 'alleged', 'perpetrator', 'crime', 'regardless', 'outcome', 'proceeding']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9767\n",
            "    After Stopword Removal: ['slobol', 'best', 'known', 'speed', '(', 'lack', 'thereof', ')', 'compiler', '.']\n",
            "    After Regex: ['slobol', 'best', 'known', 'speed', 'lack', 'thereof', 'compiler']\n",
            "    After Lemmatization: ['slobol', 'best', 'known', 'speed', 'lack', 'thereof', 'compiler']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9768\n",
            "    After Stopword Removal: ['grow', 'crops', 'ecologically', 'shadow', '-', 'free', 'plots', 'region', ',', 'makes', 'perfect', 'basis', 'pure', 'cannabis', 'oil', '.']\n",
            "    After Regex: ['grow', 'crops', 'ecologically', 'shadow', 'free', 'plots', 'region', 'makes', 'perfect', 'basis', 'pure', 'cannabis', 'oil']\n",
            "    After Lemmatization: ['grow', 'crop', 'ecologically', 'shadow', 'free', 'plot', 'region', 'make', 'perfect', 'basis', 'pure', 'cannabis', 'oil']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9769\n",
            "    After Stopword Removal: ['united', 'nations', 'saddam', 'hussein', \"'\", 'government', 'published', 'high', 'estimates', 'civilian', 'deaths', '.']\n",
            "    After Regex: ['united', 'nations', 'saddam', 'hussein', 'government', 'published', 'high', 'estimates', 'civilian', 'deaths']\n",
            "    After Lemmatization: ['united', 'nation', 'saddam', 'hussein', 'government', 'published', 'high', 'estimate', 'civilian', 'death']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9770\n",
            "    After Stopword Removal: ['dolichols', '19,22', '23', 'isoprenoid', 'units', 'described', 'early', '1972', 'marine', 'invertebrates', '(', 'walton', 'mj', 'et', 'al', '.,', 'biochem', 'j', '1972,127,471).']\n",
            "    After Regex: ['dolichols', 'isoprenoid', 'units', 'described', 'early', 'marine', 'invertebrates', 'walton', 'mj', 'et', 'al', 'biochem', 'j']\n",
            "    After Lemmatization: ['dolichols', 'isoprenoid', 'unit', 'described', 'early', 'marine', 'invertebrate', 'walton', 'mj', 'et', 'al', 'biochem', 'j']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9771\n",
            "    After Stopword Removal: ['waters', 'used', 'sailing', 'events', '2012', 'olympic', 'games', '.']\n",
            "    After Regex: ['waters', 'used', 'sailing', 'events', 'olympic', 'games']\n",
            "    After Lemmatization: ['water', 'used', 'sailing', 'event', 'olympic', 'game']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9772\n",
            "    After Stopword Removal: ['fitness', 'essential', 'component', 'self', '-', 'care', ',', 'like', 'mindfulness', 'balanced', 'nutrition', '.']\n",
            "    After Regex: ['fitness', 'essential', 'component', 'self', 'care', 'like', 'mindfulness', 'balanced', 'nutrition']\n",
            "    After Lemmatization: ['fitness', 'essential', 'component', 'self', 'care', 'like', 'mindfulness', 'balanced', 'nutrition']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9773\n",
            "    After Stopword Removal: ['exercise', 'stressful', 'gastrointestinal', 'system', '.']\n",
            "    After Regex: ['exercise', 'stressful', 'gastrointestinal', 'system']\n",
            "    After Lemmatization: ['exercise', 'stressful', 'gastrointestinal', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9774\n",
            "    After Stopword Removal: ['miltefosine', 'toxic', 'effects', 'reproductive', 'capacity', 'female', 'animals', ',', 'pregnancy', 'strictly', 'avoided', 'drug', 'two', 'months', 'completion', 'therapy', '.']\n",
            "    After Regex: ['miltefosine', 'toxic', 'effects', 'reproductive', 'capacity', 'female', 'animals', 'pregnancy', 'strictly', 'avoided', 'drug', 'two', 'months', 'completion', 'therapy']\n",
            "    After Lemmatization: ['miltefosine', 'toxic', 'effect', 'reproductive', 'capacity', 'female', 'animal', 'pregnancy', 'strictly', 'avoided', 'drug', 'two', 'month', 'completion', 'therapy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9775\n",
            "    After Stopword Removal: ['one', 'party', 'control', 'census', ',', 'could', 'arguably', 'try', 'perpetuate', 'hold', 'political', 'power', '.']\n",
            "    After Regex: ['one', 'party', 'control', 'census', 'could', 'arguably', 'try', 'perpetuate', 'hold', 'political', 'power']\n",
            "    After Lemmatization: ['one', 'party', 'control', 'census', 'could', 'arguably', 'try', 'perpetuate', 'hold', 'political', 'power']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9776\n",
            "    After Stopword Removal: ['tripp', 'says', 'policies', 'detrimental', 'tribal', 'traditions', 'forest', 'environment', '.']\n",
            "    After Regex: ['tripp', 'says', 'policies', 'detrimental', 'tribal', 'traditions', 'forest', 'environment']\n",
            "    After Lemmatization: ['tripp', 'say', 'policy', 'detrimental', 'tribal', 'tradition', 'forest', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9777\n",
            "    After Stopword Removal: ['implies', 'phases', 'heterogeneously', 'charged', 'colloids', 'liquid', '-', 'like', 'respect', 'translational', 'degrees', 'freedom', 'also', 'isotropic', 'respect', 'particle', 'orientation', '.']\n",
            "    After Regex: ['implies', 'phases', 'heterogeneously', 'charged', 'colloids', 'liquid', 'like', 'respect', 'translational', 'degrees', 'freedom', 'also', 'isotropic', 'respect', 'particle', 'orientation']\n",
            "    After Lemmatization: ['implies', 'phase', 'heterogeneously', 'charged', 'colloid', 'liquid', 'like', 'respect', 'translational', 'degree', 'freedom', 'also', 'isotropic', 'respect', 'particle', 'orientation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9778\n",
            "    After Stopword Removal: ['court', 'permitted', ',', 'require', ',', 'identification', 'relevant', 'scientific', 'community', 'determination', 'degree', 'acceptance', 'within', 'community', '.']\n",
            "    After Regex: ['court', 'permitted', 'require', 'identification', 'relevant', 'scientific', 'community', 'determination', 'degree', 'acceptance', 'within', 'community']\n",
            "    After Lemmatization: ['court', 'permitted', 'require', 'identification', 'relevant', 'scientific', 'community', 'determination', 'degree', 'acceptance', 'within', 'community']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9779\n",
            "    After Stopword Removal: ['hard', 'issues', 'millet', 'deal', 'questions', 'latter', '-', 'day', 'saints', 'handle', 'whole', 'teachings', 'past', 'leaders', 'light', 'occasional', 'false', 'teachings', 'doctrines', 'leaders', '.']\n",
            "    After Regex: ['hard', 'issues', 'millet', 'deal', 'questions', 'latter', 'day', 'saints', 'handle', 'whole', 'teachings', 'past', 'leaders', 'light', 'occasional', 'false', 'teachings', 'doctrines', 'leaders']\n",
            "    After Lemmatization: ['hard', 'issue', 'millet', 'deal', 'question', 'latter', 'day', 'saint', 'handle', 'whole', 'teaching', 'past', 'leader', 'light', 'occasional', 'false', 'teaching', 'doctrine', 'leader']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9780\n",
            "    After Stopword Removal: ['generally', 'speaking', ',', 'price', 'wooden', 'fencing', 'really', 'depend', 'several', 'factors', ',', 'include', 'fence', \"'\", 'height', ',', 'design', ',', 'materials', 'used', 'conjunction', 'fencing', '(', 'capping', 'rails', '),', 'kind', 'wood', 'used', '.']\n",
            "    After Regex: ['generally', 'speaking', 'price', 'wooden', 'fencing', 'really', 'depend', 'several', 'factors', 'include', 'fence', 'height', 'design', 'materials', 'used', 'conjunction', 'fencing', 'capping', 'rails', 'kind', 'wood', 'used']\n",
            "    After Lemmatization: ['generally', 'speaking', 'price', 'wooden', 'fencing', 'really', 'depend', 'several', 'factor', 'include', 'fence', 'height', 'design', 'material', 'used', 'conjunction', 'fencing', 'capping', 'rail', 'kind', 'wood', 'used']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9781\n",
            "    After Stopword Removal: ['hebrew', 'desire', 'concrete', 'expression', 'practical', 'expression', ',', 'often', 'spoke', 'member', 'body', 'guilty', 'party', '.']\n",
            "    After Regex: ['hebrew', 'desire', 'concrete', 'expression', 'practical', 'expression', 'often', 'spoke', 'member', 'body', 'guilty', 'party']\n",
            "    After Lemmatization: ['hebrew', 'desire', 'concrete', 'expression', 'practical', 'expression', 'often', 'spoke', 'member', 'body', 'guilty', 'party']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9782\n",
            "    After Stopword Removal: ['poverty', 'deepens', 'wealth', 'increases', ',', 'wages', 'forced', 'productive', 'power', 'grows', ',', 'land', ',', 'source', 'wealth', 'field', 'labor', ',', 'monopolized', '.']\n",
            "    After Regex: ['poverty', 'deepens', 'wealth', 'increases', 'wages', 'forced', 'productive', 'power', 'grows', 'land', 'source', 'wealth', 'field', 'labor', 'monopolized']\n",
            "    After Lemmatization: ['poverty', 'deepens', 'wealth', 'increase', 'wage', 'forced', 'productive', 'power', 'grows', 'land', 'source', 'wealth', 'field', 'labor', 'monopolized']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9783\n",
            "    After Stopword Removal: ['primary', 'mirror', '85', 'centimetres', '(33', ')', 'diameter', ',', 'f', '/12', 'made', 'beryllium', 'cooled', '5.5', 'k', '(−449.77°', 'f', ').']\n",
            "    After Regex: ['primary', 'mirror', 'centimetres', 'diameter', 'f', 'made', 'beryllium', 'cooled', 'k', 'f']\n",
            "    After Lemmatization: ['primary', 'mirror', 'centimetre', 'diameter', 'f', 'made', 'beryllium', 'cooled', 'k', 'f']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9784\n",
            "    After Stopword Removal: ['inductively', 'coupled', 'plasma', 'mass', 'spectrometer', 'claimed', 'claim', '1,', 'plasma', 'argon', 'plasma', '.']\n",
            "    After Regex: ['inductively', 'coupled', 'plasma', 'mass', 'spectrometer', 'claimed', 'claim', 'plasma', 'argon', 'plasma']\n",
            "    After Lemmatization: ['inductively', 'coupled', 'plasma', 'mass', 'spectrometer', 'claimed', 'claim', 'plasma', 'argon', 'plasma']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9785\n",
            "    After Stopword Removal: ['atlantis', 'made', 'way', 'broad', 'industrial', 'avenues', ',', '-', 'limits', 'public', '.']\n",
            "    After Regex: ['atlantis', 'made', 'way', 'broad', 'industrial', 'avenues', 'limits', 'public']\n",
            "    After Lemmatization: ['atlantis', 'made', 'way', 'broad', 'industrial', 'avenue', 'limit', 'public']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9786\n",
            "    After Stopword Removal: [\"'\", 'age', 'kids', 'need', 'learn', 'behave', 'group', ',', 'line', ',', 'share', '.']\n",
            "    After Regex: ['age', 'kids', 'need', 'learn', 'behave', 'group', 'line', 'share']\n",
            "    After Lemmatization: ['age', 'kid', 'need', 'learn', 'behave', 'group', 'line', 'share']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9787\n",
            "    After Stopword Removal: ['even', 'captain', 'cook', \"'\", 'endeavour', '1788.']\n",
            "    After Regex: ['even', 'captain', 'cook', 'endeavour']\n",
            "    After Lemmatization: ['even', 'captain', 'cook', 'endeavour']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9788\n",
            "    After Stopword Removal: ['essence', 'ephesians', '2:8-10.']\n",
            "    After Regex: ['essence', 'ephesians']\n",
            "    After Lemmatization: ['essence', 'ephesian']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9789\n",
            "    After Stopword Removal: ['benefits', 'reaped', 'facilities', 'run', 'numerous', 'processes', 'simultaneously', ',', 'example', 'water', 'facilities', 'oil', 'gas', 'company', '.']\n",
            "    After Regex: ['benefits', 'reaped', 'facilities', 'run', 'numerous', 'processes', 'simultaneously', 'example', 'water', 'facilities', 'oil', 'gas', 'company']\n",
            "    After Lemmatization: ['benefit', 'reaped', 'facility', 'run', 'numerous', 'process', 'simultaneously', 'example', 'water', 'facility', 'oil', 'gas', 'company']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9790\n",
            "    After Stopword Removal: ['go', 'grief', 'counselling', '?']\n",
            "    After Regex: ['go', 'grief', 'counselling']\n",
            "    After Lemmatization: ['go', 'grief', 'counselling']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9791\n",
            "    After Stopword Removal: ['london', '(', 'uk', '):', 'national', 'institute', 'health', 'clinical', 'excellence', '(', 'nice', ').']\n",
            "    After Regex: ['london', 'uk', 'national', 'institute', 'health', 'clinical', 'excellence', 'nice']\n",
            "    After Lemmatization: ['london', 'uk', 'national', 'institute', 'health', 'clinical', 'excellence', 'nice']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9792\n",
            "    After Stopword Removal: ['single', '-', 'digit', 'interest', 'rates', 'subdued', 'inflation', ',', \"'\", 'missing', 'growth', 'materialization', 'government', \"'\", 'agenda', 'reforms', ',', 'said', '.']\n",
            "    After Regex: ['single', 'digit', 'interest', 'rates', 'subdued', 'inflation', 'missing', 'growth', 'materialization', 'government', 'agenda', 'reforms', 'said']\n",
            "    After Lemmatization: ['single', 'digit', 'interest', 'rate', 'subdued', 'inflation', 'missing', 'growth', 'materialization', 'government', 'agenda', 'reform', 'said']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9793\n",
            "    After Stopword Removal: ['meanwhile', ',', 'people', 'starving', 'information', 'health', ',', 'nutrition', 'exercise', ',', 'leading', 'cycle', 'yo', '-', 'yo', 'dieting', 'misguided', 'efforts', '.']\n",
            "    After Regex: ['meanwhile', 'people', 'starving', 'information', 'health', 'nutrition', 'exercise', 'leading', 'cycle', 'yo', 'yo', 'dieting', 'misguided', 'efforts']\n",
            "    After Lemmatization: ['meanwhile', 'people', 'starving', 'information', 'health', 'nutrition', 'exercise', 'leading', 'cycle', 'yo', 'yo', 'dieting', 'misguided', 'effort']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9794\n",
            "    After Stopword Removal: ['essentially', 'came', 'fruition', '1960', 'influence', 'freud', 'crowley', 'peak', '.']\n",
            "    After Regex: ['essentially', 'came', 'fruition', 'influence', 'freud', 'crowley', 'peak']\n",
            "    After Lemmatization: ['essentially', 'came', 'fruition', 'influence', 'freud', 'crowley', 'peak']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9795\n",
            "    After Stopword Removal: ['prognosis', 'urinary', 'incontinence', 'dependent', 'upon', 'underlying', 'cause', ',', 'however', ',', 'children', 'suffer', 'nonorganic', 'causes', 'enuresis', 'simply', 'outgrow', 'symptoms', ',', 'less', '1%', 'affected', 'children', 'persistent', 'enuresis', 'adult', 'years', '.']\n",
            "    After Regex: ['prognosis', 'urinary', 'incontinence', 'dependent', 'upon', 'underlying', 'cause', 'however', 'children', 'suffer', 'nonorganic', 'causes', 'enuresis', 'simply', 'outgrow', 'symptoms', 'less', 'affected', 'children', 'persistent', 'enuresis', 'adult', 'years']\n",
            "    After Lemmatization: ['prognosis', 'urinary', 'incontinence', 'dependent', 'upon', 'underlying', 'cause', 'however', 'child', 'suffer', 'nonorganic', 'cause', 'enuresis', 'simply', 'outgrow', 'symptom', 'less', 'affected', 'child', 'persistent', 'enuresis', 'adult', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9796\n",
            "    After Stopword Removal: ['one', 'unattractive', 'aspect', 'certain', 'expenditures', 'deductible', 'u', '.', '.', 'tax', 'code', 'deduction', 'worth', 'someone', 'higher', 'tax', 'bracket', '.']\n",
            "    After Regex: ['one', 'unattractive', 'aspect', 'certain', 'expenditures', 'deductible', 'u', 'tax', 'code', 'deduction', 'worth', 'someone', 'higher', 'tax', 'bracket']\n",
            "    After Lemmatization: ['one', 'unattractive', 'aspect', 'certain', 'expenditure', 'deductible', 'u', 'tax', 'code', 'deduction', 'worth', 'someone', 'higher', 'tax', 'bracket']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9797\n",
            "    After Stopword Removal: ['thereafter', ',', 'glucose', 'removed', 'jaggery', 'produce', 'sugar', '.']\n",
            "    After Regex: ['thereafter', 'glucose', 'removed', 'jaggery', 'produce', 'sugar']\n",
            "    After Lemmatization: ['thereafter', 'glucose', 'removed', 'jaggery', 'produce', 'sugar']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9798\n",
            "    After Stopword Removal: ['pacific', 'linguistics', ',', 'ser', '.']\n",
            "    After Regex: ['pacific', 'linguistics', 'ser']\n",
            "    After Lemmatization: ['pacific', 'linguistics', 'ser']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9799\n",
            "    After Stopword Removal: ['makes', 'stronger', 'argument', 'say', 'marriage', 'given', 'special', 'recognition', ',', 'particular', 'type', 'marriage', '(', 'one', 'produced', 'children', 'maintained', 'stable', 'domestic', 'environment', ')', 'given', 'special', 'recognition', '.', 'society', \"'\", 'care', \"'\", 'happy', ',', 'cares', 'produce', 'functional', 'next', 'generation', '.']\n",
            "    After Regex: ['makes', 'stronger', 'argument', 'say', 'marriage', 'given', 'special', 'recognition', 'particular', 'type', 'marriage', 'one', 'produced', 'children', 'maintained', 'stable', 'domestic', 'environment', 'given', 'special', 'recognition', 'society', 'care', 'happy', 'cares', 'produce', 'functional', 'next', 'generation']\n",
            "    After Lemmatization: ['make', 'stronger', 'argument', 'say', 'marriage', 'given', 'special', 'recognition', 'particular', 'type', 'marriage', 'one', 'produced', 'child', 'maintained', 'stable', 'domestic', 'environment', 'given', 'special', 'recognition', 'society', 'care', 'happy', 'care', 'produce', 'functional', 'next', 'generation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9800\n",
            "    After Stopword Removal: ['learned', '?']\n",
            "    After Regex: ['learned']\n",
            "    After Lemmatization: ['learned']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9801\n",
            "    After Stopword Removal: ['limiting', 'right', 'people', 'handgun', 'carry', 'permits', ',', 'bill', 'may', 'actually', 'tighten', 'restrictions', 'educational', 'institutions', '.']\n",
            "    After Regex: ['limiting', 'right', 'people', 'handgun', 'carry', 'permits', 'bill', 'may', 'actually', 'tighten', 'restrictions', 'educational', 'institutions']\n",
            "    After Lemmatization: ['limiting', 'right', 'people', 'handgun', 'carry', 'permit', 'bill', 'may', 'actually', 'tighten', 'restriction', 'educational', 'institution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9802\n",
            "    After Stopword Removal: ['eggs', 'may', 'number', 'three', 'six', 'bluish', 'beige', 'color', ',', 'marked', 'touches', 'olive', '-', 'brown', '.']\n",
            "    After Regex: ['eggs', 'may', 'number', 'three', 'six', 'bluish', 'beige', 'color', 'marked', 'touches', 'olive', 'brown']\n",
            "    After Lemmatization: ['egg', 'may', 'number', 'three', 'six', 'bluish', 'beige', 'color', 'marked', 'touch', 'olive', 'brown']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9803\n",
            "    After Stopword Removal: ['refer', 'type', 'dwelling', ',', 'made', 'material', '(', 'even', 'modern', 'wood', '-', 'frame', ',', 'brick', ',', 'concrete', 'house', ').']\n",
            "    After Regex: ['refer', 'type', 'dwelling', 'made', 'material', 'even', 'modern', 'wood', 'frame', 'brick', 'concrete', 'house']\n",
            "    After Lemmatization: ['refer', 'type', 'dwelling', 'made', 'material', 'even', 'modern', 'wood', 'frame', 'brick', 'concrete', 'house']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9804\n",
            "    After Stopword Removal: ['canal', 'offshoot', 'city', \"'\", 'brda', 'river', ',', 'brings', 'water', 'small', 'hydroelectric', 'plant', '.']\n",
            "    After Regex: ['canal', 'offshoot', 'city', 'brda', 'river', 'brings', 'water', 'small', 'hydroelectric', 'plant']\n",
            "    After Lemmatization: ['canal', 'offshoot', 'city', 'brda', 'river', 'brings', 'water', 'small', 'hydroelectric', 'plant']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9805\n",
            "    After Stopword Removal: ['microsoft', 'visio', 'professional', '2010', 'takes', 'diagramming', 'bold', 'new', 'level', 'dynamic', ',', 'data', '-', 'driven', 'visualization', 'tools', 'templates', ',', 'advanced', 'sharing', 'web', '.']\n",
            "    After Regex: ['microsoft', 'visio', 'professional', 'takes', 'diagramming', 'bold', 'new', 'level', 'dynamic', 'data', 'driven', 'visualization', 'tools', 'templates', 'advanced', 'sharing', 'web']\n",
            "    After Lemmatization: ['microsoft', 'visio', 'professional', 'take', 'diagramming', 'bold', 'new', 'level', 'dynamic', 'data', 'driven', 'visualization', 'tool', 'template', 'advanced', 'sharing', 'web']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9806\n",
            "    After Stopword Removal: ['many', ',', 'many', 'years', \"'\", 'relatively', 'easy', 'connect', 'tvs', 'vcrs', 'cable', 'systems', '.']\n",
            "    After Regex: ['many', 'many', 'years', 'relatively', 'easy', 'connect', 'tvs', 'vcrs', 'cable', 'systems']\n",
            "    After Lemmatization: ['many', 'many', 'year', 'relatively', 'easy', 'connect', 'tv', 'vcr', 'cable', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9807\n",
            "    After Stopword Removal: ['collaborative', 'effort', ',', 'done', '12,000', 'members', 'church', 'center', 'opened', '2001,', 'helped', 'research', ',', 'document', 'transcribe', 'historical', 'immigration', 'records', 'microfilm', '.']\n",
            "    After Regex: ['collaborative', 'effort', 'done', 'members', 'church', 'center', 'opened', 'helped', 'research', 'document', 'transcribe', 'historical', 'immigration', 'records', 'microfilm']\n",
            "    After Lemmatization: ['collaborative', 'effort', 'done', 'member', 'church', 'center', 'opened', 'helped', 'research', 'document', 'transcribe', 'historical', 'immigration', 'record', 'microfilm']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9808\n",
            "    After Stopword Removal: ['citizenship', 'immigration', 'services', '(', 'uscis', ')', 'undercounted', 'h', '-1', 'b', 'usage', 'almost', '15', 'percent', 'past', 'five', 'years', '.']\n",
            "    After Regex: ['citizenship', 'immigration', 'services', 'uscis', 'undercounted', 'h', 'b', 'usage', 'almost', 'percent', 'past', 'five', 'years']\n",
            "    After Lemmatization: ['citizenship', 'immigration', 'service', 'uscis', 'undercounted', 'h', 'b', 'usage', 'almost', 'percent', 'past', 'five', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9809\n",
            "    After Stopword Removal: ['article', 'discuss', 'making', 'kirtle', 'gown', 'suitable', 'henrician', 'lady', 'around', '1530–40.']\n",
            "    After Regex: ['article', 'discuss', 'making', 'kirtle', 'gown', 'suitable', 'henrician', 'lady', 'around']\n",
            "    After Lemmatization: ['article', 'discus', 'making', 'kirtle', 'gown', 'suitable', 'henrician', 'lady', 'around']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9810\n",
            "    After Stopword Removal: ['seems', 'likely', 'contemporary', 'gautama', 'buddha', 'meet', '.']\n",
            "    After Regex: ['seems', 'likely', 'contemporary', 'gautama', 'buddha', 'meet']\n",
            "    After Lemmatization: ['seems', 'likely', 'contemporary', 'gautama', 'buddha', 'meet']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9811\n",
            "    After Stopword Removal: ['maharam', 'said', 'immunity', 'marathoners', 'decreased', '72', 'hours', 'race', '.']\n",
            "    After Regex: ['maharam', 'said', 'immunity', 'marathoners', 'decreased', 'hours', 'race']\n",
            "    After Lemmatization: ['maharam', 'said', 'immunity', 'marathoner', 'decreased', 'hour', 'race']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9812\n",
            "    After Stopword Removal: ['antisemitism', 'used', 'express', 'demanding', 'good', 'aryans', 'boycott', 'jewish', 'shops', '.']\n",
            "    After Regex: ['antisemitism', 'used', 'express', 'demanding', 'good', 'aryans', 'boycott', 'jewish', 'shops']\n",
            "    After Lemmatization: ['antisemitism', 'used', 'express', 'demanding', 'good', 'aryan', 'boycott', 'jewish', 'shop']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9813\n",
            "    After Stopword Removal: ['perhaps', 'forms', 'reading', ':', 'listening', ',', 'observation', '.']\n",
            "    After Regex: ['perhaps', 'forms', 'reading', 'listening', 'observation']\n",
            "    After Lemmatization: ['perhaps', 'form', 'reading', 'listening', 'observation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9814\n",
            "    After Stopword Removal: ['trust', ',', 'assurance', ',', 'confidence', 'imply', 'feeling', 'security', '.']\n",
            "    After Regex: ['trust', 'assurance', 'confidence', 'imply', 'feeling', 'security']\n",
            "    After Lemmatization: ['trust', 'assurance', 'confidence', 'imply', 'feeling', 'security']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9815\n",
            "    After Stopword Removal: ['industry', ',', 'consists', 'mainly', 'garment', 'production', ',', 'boat', 'building', ',', 'handicrafts', ',', 'accounts', '7%', 'gdp', '.']\n",
            "    After Regex: ['industry', 'consists', 'mainly', 'garment', 'production', 'boat', 'building', 'handicrafts', 'accounts', 'gdp']\n",
            "    After Lemmatization: ['industry', 'consists', 'mainly', 'garment', 'production', 'boat', 'building', 'handicraft', 'account', 'gdp']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9816\n",
            "    After Stopword Removal: ['eastern', 'sociological', 'society', 'annual', 'meetings', '.']\n",
            "    After Regex: ['eastern', 'sociological', 'society', 'annual', 'meetings']\n",
            "    After Lemmatization: ['eastern', 'sociological', 'society', 'annual', 'meeting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9817\n",
            "    After Stopword Removal: ['whilst', 'neo', '-', 'colonial', 'economy', ',', 'hence', 'neo', '-', 'colonial', 'state', ',', ',', 'ultimate', 'analysis', ',', 'controlled', 'empire', '-', 'behalf', 'global', 'finance', 'capital', '.']\n",
            "    After Regex: ['whilst', 'neo', 'colonial', 'economy', 'hence', 'neo', 'colonial', 'state', 'ultimate', 'analysis', 'controlled', 'empire', 'behalf', 'global', 'finance', 'capital']\n",
            "    After Lemmatization: ['whilst', 'neo', 'colonial', 'economy', 'hence', 'neo', 'colonial', 'state', 'ultimate', 'analysis', 'controlled', 'empire', 'behalf', 'global', 'finance', 'capital']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9818\n",
            "    After Stopword Removal: ['man', '-', 'made', 'co', '2', 'trumped', 'villain', ',', 'things', 'impact', 'climate', ',', 'co', '2', 'emissions', 'used', 'extract', 'taxes', 'impose', 'controls', 'u', '.', '.', 'businesses', 'citizens', '.']\n",
            "    After Regex: ['man', 'made', 'co', 'trumped', 'villain', 'things', 'impact', 'climate', 'co', 'emissions', 'used', 'extract', 'taxes', 'impose', 'controls', 'u', 'businesses', 'citizens']\n",
            "    After Lemmatization: ['man', 'made', 'co', 'trumped', 'villain', 'thing', 'impact', 'climate', 'co', 'emission', 'used', 'extract', 'tax', 'impose', 'control', 'u', 'business', 'citizen']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9819\n",
            "    After Stopword Removal: ['additive', 'risk', 'predictive', 'scoring', 'system', 'based', 'regression', 'parameters', 'created', ',', 'receiver', 'operating', 'characteristic', '(', 'roc', ')', 'curve', 'plotted', 'measure', 'predictive', 'accuracy', 'models', '.']\n",
            "    After Regex: ['additive', 'risk', 'predictive', 'scoring', 'system', 'based', 'regression', 'parameters', 'created', 'receiver', 'operating', 'characteristic', 'roc', 'curve', 'plotted', 'measure', 'predictive', 'accuracy', 'models']\n",
            "    After Lemmatization: ['additive', 'risk', 'predictive', 'scoring', 'system', 'based', 'regression', 'parameter', 'created', 'receiver', 'operating', 'characteristic', 'roc', 'curve', 'plotted', 'measure', 'predictive', 'accuracy', 'model']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9820\n",
            "    After Stopword Removal: ['batteries', 'cause', 'chemical', 'burns', 'heavy', 'metal', 'poisoning', 'chewed', 'dog', '.']\n",
            "    After Regex: ['batteries', 'cause', 'chemical', 'burns', 'heavy', 'metal', 'poisoning', 'chewed', 'dog']\n",
            "    After Lemmatization: ['battery', 'cause', 'chemical', 'burn', 'heavy', 'metal', 'poisoning', 'chewed', 'dog']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9821\n",
            "    After Stopword Removal: ['without', 'harnessing', 'resources', ',', 'world', 'relies', 'taking', 'raw', 'materials', 'environment', 'manufacture', 'new', 'products', '.']\n",
            "    After Regex: ['without', 'harnessing', 'resources', 'world', 'relies', 'taking', 'raw', 'materials', 'environment', 'manufacture', 'new', 'products']\n",
            "    After Lemmatization: ['without', 'harnessing', 'resource', 'world', 'relies', 'taking', 'raw', 'material', 'environment', 'manufacture', 'new', 'product']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9822\n",
            "    After Stopword Removal: ['animals', 'store', 'eat', 'food', 'prepare', 'hibernation', 'dormancy', '.']\n",
            "    After Regex: ['animals', 'store', 'eat', 'food', 'prepare', 'hibernation', 'dormancy']\n",
            "    After Lemmatization: ['animal', 'store', 'eat', 'food', 'prepare', 'hibernation', 'dormancy']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9823\n",
            "    After Stopword Removal: ['judge', 'must', 'determine', 'removal', 'necessary', 'ensure', 'child', \"'\", 'safety', ',', 'even', 'intervention', 'prevention', 'services', 'offered', '.']\n",
            "    After Regex: ['judge', 'must', 'determine', 'removal', 'necessary', 'ensure', 'child', 'safety', 'even', 'intervention', 'prevention', 'services', 'offered']\n",
            "    After Lemmatization: ['judge', 'must', 'determine', 'removal', 'necessary', 'ensure', 'child', 'safety', 'even', 'intervention', 'prevention', 'service', 'offered']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9824\n",
            "    After Stopword Removal: ['little', 'information', 'regarding', 'cpeb', 'phosphorylation', 'mitosis', 'mammalian', 'cells', ',', 'double', 'thymidine', 'blocked', 'cells', 'labeled', 'vivo', 'adding', '[', 'γ', '-32', 'p', ']', 'atp', 'culture', 'medium', '3', 'h', 'prior', 'release', 'block', '.']\n",
            "    After Regex: ['little', 'information', 'regarding', 'cpeb', 'phosphorylation', 'mitosis', 'mammalian', 'cells', 'double', 'thymidine', 'blocked', 'cells', 'labeled', 'vivo', 'adding', 'p', 'atp', 'culture', 'medium', 'h', 'prior', 'release', 'block']\n",
            "    After Lemmatization: ['little', 'information', 'regarding', 'cpeb', 'phosphorylation', 'mitosis', 'mammalian', 'cell', 'double', 'thymidine', 'blocked', 'cell', 'labeled', 'vivo', 'adding', 'p', 'atp', 'culture', 'medium', 'h', 'prior', 'release', 'block']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9825\n",
            "    After Stopword Removal: ['indian', 'supreme', 'court', 'rejected', 'petition', 'major', 'firms', 'seeking', 'withdrawal', 'tax', 'movement', 'goods', 'states', ',', 'decision', 'would', 'force', 'pay', 'estimated', '$4.5', 'billion', 'back', 'taxes', 'interest', ',', 'reuters', 'reports', '.']\n",
            "    After Regex: ['indian', 'supreme', 'court', 'rejected', 'petition', 'major', 'firms', 'seeking', 'withdrawal', 'tax', 'movement', 'goods', 'states', 'decision', 'would', 'force', 'pay', 'estimated', 'billion', 'back', 'taxes', 'interest', 'reuters', 'reports']\n",
            "    After Lemmatization: ['indian', 'supreme', 'court', 'rejected', 'petition', 'major', 'firm', 'seeking', 'withdrawal', 'tax', 'movement', 'good', 'state', 'decision', 'would', 'force', 'pay', 'estimated', 'billion', 'back', 'tax', 'interest', 'reuters', 'report']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9826\n",
            "    After Stopword Removal: ['learn', 'different', 'techniques', 'dynamization', 'team', 'building', ',', 'mainly', 'oriented', 'social', 'inclusion', '.']\n",
            "    After Regex: ['learn', 'different', 'techniques', 'dynamization', 'team', 'building', 'mainly', 'oriented', 'social', 'inclusion']\n",
            "    After Lemmatization: ['learn', 'different', 'technique', 'dynamization', 'team', 'building', 'mainly', 'oriented', 'social', 'inclusion']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9827\n",
            "    After Stopword Removal: ['president', 'proposing', 'limits', 'guns', ',', 'clips', 'enforcement', 'existing', 'laws', 'also', 'option', 'january', '16,2013', 'washington', '(', 'ap', ')—', 'president', 'barack', 'obama', 'launching', 'nation', \"'\", 'sweeping', 'effort', 'curb', 'gun', 'violence', 'nearly', 'two', 'decades', ',', 'urging', 'reluctant', 'congress', 'ban', 'military', '-', 'style', 'assault', 'weapons', '......']\n",
            "    After Regex: ['president', 'proposing', 'limits', 'guns', 'clips', 'enforcement', 'existing', 'laws', 'also', 'option', 'january', 'washington', 'ap', 'president', 'barack', 'obama', 'launching', 'nation', 'sweeping', 'effort', 'curb', 'gun', 'violence', 'nearly', 'two', 'decades', 'urging', 'reluctant', 'congress', 'ban', 'military', 'style', 'assault', 'weapons']\n",
            "    After Lemmatization: ['president', 'proposing', 'limit', 'gun', 'clip', 'enforcement', 'existing', 'law', 'also', 'option', 'january', 'washington', 'ap', 'president', 'barack', 'obama', 'launching', 'nation', 'sweeping', 'effort', 'curb', 'gun', 'violence', 'nearly', 'two', 'decade', 'urging', 'reluctant', 'congress', 'ban', 'military', 'style', 'assault', 'weapon']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9828\n",
            "    After Stopword Removal: ['courses', 'learner', '-', 'centered', 'industry', '-', 'focused', ';', 'aimed', 'producing', 'students', 'practical', 'skills', 'knowledge', ',', 'opposed', 'regurgitation', 'course', 'texts', 'lecture', 'notes', '.']\n",
            "    After Regex: ['courses', 'learner', 'centered', 'industry', 'focused', 'aimed', 'producing', 'students', 'practical', 'skills', 'knowledge', 'opposed', 'regurgitation', 'course', 'texts', 'lecture', 'notes']\n",
            "    After Lemmatization: ['course', 'learner', 'centered', 'industry', 'focused', 'aimed', 'producing', 'student', 'practical', 'skill', 'knowledge', 'opposed', 'regurgitation', 'course', 'text', 'lecture', 'note']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9829\n",
            "    After Stopword Removal: ['continuous', 'field', 'wave', 'character', 'matter', '.']\n",
            "    After Regex: ['continuous', 'field', 'wave', 'character', 'matter']\n",
            "    After Lemmatization: ['continuous', 'field', 'wave', 'character', 'matter']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9830\n",
            "    After Stopword Removal: ['neurofeedback', 'type', 'biofeedback', 'measures', 'brain', 'waves', 'produce', 'signal', 'used', 'feedback', 'teach', 'self', '-', 'regulation', 'brain', 'function', '.']\n",
            "    After Regex: ['neurofeedback', 'type', 'biofeedback', 'measures', 'brain', 'waves', 'produce', 'signal', 'used', 'feedback', 'teach', 'self', 'regulation', 'brain', 'function']\n",
            "    After Lemmatization: ['neurofeedback', 'type', 'biofeedback', 'measure', 'brain', 'wave', 'produce', 'signal', 'used', 'feedback', 'teach', 'self', 'regulation', 'brain', 'function']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9831\n",
            "    After Stopword Removal: ['virtually', 'every', 'medical', 'practice', ',', 'matter', 'large', 'small', ',', 'make', 'use', 'medical', 'assistant', '.']\n",
            "    After Regex: ['virtually', 'every', 'medical', 'practice', 'matter', 'large', 'small', 'make', 'use', 'medical', 'assistant']\n",
            "    After Lemmatization: ['virtually', 'every', 'medical', 'practice', 'matter', 'large', 'small', 'make', 'use', 'medical', 'assistant']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9832\n",
            "    After Stopword Removal: ['monitoring', 'tyres', 'maintaining', 'correct', 'pressure', 'also', 'means', 'fuel', 'usage', 'efficient', 'length', 'tyre', 'life', 'maximised', '.']\n",
            "    After Regex: ['monitoring', 'tyres', 'maintaining', 'correct', 'pressure', 'also', 'means', 'fuel', 'usage', 'efficient', 'length', 'tyre', 'life', 'maximised']\n",
            "    After Lemmatization: ['monitoring', 'tyre', 'maintaining', 'correct', 'pressure', 'also', 'mean', 'fuel', 'usage', 'efficient', 'length', 'tyre', 'life', 'maximised']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9833\n",
            "    After Stopword Removal: ['besides', 'largest', 'teeth', 'available', '(', 'prevents', 'debris', 'causing', 'stick', '),', \"'\", 'rust', 'chemical', 'corrosion', 'proof', '.']\n",
            "    After Regex: ['besides', 'largest', 'teeth', 'available', 'prevents', 'debris', 'causing', 'stick', 'rust', 'chemical', 'corrosion', 'proof']\n",
            "    After Lemmatization: ['besides', 'largest', 'teeth', 'available', 'prevents', 'debris', 'causing', 'stick', 'rust', 'chemical', 'corrosion', 'proof']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9834\n",
            "    After Stopword Removal: ['tuesday', ',', 'dec', '.4,2012', 'order', 'importance', ':', 'study', 'finds', 'prioritizing', 'rather', 'canvassing', 'entire', 'plant', 'genome', 'may', 'lead', 'improved', 'crops', 'manhattan', '--', 'new', 'study', 'may', 'help', 'scientists', 'produce', 'better', 'climate', '-', 'resistant', 'corn', 'food', 'production', 'plants', 'putting', 'spin', 'notion', 'eat', '.']\n",
            "    After Regex: ['tuesday', 'dec', 'order', 'importance', 'study', 'finds', 'prioritizing', 'rather', 'canvassing', 'entire', 'plant', 'genome', 'may', 'lead', 'improved', 'crops', 'manhattan', 'new', 'study', 'may', 'help', 'scientists', 'produce', 'better', 'climate', 'resistant', 'corn', 'food', 'production', 'plants', 'putting', 'spin', 'notion', 'eat']\n",
            "    After Lemmatization: ['tuesday', 'dec', 'order', 'importance', 'study', 'find', 'prioritizing', 'rather', 'canvassing', 'entire', 'plant', 'genome', 'may', 'lead', 'improved', 'crop', 'manhattan', 'new', 'study', 'may', 'help', 'scientist', 'produce', 'better', 'climate', 'resistant', 'corn', 'food', 'production', 'plant', 'putting', 'spin', 'notion', 'eat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9835\n",
            "    After Stopword Removal: ['also', ',', 'establish', 'test', 'disaster', 'recovery', 'plan', '.']\n",
            "    After Regex: ['also', 'establish', 'test', 'disaster', 'recovery', 'plan']\n",
            "    After Lemmatization: ['also', 'establish', 'test', 'disaster', 'recovery', 'plan']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9836\n",
            "    After Stopword Removal: ['makes', 'rub', 'different', 'marinade', 'apply', 'flavoring', 'directly', 'surface', 'meat', '.']\n",
            "    After Regex: ['makes', 'rub', 'different', 'marinade', 'apply', 'flavoring', 'directly', 'surface', 'meat']\n",
            "    After Lemmatization: ['make', 'rub', 'different', 'marinade', 'apply', 'flavoring', 'directly', 'surface', 'meat']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9837\n",
            "    After Stopword Removal: ['late', 'nineteenth', 'early', 'twentieth', 'centuries', ',', 'domestic', 'gas', 'produced', 'destructive', 'distillation', 'bituminous', 'coal', '(', '-', 'called', \"'\", 'town', 'gas', \"').\"]\n",
            "    After Regex: ['late', 'nineteenth', 'early', 'twentieth', 'centuries', 'domestic', 'gas', 'produced', 'destructive', 'distillation', 'bituminous', 'coal', 'called', 'town', 'gas']\n",
            "    After Lemmatization: ['late', 'nineteenth', 'early', 'twentieth', 'century', 'domestic', 'gas', 'produced', 'destructive', 'distillation', 'bituminous', 'coal', 'called', 'town', 'gas']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9838\n",
            "    After Stopword Removal: ['amorphous', 'drugs', 'typically', 'efficiently', 'taken', 'body', 'crystalline', 'cousins', ';', 'amorphous', 'drugs', 'highly', 'soluble', 'higher', 'bioavailability', ',', 'suggesting', 'lower', 'dose', 'produce', 'desired', 'effect', '.']\n",
            "    After Regex: ['amorphous', 'drugs', 'typically', 'efficiently', 'taken', 'body', 'crystalline', 'cousins', 'amorphous', 'drugs', 'highly', 'soluble', 'higher', 'bioavailability', 'suggesting', 'lower', 'dose', 'produce', 'desired', 'effect']\n",
            "    After Lemmatization: ['amorphous', 'drug', 'typically', 'efficiently', 'taken', 'body', 'crystalline', 'cousin', 'amorphous', 'drug', 'highly', 'soluble', 'higher', 'bioavailability', 'suggesting', 'lower', 'dose', 'produce', 'desired', 'effect']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9839\n",
            "    After Stopword Removal: ['report', 'also', 'include', 'advice', 'future', 'location', 'current', 'functions', 'department', 'building', 'housing', ',', 'social', 'housing', 'policy', 'tenancy', 'service', '.']\n",
            "    After Regex: ['report', 'also', 'include', 'advice', 'future', 'location', 'current', 'functions', 'department', 'building', 'housing', 'social', 'housing', 'policy', 'tenancy', 'service']\n",
            "    After Lemmatization: ['report', 'also', 'include', 'advice', 'future', 'location', 'current', 'function', 'department', 'building', 'housing', 'social', 'housing', 'policy', 'tenancy', 'service']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9840\n",
            "    After Stopword Removal: ['please', 'see', 'question', '36', 'details', 'google', \"'\", 'ipv', '6', 'implementation', '.']\n",
            "    After Regex: ['please', 'see', 'question', 'details', 'google', 'ipv', 'implementation']\n",
            "    After Lemmatization: ['please', 'see', 'question', 'detail', 'google', 'ipv', 'implementation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9841\n",
            "    After Stopword Removal: ['another', 'suggestion', 'disseminate', 'list', 'questions', 'patients', 'could', 'ask', 'physicians', '.']\n",
            "    After Regex: ['another', 'suggestion', 'disseminate', 'list', 'questions', 'patients', 'could', 'ask', 'physicians']\n",
            "    After Lemmatization: ['another', 'suggestion', 'disseminate', 'list', 'question', 'patient', 'could', 'ask', 'physician']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9842\n",
            "    After Stopword Removal: ['team', 'researchers', 'cornell', 'university', 'surveyed', '501', 'females', '20', '35', 'years', 'age', ',', 'asking', 'questions', 'cooking', 'habits', ',', 'weight', 'height', ',', 'respondents', 'found', 'new', 'recipes', '.']\n",
            "    After Regex: ['team', 'researchers', 'cornell', 'university', 'surveyed', 'females', 'years', 'age', 'asking', 'questions', 'cooking', 'habits', 'weight', 'height', 'respondents', 'found', 'new', 'recipes']\n",
            "    After Lemmatization: ['team', 'researcher', 'cornell', 'university', 'surveyed', 'female', 'year', 'age', 'asking', 'question', 'cooking', 'habit', 'weight', 'height', 'respondent', 'found', 'new', 'recipe']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9843\n",
            "    After Stopword Removal: ['goodwill', 'human', 'effort', 'alone', 'keep', 'alive', ',', 'would', 'died', '.']\n",
            "    After Regex: ['goodwill', 'human', 'effort', 'alone', 'keep', 'alive', 'would', 'died']\n",
            "    After Lemmatization: ['goodwill', 'human', 'effort', 'alone', 'keep', 'alive', 'would', 'died']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9844\n",
            "    After Stopword Removal: ['power', 'outage', 'widespread', ',', 'traffic', 'lights', 'could', 'roads', 'could', 'become', 'congested', '.']\n",
            "    After Regex: ['power', 'outage', 'widespread', 'traffic', 'lights', 'could', 'roads', 'could', 'become', 'congested']\n",
            "    After Lemmatization: ['power', 'outage', 'widespread', 'traffic', 'light', 'could', 'road', 'could', 'become', 'congested']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9845\n",
            "    After Stopword Removal: ['mediation', 'process', 'mediation', 'voluntary', 'process', 'parties', 'dispute', 'attempt', 'resolve', 'differences', 'discussion', ',', 'clarification', ',', 'orderly', 'negotiation', '.']\n",
            "    After Regex: ['mediation', 'process', 'mediation', 'voluntary', 'process', 'parties', 'dispute', 'attempt', 'resolve', 'differences', 'discussion', 'clarification', 'orderly', 'negotiation']\n",
            "    After Lemmatization: ['mediation', 'process', 'mediation', 'voluntary', 'process', 'party', 'dispute', 'attempt', 'resolve', 'difference', 'discussion', 'clarification', 'orderly', 'negotiation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9846\n",
            "    After Stopword Removal: ['fun', 'learning', 'french', 'listening', '-', 'based', 'interactive', 'program', '.']\n",
            "    After Regex: ['fun', 'learning', 'french', 'listening', 'based', 'interactive', 'program']\n",
            "    After Lemmatization: ['fun', 'learning', 'french', 'listening', 'based', 'interactive', 'program']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9847\n",
            "    After Stopword Removal: ['conditions', 'explained', 'parental', 'comments', ',', 'friends', \"'\", 'comments', ',', 'newscasters', \"'\", 'comments', ',', 'attitudes', 'displayed', 'institutions', 'church', ',', 'school', ',', 'government', ',', 'sports', ',', '.']\n",
            "    After Regex: ['conditions', 'explained', 'parental', 'comments', 'friends', 'comments', 'newscasters', 'comments', 'attitudes', 'displayed', 'institutions', 'church', 'school', 'government', 'sports']\n",
            "    After Lemmatization: ['condition', 'explained', 'parental', 'comment', 'friend', 'comment', 'newscaster', 'comment', 'attitude', 'displayed', 'institution', 'church', 'school', 'government', 'sport']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9848\n",
            "    After Stopword Removal: ['government', \"'\", 'rationale', 'eliminating', 'csumb', 'size', 'provincial', 'government', \"'\", 'deficit', '.']\n",
            "    After Regex: ['government', 'rationale', 'eliminating', 'csumb', 'size', 'provincial', 'government', 'deficit']\n",
            "    After Lemmatization: ['government', 'rationale', 'eliminating', 'csumb', 'size', 'provincial', 'government', 'deficit']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9849\n",
            "    After Stopword Removal: ['symptoms', 'rett', 'syndrome', ',', 'neurological', 'disorder', 'primarily', 'affecting', 'girls', '.']\n",
            "    After Regex: ['symptoms', 'rett', 'syndrome', 'neurological', 'disorder', 'primarily', 'affecting', 'girls']\n",
            "    After Lemmatization: ['symptom', 'rett', 'syndrome', 'neurological', 'disorder', 'primarily', 'affecting', 'girl']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9850\n",
            "    After Stopword Removal: ['similarly', ',', 'absence', 'public', 'charging', 'facilities', 'also', 'makes', 'journeys', 'bit', 'risky', '.']\n",
            "    After Regex: ['similarly', 'absence', 'public', 'charging', 'facilities', 'also', 'makes', 'journeys', 'bit', 'risky']\n",
            "    After Lemmatization: ['similarly', 'absence', 'public', 'charging', 'facility', 'also', 'make', 'journey', 'bit', 'risky']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9851\n",
            "    After Stopword Removal: ['volatile', 'organic', 'compounds', ',', 'vocs', ',', 'gases', 'released', 'commonly', 'used', 'products', '.']\n",
            "    After Regex: ['volatile', 'organic', 'compounds', 'vocs', 'gases', 'released', 'commonly', 'used', 'products']\n",
            "    After Lemmatization: ['volatile', 'organic', 'compound', 'vocs', 'gas', 'released', 'commonly', 'used', 'product']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9852\n",
            "    After Stopword Removal: ['u', '.', '.', 'launching', 'investigation', 'chinese', 'trade', 'practices', 'clean', 'energy', 'sector', '.']\n",
            "    After Regex: ['u', 'launching', 'investigation', 'chinese', 'trade', 'practices', 'clean', 'energy', 'sector']\n",
            "    After Lemmatization: ['u', 'launching', 'investigation', 'chinese', 'trade', 'practice', 'clean', 'energy', 'sector']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9853\n",
            "    After Stopword Removal: ['police', 'say', 'hiring', 'recruitment', 'efforts', 'result', 'budget', ',', 'thus', 'possibly', 'discouraging', 'young', 'men', 'women', 'taking', 'test', '.']\n",
            "    After Regex: ['police', 'say', 'hiring', 'recruitment', 'efforts', 'result', 'budget', 'thus', 'possibly', 'discouraging', 'young', 'men', 'women', 'taking', 'test']\n",
            "    After Lemmatization: ['police', 'say', 'hiring', 'recruitment', 'effort', 'result', 'budget', 'thus', 'possibly', 'discouraging', 'young', 'men', 'woman', 'taking', 'test']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9854\n",
            "    After Stopword Removal: ['using', 'latest', 'versions', 'programs', 'makes', 'system', 'secure', 'minimizes', 'vulnerabilities', 'may', 'jeopardize', 'data', '.']\n",
            "    After Regex: ['using', 'latest', 'versions', 'programs', 'makes', 'system', 'secure', 'minimizes', 'vulnerabilities', 'may', 'jeopardize', 'data']\n",
            "    After Lemmatization: ['using', 'latest', 'version', 'program', 'make', 'system', 'secure', 'minimizes', 'vulnerability', 'may', 'jeopardize', 'data']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9855\n",
            "    After Stopword Removal: ['researchers', 'found', '43', 'percent', 'participants', 'one', 'weight', '-', 'loss', 'program', '—', 'including', 'underwent', 'bariatric', 'surgery', '—', 'dropped', 'program', 'dropping', 'desired', 'pounds', '.']\n",
            "    After Regex: ['researchers', 'found', 'percent', 'participants', 'one', 'weight', 'loss', 'program', 'including', 'underwent', 'bariatric', 'surgery', 'dropped', 'program', 'dropping', 'desired', 'pounds']\n",
            "    After Lemmatization: ['researcher', 'found', 'percent', 'participant', 'one', 'weight', 'loss', 'program', 'including', 'underwent', 'bariatric', 'surgery', 'dropped', 'program', 'dropping', 'desired', 'pound']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9856\n",
            "    After Stopword Removal: ['joined', 'communist', 'party', 'china', '(', 'cpc', ')', 'april', '1960.']\n",
            "    After Regex: ['joined', 'communist', 'party', 'china', 'cpc', 'april']\n",
            "    After Lemmatization: ['joined', 'communist', 'party', 'china', 'cpc', 'april']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9857\n",
            "    After Stopword Removal: ['finest', 'tools', 'educate', 'kids', 'coding', 'programming', 'fun', 'way', 'consistently', 'coach', 'new', 'technology', 'skills', 'need', 'future', 'jobs', '?']\n",
            "    After Regex: ['finest', 'tools', 'educate', 'kids', 'coding', 'programming', 'fun', 'way', 'consistently', 'coach', 'new', 'technology', 'skills', 'need', 'future', 'jobs']\n",
            "    After Lemmatization: ['finest', 'tool', 'educate', 'kid', 'coding', 'programming', 'fun', 'way', 'consistently', 'coach', 'new', 'technology', 'skill', 'need', 'future', 'job']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9858\n",
            "    After Stopword Removal: ['article', 'talks', 'controversy', 'misuse', 'idea', 'new', 'teaching', 'methods', '.']\n",
            "    After Regex: ['article', 'talks', 'controversy', 'misuse', 'idea', 'new', 'teaching', 'methods']\n",
            "    After Lemmatization: ['article', 'talk', 'controversy', 'misuse', 'idea', 'new', 'teaching', 'method']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9859\n",
            "    After Stopword Removal: ['multidisciplinary', 'public', 'clinics', 'developed', 'selected', 'locales', 'countries', 'enhance', 'services', 'hard', '-', '-', 'reach', 'populations', '(', 'bel', '-', 'ocr', 'page', '221', 'appendix', 'b', '221', 'glum', ',', 'united', 'kingdom', ').']\n",
            "    After Regex: ['multidisciplinary', 'public', 'clinics', 'developed', 'selected', 'locales', 'countries', 'enhance', 'services', 'hard', 'reach', 'populations', 'bel', 'ocr', 'page', 'appendix', 'b', 'glum', 'united', 'kingdom']\n",
            "    After Lemmatization: ['multidisciplinary', 'public', 'clinic', 'developed', 'selected', 'locale', 'country', 'enhance', 'service', 'hard', 'reach', 'population', 'bel', 'ocr', 'page', 'appendix', 'b', 'glum', 'united', 'kingdom']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9860\n",
            "    After Stopword Removal: ['ewo', ',', 'big', 'british', 'cotton', 'mill', ',', 'made', 'profit', '$8,000,000,', 'cotton', 'mills', 'settlement', 'made', 'profits', 'million', '.']\n",
            "    After Regex: ['ewo', 'big', 'british', 'cotton', 'mill', 'made', 'profit', 'cotton', 'mills', 'settlement', 'made', 'profits', 'million']\n",
            "    After Lemmatization: ['ewo', 'big', 'british', 'cotton', 'mill', 'made', 'profit', 'cotton', 'mill', 'settlement', 'made', 'profit', 'million']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9861\n",
            "    After Stopword Removal: ['since', 'biomass', 'contains', 'hq', 'ct', '-', 'type', 'species', ',', 'likely', 'combustion', 'biomass', 'also', 'form', 'semiquinone', '-', 'type', 'radicals', '.']\n",
            "    After Regex: ['since', 'biomass', 'contains', 'hq', 'ct', 'type', 'species', 'likely', 'combustion', 'biomass', 'also', 'form', 'semiquinone', 'type', 'radicals']\n",
            "    After Lemmatization: ['since', 'biomass', 'contains', 'hq', 'ct', 'type', 'specie', 'likely', 'combustion', 'biomass', 'also', 'form', 'semiquinone', 'type', 'radical']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9862\n",
            "    After Stopword Removal: ['configuration', 'parameters', 'pertinent', 'device', '18,', 'wireless', 'service', 'provider', '16,', 'computer', 'network', '12', 'stored', 'device', '18', 'configuration', 'file', 'determine', 'optimal', 'packet', 'sizes', '.']\n",
            "    After Regex: ['configuration', 'parameters', 'pertinent', 'device', 'wireless', 'service', 'provider', 'computer', 'network', 'stored', 'device', 'configuration', 'file', 'determine', 'optimal', 'packet', 'sizes']\n",
            "    After Lemmatization: ['configuration', 'parameter', 'pertinent', 'device', 'wireless', 'service', 'provider', 'computer', 'network', 'stored', 'device', 'configuration', 'file', 'determine', 'optimal', 'packet', 'size']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9863\n",
            "    After Stopword Removal: ['june', ',1982(3)', 'oral', 'acyclovir', 'treatment', 'suppression', 'genital', 'herpes', 'simplex', 'virus', '.']\n",
            "    After Regex: ['june', 'oral', 'acyclovir', 'treatment', 'suppression', 'genital', 'herpes', 'simplex', 'virus']\n",
            "    After Lemmatization: ['june', 'oral', 'acyclovir', 'treatment', 'suppression', 'genital', 'herpes', 'simplex', 'virus']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9864\n",
            "    After Stopword Removal: ['herding', 'dog', 'required', 'constant', 'motion', 'flock', 'driven', ',', 'correct', ',', 'efficient', 'movement', 'essential', '.']\n",
            "    After Regex: ['herding', 'dog', 'required', 'constant', 'motion', 'flock', 'driven', 'correct', 'efficient', 'movement', 'essential']\n",
            "    After Lemmatization: ['herding', 'dog', 'required', 'constant', 'motion', 'flock', 'driven', 'correct', 'efficient', 'movement', 'essential']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9865\n",
            "    After Stopword Removal: ['way', ',', 'many', 'government', 'processes', 'designed', 'million', 'event', '.']\n",
            "    After Regex: ['way', 'many', 'government', 'processes', 'designed', 'million', 'event']\n",
            "    After Lemmatization: ['way', 'many', 'government', 'process', 'designed', 'million', 'event']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9866\n",
            "    After Stopword Removal: ['beholding', ',', 'soldiers', 'could', 'conclude', 'would', 'victorious', '.']\n",
            "    After Regex: ['beholding', 'soldiers', 'could', 'conclude', 'would', 'victorious']\n",
            "    After Lemmatization: ['beholding', 'soldier', 'could', 'conclude', 'would', 'victorious']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9867\n",
            "    After Stopword Removal: ['dr', '.', 'vander', 'woude', 'said', 'residual', 'tumor', 'detected', 'chemotherapy', ',', 'sally', 'decreased', 'risk', 'developing', 'recurrent', 'disease', 'future', '.']\n",
            "    After Regex: ['dr', 'vander', 'woude', 'said', 'residual', 'tumor', 'detected', 'chemotherapy', 'sally', 'decreased', 'risk', 'developing', 'recurrent', 'disease', 'future']\n",
            "    After Lemmatization: ['dr', 'vander', 'woude', 'said', 'residual', 'tumor', 'detected', 'chemotherapy', 'sally', 'decreased', 'risk', 'developing', 'recurrent', 'disease', 'future']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9868\n",
            "    After Stopword Removal: ['gorilla', \"'\", 'huge', 'nostrils', '?']\n",
            "    After Regex: ['gorilla', 'huge', 'nostrils']\n",
            "    After Lemmatization: ['gorilla', 'huge', 'nostril']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9869\n",
            "    After Stopword Removal: ['chiropractic', 'seem', 'effective', 'placebo', ',', 'great', 'deal', '.']\n",
            "    After Regex: ['chiropractic', 'seem', 'effective', 'placebo', 'great', 'deal']\n",
            "    After Lemmatization: ['chiropractic', 'seem', 'effective', 'placebo', 'great', 'deal']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9870\n",
            "    After Stopword Removal: ['second', ',', 'traditional', 'antigen', 'tests', 'simply', 'sensitive', 'enough', ',', 'dr', '.', 'schupbach', 'says', '.']\n",
            "    After Regex: ['second', 'traditional', 'antigen', 'tests', 'simply', 'sensitive', 'enough', 'dr', 'schupbach', 'says']\n",
            "    After Lemmatization: ['second', 'traditional', 'antigen', 'test', 'simply', 'sensitive', 'enough', 'dr', 'schupbach', 'say']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9871\n",
            "    After Stopword Removal: ['brushing', 'dog', \"'\", 'teeth', ',', 'sure', 'hold', 'brush', 'sponge', 'downward', '45', 'degree', 'angle', '.']\n",
            "    After Regex: ['brushing', 'dog', 'teeth', 'sure', 'hold', 'brush', 'sponge', 'downward', 'degree', 'angle']\n",
            "    After Lemmatization: ['brushing', 'dog', 'teeth', 'sure', 'hold', 'brush', 'sponge', 'downward', 'degree', 'angle']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9872\n",
            "    After Stopword Removal: ['inconsistency', 'care', 'delivery', 'poor', 'knowledge', 'underlying', 'costs', '.']\n",
            "    After Regex: ['inconsistency', 'care', 'delivery', 'poor', 'knowledge', 'underlying', 'costs']\n",
            "    After Lemmatization: ['inconsistency', 'care', 'delivery', 'poor', 'knowledge', 'underlying', 'cost']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9873\n",
            "    After Stopword Removal: ['kmd', 'living', 'clay', 'enormous', 'absorbent', 'powers', 'due', 'structure', 'micro', 'molecules', '.']\n",
            "    After Regex: ['kmd', 'living', 'clay', 'enormous', 'absorbent', 'powers', 'due', 'structure', 'micro', 'molecules']\n",
            "    After Lemmatization: ['kmd', 'living', 'clay', 'enormous', 'absorbent', 'power', 'due', 'structure', 'micro', 'molecule']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9874\n",
            "    After Stopword Removal: ['every', 'nation', 'features', 'visible', 'first', 'sight', '.']\n",
            "    After Regex: ['every', 'nation', 'features', 'visible', 'first', 'sight']\n",
            "    After Lemmatization: ['every', 'nation', 'feature', 'visible', 'first', 'sight']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9875\n",
            "    After Stopword Removal: ['may', 'consider', 'computers', 'hard', 'drive', ';', 'simply', 'connect', 'network', '.']\n",
            "    After Regex: ['may', 'consider', 'computers', 'hard', 'drive', 'simply', 'connect', 'network']\n",
            "    After Lemmatization: ['may', 'consider', 'computer', 'hard', 'drive', 'simply', 'connect', 'network']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9876\n",
            "    After Stopword Removal: ['basic', 'metabolic', 'panels', 'typically', 'performed', 'shortly', 'initiation', 'therapy', '(', 'least', 'daily', 'hospitalized', '),', 'monitor', 'renal', 'function', 'ensure', 'diuretics', 'causing', 'dangerous', 'perturbations', 'serum', 'potassium', 'levels', '.']\n",
            "    After Regex: ['basic', 'metabolic', 'panels', 'typically', 'performed', 'shortly', 'initiation', 'therapy', 'least', 'daily', 'hospitalized', 'monitor', 'renal', 'function', 'ensure', 'diuretics', 'causing', 'dangerous', 'perturbations', 'serum', 'potassium', 'levels']\n",
            "    After Lemmatization: ['basic', 'metabolic', 'panel', 'typically', 'performed', 'shortly', 'initiation', 'therapy', 'least', 'daily', 'hospitalized', 'monitor', 'renal', 'function', 'ensure', 'diuretic', 'causing', 'dangerous', 'perturbation', 'serum', 'potassium', 'level']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9877\n",
            "    After Stopword Removal: ['impoverished', 'people', 'needed', 'health', 'care', ',', 'education', 'jobs', ',', 'yet', 'raja', 'mustang', \"'\", 'first', 'plea', 'outsiders', 'offered', 'help', 'save', 'monasteries', '.']\n",
            "    After Regex: ['impoverished', 'people', 'needed', 'health', 'care', 'education', 'jobs', 'yet', 'raja', 'mustang', 'first', 'plea', 'outsiders', 'offered', 'help', 'save', 'monasteries']\n",
            "    After Lemmatization: ['impoverished', 'people', 'needed', 'health', 'care', 'education', 'job', 'yet', 'raja', 'mustang', 'first', 'plea', 'outsider', 'offered', 'help', 'save', 'monastery']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9878\n",
            "    After Stopword Removal: ['violation', 'regulation', 'relating', 'vaccination', ',', 'screening', 'immunization', '.']\n",
            "    After Regex: ['violation', 'regulation', 'relating', 'vaccination', 'screening', 'immunization']\n",
            "    After Lemmatization: ['violation', 'regulation', 'relating', 'vaccination', 'screening', 'immunization']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9879\n",
            "    After Stopword Removal: ['mineral', 'based', 'industries', '-', 'abundance', 'minerals', 'dholpur', 'given', 'boost', 'industries', 'totaling', 'around', '527', 'units', 'work', 'force', '2998', 'employees', '.']\n",
            "    After Regex: ['mineral', 'based', 'industries', 'abundance', 'minerals', 'dholpur', 'given', 'boost', 'industries', 'totaling', 'around', 'units', 'work', 'force', 'employees']\n",
            "    After Lemmatization: ['mineral', 'based', 'industry', 'abundance', 'mineral', 'dholpur', 'given', 'boost', 'industry', 'totaling', 'around', 'unit', 'work', 'force', 'employee']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9880\n",
            "    After Stopword Removal: ['root', 'cause', 'conditions', 'inflammation', 'chronic', 'dehydration', '.']\n",
            "    After Regex: ['root', 'cause', 'conditions', 'inflammation', 'chronic', 'dehydration']\n",
            "    After Lemmatization: ['root', 'cause', 'condition', 'inflammation', 'chronic', 'dehydration']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9881\n",
            "    After Stopword Removal: ['society', 'reveres', 'idea', 'selfless', 'mother', 'thinks', 'nothing', 'themself', ',', 'somehow', 'children', 'privy', 'ideal', 'early', 'age', '.']\n",
            "    After Regex: ['society', 'reveres', 'idea', 'selfless', 'mother', 'thinks', 'nothing', 'themself', 'somehow', 'children', 'privy', 'ideal', 'early', 'age']\n",
            "    After Lemmatization: ['society', 'revere', 'idea', 'selfless', 'mother', 'think', 'nothing', 'themself', 'somehow', 'child', 'privy', 'ideal', 'early', 'age']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9882\n",
            "    After Stopword Removal: ['document', 'object', 'model', '(', 'dom', ')', 'platform', '-', 'language', '-', 'independent', 'interface', 'allow', 'programs', 'scripts', 'dynamically', 'access', 'update', 'content', ',', 'structure', 'style', 'documents', '.']\n",
            "    After Regex: ['document', 'object', 'model', 'dom', 'platform', 'language', 'independent', 'interface', 'allow', 'programs', 'scripts', 'dynamically', 'access', 'update', 'content', 'structure', 'style', 'documents']\n",
            "    After Lemmatization: ['document', 'object', 'model', 'dom', 'platform', 'language', 'independent', 'interface', 'allow', 'program', 'script', 'dynamically', 'access', 'update', 'content', 'structure', 'style', 'document']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9883\n",
            "    After Stopword Removal: ['last', 'remnant', 'temple', 'quickly', 'became', 'one', 'celebrated', 'religious', 'sites', 'jerusalem', '.']\n",
            "    After Regex: ['last', 'remnant', 'temple', 'quickly', 'became', 'one', 'celebrated', 'religious', 'sites', 'jerusalem']\n",
            "    After Lemmatization: ['last', 'remnant', 'temple', 'quickly', 'became', 'one', 'celebrated', 'religious', 'site', 'jerusalem']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9884\n",
            "    After Stopword Removal: ['butler', 'died', '18', 'th', 'june', ',1902.1', '1908,', 'president', 'british', 'association', ',', 'inaugural', 'address', 'dublin', ',', 'mr', '.', 'francis', 'darwin', 'paid', 'butler', 'posthumous', 'honour', 'quoting', 'translation', 'hering', \"'\", 'lecture', 'memory', 'unconscious', 'memory', ',', 'mentioning', 'butler', 'independently', 'arrived', '1[', 'letter', 'l', ',', 'p', '.216', 'francis', 'darwin', 'henrietta', 'litchfield', '].']\n",
            "    After Regex: ['butler', 'died', 'th', 'june', 'president', 'british', 'association', 'inaugural', 'address', 'dublin', 'mr', 'francis', 'darwin', 'paid', 'butler', 'posthumous', 'honour', 'quoting', 'translation', 'hering', 'lecture', 'memory', 'unconscious', 'memory', 'mentioning', 'butler', 'independently', 'arrived', 'letter', 'l', 'p', 'francis', 'darwin', 'henrietta', 'litchfield']\n",
            "    After Lemmatization: ['butler', 'died', 'th', 'june', 'president', 'british', 'association', 'inaugural', 'address', 'dublin', 'mr', 'francis', 'darwin', 'paid', 'butler', 'posthumous', 'honour', 'quoting', 'translation', 'hering', 'lecture', 'memory', 'unconscious', 'memory', 'mentioning', 'butler', 'independently', 'arrived', 'letter', 'l', 'p', 'francis', 'darwin', 'henrietta', 'litchfield']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9885\n",
            "    After Stopword Removal: ['exhibition', 'explores', 'challenges', 'commemorating', 'ongoing', 'contribution', 'people', 'african', 'diaspora', 'history', ',', 'culture', 'fabric', 'liverpool', '.']\n",
            "    After Regex: ['exhibition', 'explores', 'challenges', 'commemorating', 'ongoing', 'contribution', 'people', 'african', 'diaspora', 'history', 'culture', 'fabric', 'liverpool']\n",
            "    After Lemmatization: ['exhibition', 'explores', 'challenge', 'commemorating', 'ongoing', 'contribution', 'people', 'african', 'diaspora', 'history', 'culture', 'fabric', 'liverpool']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9886\n",
            "    After Stopword Removal: ['endesa', 'propose', 'building', '430', 'mw', 'combined', 'cycle', 'gas', 'turbine', '(', 'ccgt', ')', 'gas', 'fired', 'plant', 'site', '.']\n",
            "    After Regex: ['endesa', 'propose', 'building', 'mw', 'combined', 'cycle', 'gas', 'turbine', 'ccgt', 'gas', 'fired', 'plant', 'site']\n",
            "    After Lemmatization: ['endesa', 'propose', 'building', 'mw', 'combined', 'cycle', 'gas', 'turbine', 'ccgt', 'gas', 'fired', 'plant', 'site']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9887\n",
            "    After Stopword Removal: ['first', 'major', 'piece', 'social', 'legislation', 'within', 'memory', 'pass', 'congress', 'without', 'single', 'vote', 'opposition', 'party', '.']\n",
            "    After Regex: ['first', 'major', 'piece', 'social', 'legislation', 'within', 'memory', 'pass', 'congress', 'without', 'single', 'vote', 'opposition', 'party']\n",
            "    After Lemmatization: ['first', 'major', 'piece', 'social', 'legislation', 'within', 'memory', 'pas', 'congress', 'without', 'single', 'vote', 'opposition', 'party']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9888\n",
            "    After Stopword Removal: ['centuries', 'ago', 'believed', 'beads', 'worn', 'armlets', 'bring', \"'\", 'good', 'luck', \"'\", 'hunters', '.']\n",
            "    After Regex: ['centuries', 'ago', 'believed', 'beads', 'worn', 'armlets', 'bring', 'good', 'luck', 'hunters']\n",
            "    After Lemmatization: ['century', 'ago', 'believed', 'bead', 'worn', 'armlet', 'bring', 'good', 'luck', 'hunter']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9889\n",
            "    After Stopword Removal: ['kentucky', 'department', 'education', 'must', 'change', 'meet', 'demands', 'future', '.']\n",
            "    After Regex: ['kentucky', 'department', 'education', 'must', 'change', 'meet', 'demands', 'future']\n",
            "    After Lemmatization: ['kentucky', 'department', 'education', 'must', 'change', 'meet', 'demand', 'future']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9890\n",
            "    After Stopword Removal: ['clearly', 'sharpening', 'knives', 'process', ',', '.', 'e', '.']\n",
            "    After Regex: ['clearly', 'sharpening', 'knives', 'process', 'e']\n",
            "    After Lemmatization: ['clearly', 'sharpening', 'knife', 'process', 'e']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9891\n",
            "    After Stopword Removal: ['demand', 'residential', 'real', 'estate', 'high', 'canada', \"'\", 'major', 'cities', ',', 'national', 'association', 'realtors', 'predicting', '3.2', 'percent', 'rise', 'home', 'prices', '2018', 'back', \"2017'\", '5.5', 'percent', 'increase', '.']\n",
            "    After Regex: ['demand', 'residential', 'real', 'estate', 'high', 'canada', 'major', 'cities', 'national', 'association', 'realtors', 'predicting', 'percent', 'rise', 'home', 'prices', 'back', 'percent', 'increase']\n",
            "    After Lemmatization: ['demand', 'residential', 'real', 'estate', 'high', 'canada', 'major', 'city', 'national', 'association', 'realtor', 'predicting', 'percent', 'rise', 'home', 'price', 'back', 'percent', 'increase']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9892\n",
            "    After Stopword Removal: ['fda', 'also', 'working', 'states', 'eliminate', 'unlawful', 'production', 'sale', 'raw', 'milk', 'soft', 'cheeses', '.']\n",
            "    After Regex: ['fda', 'also', 'working', 'states', 'eliminate', 'unlawful', 'production', 'sale', 'raw', 'milk', 'soft', 'cheeses']\n",
            "    After Lemmatization: ['fda', 'also', 'working', 'state', 'eliminate', 'unlawful', 'production', 'sale', 'raw', 'milk', 'soft', 'cheese']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9893\n",
            "    After Stopword Removal: ['early', '1600', 'new', 'england', ',', 'family', '6(', 'mother', ',', 'father', ',', 'baby', ',', 'young', 'twins', ',', 'eldest', 'daughter', ')', 'banished', 'church', '.']\n",
            "    After Regex: ['early', 'new', 'england', 'family', 'mother', 'father', 'baby', 'young', 'twins', 'eldest', 'daughter', 'banished', 'church']\n",
            "    After Lemmatization: ['early', 'new', 'england', 'family', 'mother', 'father', 'baby', 'young', 'twin', 'eldest', 'daughter', 'banished', 'church']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9894\n",
            "    After Stopword Removal: ['together', 'u', '.', '.', 'economist', 'david', 'gale', ',', 'developed', 'mathematical', 'formula', '10', 'men', '10', 'women', 'could', 'coupled', 'way', 'one', 'would', 'benefit', 'trading', 'partners', '.']\n",
            "    After Regex: ['together', 'u', 'economist', 'david', 'gale', 'developed', 'mathematical', 'formula', 'men', 'women', 'could', 'coupled', 'way', 'one', 'would', 'benefit', 'trading', 'partners']\n",
            "    After Lemmatization: ['together', 'u', 'economist', 'david', 'gale', 'developed', 'mathematical', 'formula', 'men', 'woman', 'could', 'coupled', 'way', 'one', 'would', 'benefit', 'trading', 'partner']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9895\n",
            "    After Stopword Removal: ['dollar', 'terms', ',', 'u', '.', '.', 'household', 'debt', 'still', 'climbing', '.']\n",
            "    After Regex: ['dollar', 'terms', 'u', 'household', 'debt', 'still', 'climbing']\n",
            "    After Lemmatization: ['dollar', 'term', 'u', 'household', 'debt', 'still', 'climbing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9896\n",
            "    After Stopword Removal: ['indian', 'society', 'divided', 'along', 'various', 'called', 'pure', 'impure', 'castes', '.']\n",
            "    After Regex: ['indian', 'society', 'divided', 'along', 'various', 'called', 'pure', 'impure', 'castes']\n",
            "    After Lemmatization: ['indian', 'society', 'divided', 'along', 'various', 'called', 'pure', 'impure', 'caste']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9897\n",
            "    After Stopword Removal: ['consider', 'fact', 'areas', 'highest', 'population', 'density', 'prosperous', 'often', 'hospitable', '.']\n",
            "    After Regex: ['consider', 'fact', 'areas', 'highest', 'population', 'density', 'prosperous', 'often', 'hospitable']\n",
            "    After Lemmatization: ['consider', 'fact', 'area', 'highest', 'population', 'density', 'prosperous', 'often', 'hospitable']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9898\n",
            "    After Stopword Removal: ['dissolve', 'salt', '3', 'pints', 'water', ',', 'pour', 'vegetables', '.']\n",
            "    After Regex: ['dissolve', 'salt', 'pints', 'water', 'pour', 'vegetables']\n",
            "    After Lemmatization: ['dissolve', 'salt', 'pint', 'water', 'pour', 'vegetable']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9899\n",
            "    After Stopword Removal: ['four', 'days', 'later', ',', 'one', 'destroyers', 'escorting', 'battleships', 'gneisenau', 'scharnhorst', 'north', 'sea', 'break', 'north', 'atlantic', '.']\n",
            "    After Regex: ['four', 'days', 'later', 'one', 'destroyers', 'escorting', 'battleships', 'gneisenau', 'scharnhorst', 'north', 'sea', 'break', 'north', 'atlantic']\n",
            "    After Lemmatization: ['four', 'day', 'later', 'one', 'destroyer', 'escorting', 'battleship', 'gneisenau', 'scharnhorst', 'north', 'sea', 'break', 'north', 'atlantic']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9900\n",
            "    After Stopword Removal: ['benefits', 'rolfing', '®', 'include', ',', 'improved', 'posture', ',', 'improved', 'flexibility', 'range', 'movement', ',', 'enhancing', 'ease', 'breathing', ',', 'releasing', 'painful', 'holding', 'patterns', 'managing', 'acute', 'chronic', 'pain', 'past', 'injuries', '.']\n",
            "    After Regex: ['benefits', 'rolfing', 'include', 'improved', 'posture', 'improved', 'flexibility', 'range', 'movement', 'enhancing', 'ease', 'breathing', 'releasing', 'painful', 'holding', 'patterns', 'managing', 'acute', 'chronic', 'pain', 'past', 'injuries']\n",
            "    After Lemmatization: ['benefit', 'rolfing', 'include', 'improved', 'posture', 'improved', 'flexibility', 'range', 'movement', 'enhancing', 'ease', 'breathing', 'releasing', 'painful', 'holding', 'pattern', 'managing', 'acute', 'chronic', 'pain', 'past', 'injury']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9901\n",
            "    After Stopword Removal: ['exercise', 'least', '30', 'minutes', 'per', 'day', '.']\n",
            "    After Regex: ['exercise', 'least', 'minutes', 'per', 'day']\n",
            "    After Lemmatization: ['exercise', 'least', 'minute', 'per', 'day']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9902\n",
            "    After Stopword Removal: ['historically', 'believed', 'thermal', 'applications', 'third', 'price', 'electricity', '-', 'based', 'heating', 'applications', ',', 'based', '$2', 'per', 'thousand', 'cubic', '-', 'foot', 'natural', 'gas', 'whatever', 'prevailing', 'price', 'electricity', '.']\n",
            "    After Regex: ['historically', 'believed', 'thermal', 'applications', 'third', 'price', 'electricity', 'based', 'heating', 'applications', 'based', 'per', 'thousand', 'cubic', 'foot', 'natural', 'gas', 'whatever', 'prevailing', 'price', 'electricity']\n",
            "    After Lemmatization: ['historically', 'believed', 'thermal', 'application', 'third', 'price', 'electricity', 'based', 'heating', 'application', 'based', 'per', 'thousand', 'cubic', 'foot', 'natural', 'gas', 'whatever', 'prevailing', 'price', 'electricity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9903\n",
            "    After Stopword Removal: ['work', 'transported', 'one', 'distinct', 'network', 'another', 'distinct', 'network', ',', 'via', 'laptop', 'requires', 'network', 'licensed', 'properly', '.']\n",
            "    After Regex: ['work', 'transported', 'one', 'distinct', 'network', 'another', 'distinct', 'network', 'via', 'laptop', 'requires', 'network', 'licensed', 'properly']\n",
            "    After Lemmatization: ['work', 'transported', 'one', 'distinct', 'network', 'another', 'distinct', 'network', 'via', 'laptop', 'requires', 'network', 'licensed', 'properly']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9904\n",
            "    After Stopword Removal: ['event', 'lose', 'personal', 'data', 'quickly', 'restored', 'mouse', 'clicks', 'even', 'windows', 'longer', 'able', 'start', '.']\n",
            "    After Regex: ['event', 'lose', 'personal', 'data', 'quickly', 'restored', 'mouse', 'clicks', 'even', 'windows', 'longer', 'able', 'start']\n",
            "    After Lemmatization: ['event', 'lose', 'personal', 'data', 'quickly', 'restored', 'mouse', 'click', 'even', 'window', 'longer', 'able', 'start']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9905\n",
            "    After Stopword Removal: ['change', 'notes', 'symphony', ',', 'create', 'chords', 'happiness', ',', 'sadness', ',', 'vitality', ',', 'calmness', '.']\n",
            "    After Regex: ['change', 'notes', 'symphony', 'create', 'chords', 'happiness', 'sadness', 'vitality', 'calmness']\n",
            "    After Lemmatization: ['change', 'note', 'symphony', 'create', 'chord', 'happiness', 'sadness', 'vitality', 'calmness']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9906\n",
            "    After Stopword Removal: ['saint', 'three', 'daughters', 'testimony', 'earlier', 'life', 'better', 'renaissance', 'prelates', ';', ',', 'although', 'relations', 'high', 'places', ',', 'charge', 'nepotism', 'never', 'levelled', '.']\n",
            "    After Regex: ['saint', 'three', 'daughters', 'testimony', 'earlier', 'life', 'better', 'renaissance', 'prelates', 'although', 'relations', 'high', 'places', 'charge', 'nepotism', 'never', 'levelled']\n",
            "    After Lemmatization: ['saint', 'three', 'daughter', 'testimony', 'earlier', 'life', 'better', 'renaissance', 'prelate', 'although', 'relation', 'high', 'place', 'charge', 'nepotism', 'never', 'levelled']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9907\n",
            "    After Stopword Removal: ['advanced', 'system', 'vibration', 'damping', ',', 'less', 'tired', 'hands', '.']\n",
            "    After Regex: ['advanced', 'system', 'vibration', 'damping', 'less', 'tired', 'hands']\n",
            "    After Lemmatization: ['advanced', 'system', 'vibration', 'damping', 'less', 'tired', 'hand']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9908\n",
            "    After Stopword Removal: ['although', 'pope', 'clement', 'vi', 'tried', 'protect', 'july', '6,1348,', 'papal', 'bull', 'additional', 'bull', '1348,', 'several', 'months', 'later', ',900', 'jews', 'burnt', 'alive', 'strasbourg', ',', 'plague', \"'\", 'yet', 'affected', 'city', '.']\n",
            "    After Regex: ['although', 'pope', 'clement', 'vi', 'tried', 'protect', 'july', 'papal', 'bull', 'additional', 'bull', 'several', 'months', 'later', 'jews', 'burnt', 'alive', 'strasbourg', 'plague', 'yet', 'affected', 'city']\n",
            "    After Lemmatization: ['although', 'pope', 'clement', 'vi', 'tried', 'protect', 'july', 'papal', 'bull', 'additional', 'bull', 'several', 'month', 'later', 'jew', 'burnt', 'alive', 'strasbourg', 'plague', 'yet', 'affected', 'city']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9909\n",
            "    After Stopword Removal: ['becoming', 'healthier', 'benefit', 'consumer', '.']\n",
            "    After Regex: ['becoming', 'healthier', 'benefit', 'consumer']\n",
            "    After Lemmatization: ['becoming', 'healthier', 'benefit', 'consumer']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9910\n",
            "    After Stopword Removal: ['link', 'article', 'doi', ':10.1111/', 'j', '.1471-8286.2004.00616.', 'x', 'individual', 'identification', 'via', 'dna', 'profiling', 'important', 'molecular', 'ecology', ',', 'particularly', 'case', 'noninvasive', 'sampling', '.']\n",
            "    After Regex: ['link', 'article', 'doi', 'j', 'x', 'individual', 'identification', 'via', 'dna', 'profiling', 'important', 'molecular', 'ecology', 'particularly', 'case', 'noninvasive', 'sampling']\n",
            "    After Lemmatization: ['link', 'article', 'doi', 'j', 'x', 'individual', 'identification', 'via', 'dna', 'profiling', 'important', 'molecular', 'ecology', 'particularly', 'case', 'noninvasive', 'sampling']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9911\n",
            "    After Stopword Removal: ['specific', 'excise', 'tax', 'monetary', 'tax', 'levied', 'based', 'quantity', 'tobacco', 'products', '(', 'eg', ',', 'per', 'pack', 'weight', ').']\n",
            "    After Regex: ['specific', 'excise', 'tax', 'monetary', 'tax', 'levied', 'based', 'quantity', 'tobacco', 'products', 'eg', 'per', 'pack', 'weight']\n",
            "    After Lemmatization: ['specific', 'excise', 'tax', 'monetary', 'tax', 'levied', 'based', 'quantity', 'tobacco', 'product', 'eg', 'per', 'pack', 'weight']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9912\n",
            "    After Stopword Removal: ['crucial', 'patient', 'inform', 'doctor', '/', 'nurses', 'caregivers', '-', 'dose', 'adjustments', 'made', 'promptly', 'side', 'effects', 'reversible', 'rather', 'become', 'ongoing', ',', 'chronic', 'worsening', 'problems', '.']\n",
            "    After Regex: ['crucial', 'patient', 'inform', 'doctor', 'nurses', 'caregivers', 'dose', 'adjustments', 'made', 'promptly', 'side', 'effects', 'reversible', 'rather', 'become', 'ongoing', 'chronic', 'worsening', 'problems']\n",
            "    After Lemmatization: ['crucial', 'patient', 'inform', 'doctor', 'nurse', 'caregiver', 'dose', 'adjustment', 'made', 'promptly', 'side', 'effect', 'reversible', 'rather', 'become', 'ongoing', 'chronic', 'worsening', 'problem']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9913\n",
            "    After Stopword Removal: ['consider', 'fasting', 'nourishment', 'soul', '.']\n",
            "    After Regex: ['consider', 'fasting', 'nourishment', 'soul']\n",
            "    After Lemmatization: ['consider', 'fasting', 'nourishment', 'soul']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9914\n",
            "    After Stopword Removal: ['patterns', 'projected', 'onto', 'monumental', 'surface', 'sculpture', ',', 'proceed', 'interact', 'one', 'another', ',', 'creating', 'ripple', 'effects', 'see', '.']\n",
            "    After Regex: ['patterns', 'projected', 'onto', 'monumental', 'surface', 'sculpture', 'proceed', 'interact', 'one', 'another', 'creating', 'ripple', 'effects', 'see']\n",
            "    After Lemmatization: ['pattern', 'projected', 'onto', 'monumental', 'surface', 'sculpture', 'proceed', 'interact', 'one', 'another', 'creating', 'ripple', 'effect', 'see']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9915\n",
            "    After Stopword Removal: ['clearly', ',', 'quality', 'food', 'matters', ',', 'quantity', '.']\n",
            "    After Regex: ['clearly', 'quality', 'food', 'matters', 'quantity']\n",
            "    After Lemmatization: ['clearly', 'quality', 'food', 'matter', 'quantity']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9916\n",
            "    After Stopword Removal: ['programme', 'shows', 'compilation', 'dozens', 'crime', 'stories', 'involving', 'organized', 'crime', ',', 'violence', 'children', 'women', ',', 'drug', 'dealers', ',', 'war', 'crimes', ',', 'corruption', ',', 'among', 'others', '.']\n",
            "    After Regex: ['programme', 'shows', 'compilation', 'dozens', 'crime', 'stories', 'involving', 'organized', 'crime', 'violence', 'children', 'women', 'drug', 'dealers', 'war', 'crimes', 'corruption', 'among', 'others']\n",
            "    After Lemmatization: ['programme', 'show', 'compilation', 'dozen', 'crime', 'story', 'involving', 'organized', 'crime', 'violence', 'child', 'woman', 'drug', 'dealer', 'war', 'crime', 'corruption', 'among', 'others']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9917\n",
            "    After Stopword Removal: ['effective', 'communication', 'teachers', ',', 'parents', 'students', '.']\n",
            "    After Regex: ['effective', 'communication', 'teachers', 'parents', 'students']\n",
            "    After Lemmatization: ['effective', 'communication', 'teacher', 'parent', 'student']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9918\n",
            "    After Stopword Removal: ['women', 'wanting', 'use', 'methods', 'needed', 'referred', 'another', 'facility', ',', 'may', 'impediment', 'uptake', '.']\n",
            "    After Regex: ['women', 'wanting', 'use', 'methods', 'needed', 'referred', 'another', 'facility', 'may', 'impediment', 'uptake']\n",
            "    After Lemmatization: ['woman', 'wanting', 'use', 'method', 'needed', 'referred', 'another', 'facility', 'may', 'impediment', 'uptake']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9919\n",
            "    After Stopword Removal: ['example', ',', 'consider', 'long', 'swathe', 'park', 'via', 'wegman', 'lecture', ',', 'recieved', 'trivial', 'changes', ':', 'additive', 'color', 'processes', ',', 'television', ',', 'work', 'capability', 'generate', 'image', 'composed', 'red', ',', 'green', ',', 'blue', 'light', '.']\n",
            "    After Regex: ['example', 'consider', 'long', 'swathe', 'park', 'via', 'wegman', 'lecture', 'recieved', 'trivial', 'changes', 'additive', 'color', 'processes', 'television', 'work', 'capability', 'generate', 'image', 'composed', 'red', 'green', 'blue', 'light']\n",
            "    After Lemmatization: ['example', 'consider', 'long', 'swathe', 'park', 'via', 'wegman', 'lecture', 'recieved', 'trivial', 'change', 'additive', 'color', 'process', 'television', 'work', 'capability', 'generate', 'image', 'composed', 'red', 'green', 'blue', 'light']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9920\n",
            "    After Stopword Removal: ['still', 'mine', 'buildings', ',', 'scar', 'mine', 'works', 'carved', 'hillside', '.']\n",
            "    After Regex: ['still', 'mine', 'buildings', 'scar', 'mine', 'works', 'carved', 'hillside']\n",
            "    After Lemmatization: ['still', 'mine', 'building', 'scar', 'mine', 'work', 'carved', 'hillside']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9921\n",
            "    After Stopword Removal: ['also', 'significant', 'changes', 'world', 'political', 'economic', 'environment', '.']\n",
            "    After Regex: ['also', 'significant', 'changes', 'world', 'political', 'economic', 'environment']\n",
            "    After Lemmatization: ['also', 'significant', 'change', 'world', 'political', 'economic', 'environment']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9922\n",
            "    After Stopword Removal: ['contrast', 'industry', 'disadvantages', 'include', 'scarcity', 'resources', 'clash', 'culture', 'education', 'institutions', 'industry', 'bodies', '.']\n",
            "    After Regex: ['contrast', 'industry', 'disadvantages', 'include', 'scarcity', 'resources', 'clash', 'culture', 'education', 'institutions', 'industry', 'bodies']\n",
            "    After Lemmatization: ['contrast', 'industry', 'disadvantage', 'include', 'scarcity', 'resource', 'clash', 'culture', 'education', 'institution', 'industry', 'body']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9923\n",
            "    After Stopword Removal: ['upgrading', 'trajectories', 'rely', 'self', '-', 'financing', 'reduce', 'risk', 'levels', 'increase', 'ability', 'poor', 'populations', 'participate', '.']\n",
            "    After Regex: ['upgrading', 'trajectories', 'rely', 'self', 'financing', 'reduce', 'risk', 'levels', 'increase', 'ability', 'poor', 'populations', 'participate']\n",
            "    After Lemmatization: ['upgrading', 'trajectory', 'rely', 'self', 'financing', 'reduce', 'risk', 'level', 'increase', 'ability', 'poor', 'population', 'participate']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9924\n",
            "    After Stopword Removal: ['improved', 'management', 'memory', 'improves', 'performance', 'multiple', 'vlm', 'applications', 'run', 'single', 'system', '.']\n",
            "    After Regex: ['improved', 'management', 'memory', 'improves', 'performance', 'multiple', 'vlm', 'applications', 'run', 'single', 'system']\n",
            "    After Lemmatization: ['improved', 'management', 'memory', 'improves', 'performance', 'multiple', 'vlm', 'application', 'run', 'single', 'system']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9925\n",
            "    After Stopword Removal: ['teachers', 'think', 'evaluate', 'children', \"'\", 'art', 'likely', 'accurate', 'children', \"'\", 'actions', 'narratives', 'considered', '.']\n",
            "    After Regex: ['teachers', 'think', 'evaluate', 'children', 'art', 'likely', 'accurate', 'children', 'actions', 'narratives', 'considered']\n",
            "    After Lemmatization: ['teacher', 'think', 'evaluate', 'child', 'art', 'likely', 'accurate', 'child', 'action', 'narrative', 'considered']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9926\n",
            "    After Stopword Removal: ['workshops', 'everything', 'planning', 'creative', 'actions', ',', 'fighting', 'housing', 'crisis', 'austerity', ',', 'making', 'solar', 'panel', '.']\n",
            "    After Regex: ['workshops', 'everything', 'planning', 'creative', 'actions', 'fighting', 'housing', 'crisis', 'austerity', 'making', 'solar', 'panel']\n",
            "    After Lemmatization: ['workshop', 'everything', 'planning', 'creative', 'action', 'fighting', 'housing', 'crisis', 'austerity', 'making', 'solar', 'panel']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9927\n",
            "    After Stopword Removal: ['use', 'metal', 'glass', 'rodent', '-', 'proof', 'containers', 'store', 'seeds', 'bird', 'feed', '.']\n",
            "    After Regex: ['use', 'metal', 'glass', 'rodent', 'proof', 'containers', 'store', 'seeds', 'bird', 'feed']\n",
            "    After Lemmatization: ['use', 'metal', 'glass', 'rodent', 'proof', 'container', 'store', 'seed', 'bird', 'feed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9928\n",
            "    After Stopword Removal: ['circa', 'late', '1700', '-', 'early', '1800', '.']\n",
            "    After Regex: ['circa', 'late', 'early']\n",
            "    After Lemmatization: ['circa', 'late', 'early']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9929\n",
            "    After Stopword Removal: ['one', 'benefit', 'spreading', 'tax', 'levy', 'several', 'future', 'years', 'larger', 'tax', 'base', 'would', 'paying', 'hospital', 'brampton', 'continues', 'grow', 'every', 'year', '.']\n",
            "    After Regex: ['one', 'benefit', 'spreading', 'tax', 'levy', 'several', 'future', 'years', 'larger', 'tax', 'base', 'would', 'paying', 'hospital', 'brampton', 'continues', 'grow', 'every', 'year']\n",
            "    After Lemmatization: ['one', 'benefit', 'spreading', 'tax', 'levy', 'several', 'future', 'year', 'larger', 'tax', 'base', 'would', 'paying', 'hospital', 'brampton', 'continues', 'grow', 'every', 'year']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9930\n",
            "    After Stopword Removal: ['building', 'upon', 'local', 'global', 'geopolitical', 'perspectives', ',', 'works', 'constructed', 'dreamworlds', ',', 'expressions', 'utopian', 'desires', 'transform', 'world', ',', 'backdrop', 'art', \"'\", 'tendency', 'toward', 'new', 'modes', 'production', 'aesthetic', 'sensibilities', '.']\n",
            "    After Regex: ['building', 'upon', 'local', 'global', 'geopolitical', 'perspectives', 'works', 'constructed', 'dreamworlds', 'expressions', 'utopian', 'desires', 'transform', 'world', 'backdrop', 'art', 'tendency', 'toward', 'new', 'modes', 'production', 'aesthetic', 'sensibilities']\n",
            "    After Lemmatization: ['building', 'upon', 'local', 'global', 'geopolitical', 'perspective', 'work', 'constructed', 'dreamworld', 'expression', 'utopian', 'desire', 'transform', 'world', 'backdrop', 'art', 'tendency', 'toward', 'new', 'mode', 'production', 'aesthetic', 'sensibility']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9931\n",
            "    After Stopword Removal: ['data', 'reviewed', 'last', 'month', 'team', 'included', 'nasa', 'astronauts', ',', 'u', '.', '.', 'air', 'force', 'officers', ',', 'representatives', 'commercial', 'aerospace', 'companies', '.']\n",
            "    After Regex: ['data', 'reviewed', 'last', 'month', 'team', 'included', 'nasa', 'astronauts', 'u', 'air', 'force', 'officers', 'representatives', 'commercial', 'aerospace', 'companies']\n",
            "    After Lemmatization: ['data', 'reviewed', 'last', 'month', 'team', 'included', 'nasa', 'astronaut', 'u', 'air', 'force', 'officer', 'representative', 'commercial', 'aerospace', 'company']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9932\n",
            "    After Stopword Removal: ['however', 'argentine', 'permanent', 'assembly', 'human', 'rights', 'stated', 'link', 'proven', 'pope', 'francis', 'argentine', 'military', 'junta', 'time', '.']\n",
            "    After Regex: ['however', 'argentine', 'permanent', 'assembly', 'human', 'rights', 'stated', 'link', 'proven', 'pope', 'francis', 'argentine', 'military', 'junta', 'time']\n",
            "    After Lemmatization: ['however', 'argentine', 'permanent', 'assembly', 'human', 'right', 'stated', 'link', 'proven', 'pope', 'francis', 'argentine', 'military', 'junta', 'time']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9933\n",
            "    After Stopword Removal: ['wine', 'drunk', 'glasses', 'rather', 'teacups', 'silver', 'goblets', 'glass', 'inert', ',', 'relatively', 'thin', 'allows', 'full', 'appreciation', 'wine', \"'\", 'appearance', '.']\n",
            "    After Regex: ['wine', 'drunk', 'glasses', 'rather', 'teacups', 'silver', 'goblets', 'glass', 'inert', 'relatively', 'thin', 'allows', 'full', 'appreciation', 'wine', 'appearance']\n",
            "    After Lemmatization: ['wine', 'drunk', 'glass', 'rather', 'teacup', 'silver', 'goblet', 'glass', 'inert', 'relatively', 'thin', 'allows', 'full', 'appreciation', 'wine', 'appearance']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9934\n",
            "    After Stopword Removal: ['growth', 'developing', 'economies', 'remain', 'solid', ',', 'world', 'bank', 'expects', 'activity', 'developed', 'countries', 'remain', 'subdued', '.']\n",
            "    After Regex: ['growth', 'developing', 'economies', 'remain', 'solid', 'world', 'bank', 'expects', 'activity', 'developed', 'countries', 'remain', 'subdued']\n",
            "    After Lemmatization: ['growth', 'developing', 'economy', 'remain', 'solid', 'world', 'bank', 'expects', 'activity', 'developed', 'country', 'remain', 'subdued']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9935\n",
            "    After Stopword Removal: ['concerned', 'notions', 'decay', 'loss', ',', 'try', 'counter', 'things', '.']\n",
            "    After Regex: ['concerned', 'notions', 'decay', 'loss', 'try', 'counter', 'things']\n",
            "    After Lemmatization: ['concerned', 'notion', 'decay', 'loss', 'try', 'counter', 'thing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9936\n",
            "    After Stopword Removal: [\"'\", 'natural', ',', 'non', '-', 'invasive', 'highly', 'effective', 'method', 'getting', 'patients', 'pain', 'quickly', '.']\n",
            "    After Regex: ['natural', 'non', 'invasive', 'highly', 'effective', 'method', 'getting', 'patients', 'pain', 'quickly']\n",
            "    After Lemmatization: ['natural', 'non', 'invasive', 'highly', 'effective', 'method', 'getting', 'patient', 'pain', 'quickly']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9937\n",
            "    After Stopword Removal: ['dwelling', 'along', 'things', ',', 'sharing', 'common', 'context', ',', 'part', 'means', 'inhabit', 'world', '.']\n",
            "    After Regex: ['dwelling', 'along', 'things', 'sharing', 'common', 'context', 'part', 'means', 'inhabit', 'world']\n",
            "    After Lemmatization: ['dwelling', 'along', 'thing', 'sharing', 'common', 'context', 'part', 'mean', 'inhabit', 'world']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9938\n",
            "    After Stopword Removal: ['vision', 'one', 'family', ',', 'compassion', 'inclusion', '.']\n",
            "    After Regex: ['vision', 'one', 'family', 'compassion', 'inclusion']\n",
            "    After Lemmatization: ['vision', 'one', 'family', 'compassion', 'inclusion']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9939\n",
            "    After Stopword Removal: ['source', 'eternal', 'nature', 'humans', 'universe', 'otherwise', 'temporary', 'accidental', '?']\n",
            "    After Regex: ['source', 'eternal', 'nature', 'humans', 'universe', 'otherwise', 'temporary', 'accidental']\n",
            "    After Lemmatization: ['source', 'eternal', 'nature', 'human', 'universe', 'otherwise', 'temporary', 'accidental']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9940\n",
            "    After Stopword Removal: ['therefore', ',', 'potential', 'meet', 'growing', 'demand', 'food', 'renewable', 'fuel', '.']\n",
            "    After Regex: ['therefore', 'potential', 'meet', 'growing', 'demand', 'food', 'renewable', 'fuel']\n",
            "    After Lemmatization: ['therefore', 'potential', 'meet', 'growing', 'demand', 'food', 'renewable', 'fuel']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9941\n",
            "    After Stopword Removal: ['consists', 'two', 'budget', 'accounts', ',', 'one', 'reconstruction', 'one', 'prevention', '.']\n",
            "    After Regex: ['consists', 'two', 'budget', 'accounts', 'one', 'reconstruction', 'one', 'prevention']\n",
            "    After Lemmatization: ['consists', 'two', 'budget', 'account', 'one', 'reconstruction', 'one', 'prevention']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9942\n",
            "    After Stopword Removal: ['remedial', 'action', 'faulty', 'ground', 'straightforward', '—', 'fix', 'ground', '.']\n",
            "    After Regex: ['remedial', 'action', 'faulty', 'ground', 'straightforward', 'fix', 'ground']\n",
            "    After Lemmatization: ['remedial', 'action', 'faulty', 'ground', 'straightforward', 'fix', 'ground']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9943\n",
            "    After Stopword Removal: ['absent', 'significant', 'snowpack', ',', 'reservoirs', 'lowered', 'amount', 'equal', 'runoff', 'would', 'result', 'one', '-', 'inch', 'rainfall', 'taking', 'place', 'six', '-', 'hour', 'period', '.']\n",
            "    After Regex: ['absent', 'significant', 'snowpack', 'reservoirs', 'lowered', 'amount', 'equal', 'runoff', 'would', 'result', 'one', 'inch', 'rainfall', 'taking', 'place', 'six', 'hour', 'period']\n",
            "    After Lemmatization: ['absent', 'significant', 'snowpack', 'reservoir', 'lowered', 'amount', 'equal', 'runoff', 'would', 'result', 'one', 'inch', 'rainfall', 'taking', 'place', 'six', 'hour', 'period']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9944\n",
            "    After Stopword Removal: ['time', ',', 'european', 'population', 'ageing', 'one', 'poses', 'serious', 'public', 'health', ',', 'social', 'thus', 'economic', 'challenges', '.']\n",
            "    After Regex: ['time', 'european', 'population', 'ageing', 'one', 'poses', 'serious', 'public', 'health', 'social', 'thus', 'economic', 'challenges']\n",
            "    After Lemmatization: ['time', 'european', 'population', 'ageing', 'one', 'pose', 'serious', 'public', 'health', 'social', 'thus', 'economic', 'challenge']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9945\n",
            "    After Stopword Removal: ['student', 'rights', 'ferpa', '-', 'students', 'right', 'inspect', 'review', 'everything', 'record', ',', 'must', 'respond', 'requests', 'within', '45', 'days', '.']\n",
            "    After Regex: ['student', 'rights', 'ferpa', 'students', 'right', 'inspect', 'review', 'everything', 'record', 'must', 'respond', 'requests', 'within', 'days']\n",
            "    After Lemmatization: ['student', 'right', 'ferpa', 'student', 'right', 'inspect', 'review', 'everything', 'record', 'must', 'respond', 'request', 'within', 'day']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9946\n",
            "    After Stopword Removal: ['report', 'family', 'attitudes', 'pathways', 'barriers', 'genetic', 'testing', 'screening', ':', 'molecular', 'genetics', 'meets', 'high', '-', 'risk', 'family', '(1997),', 'troy', 'duster', '(', 'university', 'california', ',', 'berkeley', ')', 'diane', 'beeson', '(', 'california', 'state', 'university', ',', 'hayward', ').']\n",
            "    After Regex: ['report', 'family', 'attitudes', 'pathways', 'barriers', 'genetic', 'testing', 'screening', 'molecular', 'genetics', 'meets', 'high', 'risk', 'family', 'troy', 'duster', 'university', 'california', 'berkeley', 'diane', 'beeson', 'california', 'state', 'university', 'hayward']\n",
            "    After Lemmatization: ['report', 'family', 'attitude', 'pathway', 'barrier', 'genetic', 'testing', 'screening', 'molecular', 'genetics', 'meet', 'high', 'risk', 'family', 'troy', 'duster', 'university', 'california', 'berkeley', 'diane', 'beeson', 'california', 'state', 'university', 'hayward']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9947\n",
            "    After Stopword Removal: ['research', 'primarily', 'funded', 'european', 'research', 'council', ',', 'national', 'institute', 'health', 'research', 'cambridge', 'biomedical', 'research', 'centre', 'evelyn', 'trust', '.']\n",
            "    After Regex: ['research', 'primarily', 'funded', 'european', 'research', 'council', 'national', 'institute', 'health', 'research', 'cambridge', 'biomedical', 'research', 'centre', 'evelyn', 'trust']\n",
            "    After Lemmatization: ['research', 'primarily', 'funded', 'european', 'research', 'council', 'national', 'institute', 'health', 'research', 'cambridge', 'biomedical', 'research', 'centre', 'evelyn', 'trust']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9948\n",
            "    After Stopword Removal: ['ambassador', 'osce', 'madrid', 'conference', 'yuri', 'orlov', ',', 'founder', 'moscow', 'helsinki', 'group', 'commission', 'security', 'cooperation', 'europe', ',', 'also', 'known', 'u', '.', '.', 'helsinki', 'commission', ',', 'independent', 'agency', 'federal', 'government', 'charged', 'monitoring', 'compliance', 'helsinki', 'accords', 'advancing', 'comprehensive', 'security', 'promotion', 'human', 'rights', ',', 'democracy', ',', 'economic', ',', 'environmental', 'military', 'cooperation', '56', 'countries', '.']\n",
            "    After Regex: ['ambassador', 'osce', 'madrid', 'conference', 'yuri', 'orlov', 'founder', 'moscow', 'helsinki', 'group', 'commission', 'security', 'cooperation', 'europe', 'also', 'known', 'u', 'helsinki', 'commission', 'independent', 'agency', 'federal', 'government', 'charged', 'monitoring', 'compliance', 'helsinki', 'accords', 'advancing', 'comprehensive', 'security', 'promotion', 'human', 'rights', 'democracy', 'economic', 'environmental', 'military', 'cooperation', 'countries']\n",
            "    After Lemmatization: ['ambassador', 'osce', 'madrid', 'conference', 'yuri', 'orlov', 'founder', 'moscow', 'helsinki', 'group', 'commission', 'security', 'cooperation', 'europe', 'also', 'known', 'u', 'helsinki', 'commission', 'independent', 'agency', 'federal', 'government', 'charged', 'monitoring', 'compliance', 'helsinki', 'accord', 'advancing', 'comprehensive', 'security', 'promotion', 'human', 'right', 'democracy', 'economic', 'environmental', 'military', 'cooperation', 'country']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9949\n",
            "    After Stopword Removal: [',', 'instead', ',', 'something', 'truly', 'new', 'unpredicted', 'went', '—', 'even', 'increase', 'mass', 'metabolically', 'expensive', 'brain', 'tissue', 'seems', 'independently', 'characterized', 'several', 'lineages', 'within', 'genus', 'homo', ',', 'though', 'clearly', 'dependent', 'development', '.']\n",
            "    After Regex: ['instead', 'something', 'truly', 'new', 'unpredicted', 'went', 'even', 'increase', 'mass', 'metabolically', 'expensive', 'brain', 'tissue', 'seems', 'independently', 'characterized', 'several', 'lineages', 'within', 'genus', 'homo', 'though', 'clearly', 'dependent', 'development']\n",
            "    After Lemmatization: ['instead', 'something', 'truly', 'new', 'unpredicted', 'went', 'even', 'increase', 'mass', 'metabolically', 'expensive', 'brain', 'tissue', 'seems', 'independently', 'characterized', 'several', 'lineage', 'within', 'genus', 'homo', 'though', 'clearly', 'dependent', 'development']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9950\n",
            "    After Stopword Removal: ['boundary', 'peripheral', 'rim', 'imbricate', 'peduncular', 'circlets', 'plates', 'sharp', 'gradual', ',', 'making', 'difficult', 'distinguish', 'two', 'parts', '.']\n",
            "    After Regex: ['boundary', 'peripheral', 'rim', 'imbricate', 'peduncular', 'circlets', 'plates', 'sharp', 'gradual', 'making', 'difficult', 'distinguish', 'two', 'parts']\n",
            "    After Lemmatization: ['boundary', 'peripheral', 'rim', 'imbricate', 'peduncular', 'circlet', 'plate', 'sharp', 'gradual', 'making', 'difficult', 'distinguish', 'two', 'part']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9951\n",
            "    After Stopword Removal: ['heard', 'professionals', 'say', 'part', 'reason', 'health', 'care', 'costs', 'much', 'government', 'involved', 'ways', \"'\", '.']\n",
            "    After Regex: ['heard', 'professionals', 'say', 'part', 'reason', 'health', 'care', 'costs', 'much', 'government', 'involved', 'ways']\n",
            "    After Lemmatization: ['heard', 'professional', 'say', 'part', 'reason', 'health', 'care', 'cost', 'much', 'government', 'involved', 'way']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9952\n",
            "    After Stopword Removal: ['develop', 'magnetic', 'resonance', 'imaging', '(', 'mri', ')-', 'based', 'approach', 'quantifying', 'absolute', 'fat', 'mass', 'organs', ',', 'muscles', ',', 'adipose', 'tissues', ',', 'validate', 'accuracy', 'reference', 'chemical', 'analysis', '(', 'ca', ').']\n",
            "    After Regex: ['develop', 'magnetic', 'resonance', 'imaging', 'mri', 'based', 'approach', 'quantifying', 'absolute', 'fat', 'mass', 'organs', 'muscles', 'adipose', 'tissues', 'validate', 'accuracy', 'reference', 'chemical', 'analysis', 'ca']\n",
            "    After Lemmatization: ['develop', 'magnetic', 'resonance', 'imaging', 'mri', 'based', 'approach', 'quantifying', 'absolute', 'fat', 'mass', 'organ', 'muscle', 'adipose', 'tissue', 'validate', 'accuracy', 'reference', 'chemical', 'analysis', 'ca']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9953\n",
            "    After Stopword Removal: ['1991', 'galileo', 'spacecraft', 'photographed', 'asteroid', ',', 'gaspra', '.']\n",
            "    After Regex: ['galileo', 'spacecraft', 'photographed', 'asteroid', 'gaspra']\n",
            "    After Lemmatization: ['galileo', 'spacecraft', 'photographed', 'asteroid', 'gaspra']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9954\n",
            "    After Stopword Removal: ['past', 'decades', ',', 'seen', 'important', 'public', 'health', '.']\n",
            "    After Regex: ['past', 'decades', 'seen', 'important', 'public', 'health']\n",
            "    After Lemmatization: ['past', 'decade', 'seen', 'important', 'public', 'health']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9955\n",
            "    After Stopword Removal: ['health', 'canada', 'reminds', 'people', 'keep', 'products', 'away', 'children', 'prevent', 'nicotine', 'poisoning', 'choking', '.']\n",
            "    After Regex: ['health', 'canada', 'reminds', 'people', 'keep', 'products', 'away', 'children', 'prevent', 'nicotine', 'poisoning', 'choking']\n",
            "    After Lemmatization: ['health', 'canada', 'reminds', 'people', 'keep', 'product', 'away', 'child', 'prevent', 'nicotine', 'poisoning', 'choking']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9956\n",
            "    After Stopword Removal: ['cuban', 'exile', 'community', 'potent', 'force', 'american', 'politics', '.']\n",
            "    After Regex: ['cuban', 'exile', 'community', 'potent', 'force', 'american', 'politics']\n",
            "    After Lemmatization: ['cuban', 'exile', 'community', 'potent', 'force', 'american', 'politics']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9957\n",
            "    After Stopword Removal: ['indonesia', ':', 'source', ':', 'mothers', 'provide', 'perfect', 'nourishment', 'march', '27,2012', 'infant', 'mortality', 'remains', 'high', 'indonesia', ',', 'result', 'acute', 'malnutrition', 'illness', '.']\n",
            "    After Regex: ['indonesia', 'source', 'mothers', 'provide', 'perfect', 'nourishment', 'march', 'infant', 'mortality', 'remains', 'high', 'indonesia', 'result', 'acute', 'malnutrition', 'illness']\n",
            "    After Lemmatization: ['indonesia', 'source', 'mother', 'provide', 'perfect', 'nourishment', 'march', 'infant', 'mortality', 'remains', 'high', 'indonesia', 'result', 'acute', 'malnutrition', 'illness']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9958\n",
            "    After Stopword Removal: ['state', 'trajectory', 'original', 'data', 'forecasting', 'data', 'given', 'figures', '7,8,9,', '10.']\n",
            "    After Regex: ['state', 'trajectory', 'original', 'data', 'forecasting', 'data', 'given', 'figures']\n",
            "    After Lemmatization: ['state', 'trajectory', 'original', 'data', 'forecasting', 'data', 'given', 'figure']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9959\n",
            "    After Stopword Removal: ['read', 'critics', \"'\", 'think', 'murray', 'deliberately', 'loudly', 'insulted', 'various', 'racial', 'ethnic', 'groups', 'bell', 'curve', ',', '1994', 'book', 'intelligence', 'coauthored', 'richard', 'herrnstein', ',', 'human', 'accomplishment', ',', '2003', 'volume', 'ended', 'paean', 'western', 'civilization', '.']\n",
            "    After Regex: ['read', 'critics', 'think', 'murray', 'deliberately', 'loudly', 'insulted', 'various', 'racial', 'ethnic', 'groups', 'bell', 'curve', 'book', 'intelligence', 'coauthored', 'richard', 'herrnstein', 'human', 'accomplishment', 'volume', 'ended', 'paean', 'western', 'civilization']\n",
            "    After Lemmatization: ['read', 'critic', 'think', 'murray', 'deliberately', 'loudly', 'insulted', 'various', 'racial', 'ethnic', 'group', 'bell', 'curve', 'book', 'intelligence', 'coauthored', 'richard', 'herrnstein', 'human', 'accomplishment', 'volume', 'ended', 'paean', 'western', 'civilization']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9960\n",
            "    After Stopword Removal: ['experiment', ',', 'almost', 'half', 'workers', 'worked', 'home', 'chose', 'go', 'back', 'office', 'despite', 'added', 'cost', 'commuting', '.']\n",
            "    After Regex: ['experiment', 'almost', 'half', 'workers', 'worked', 'home', 'chose', 'go', 'back', 'office', 'despite', 'added', 'cost', 'commuting']\n",
            "    After Lemmatization: ['experiment', 'almost', 'half', 'worker', 'worked', 'home', 'chose', 'go', 'back', 'office', 'despite', 'added', 'cost', 'commuting']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9961\n",
            "    After Stopword Removal: ['codex', 'alimentarius', 'commission', 'aims', 'protect', 'consumer', 'health', ',', 'ensure', 'fair', 'trade', 'practices', ',', 'promote', 'coordination', 'food', 'standards', 'work', 'undertaken', 'international', 'governmental', 'non', '-', 'governmental', 'organizations', '.']\n",
            "    After Regex: ['codex', 'alimentarius', 'commission', 'aims', 'protect', 'consumer', 'health', 'ensure', 'fair', 'trade', 'practices', 'promote', 'coordination', 'food', 'standards', 'work', 'undertaken', 'international', 'governmental', 'non', 'governmental', 'organizations']\n",
            "    After Lemmatization: ['codex', 'alimentarius', 'commission', 'aim', 'protect', 'consumer', 'health', 'ensure', 'fair', 'trade', 'practice', 'promote', 'coordination', 'food', 'standard', 'work', 'undertaken', 'international', 'governmental', 'non', 'governmental', 'organization']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9962\n",
            "    After Stopword Removal: ['prevent', 'legs', 'numb', 'sore', 'morning', '?']\n",
            "    After Regex: ['prevent', 'legs', 'numb', 'sore', 'morning']\n",
            "    After Lemmatization: ['prevent', 'leg', 'numb', 'sore', 'morning']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9963\n",
            "    After Stopword Removal: ['accelerate', 'exchange', 'research', 'results', 'within', 'network', 'facilitate', 'transfer', 'knowledge', ',', 'absorbtion', ',', 'organizations', 'harness', 'advance', 'canada', \"'\", 'economic', 'social', 'development', '.']\n",
            "    After Regex: ['accelerate', 'exchange', 'research', 'results', 'within', 'network', 'facilitate', 'transfer', 'knowledge', 'absorbtion', 'organizations', 'harness', 'advance', 'canada', 'economic', 'social', 'development']\n",
            "    After Lemmatization: ['accelerate', 'exchange', 'research', 'result', 'within', 'network', 'facilitate', 'transfer', 'knowledge', 'absorbtion', 'organization', 'harness', 'advance', 'canada', 'economic', 'social', 'development']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9964\n",
            "    After Stopword Removal: ['common', 'solution', 'given', 'audiences', 'young', 'earth', 'conventions', 'light', 'different', 'speeds', 'different', 'times', 'history', '.']\n",
            "    After Regex: ['common', 'solution', 'given', 'audiences', 'young', 'earth', 'conventions', 'light', 'different', 'speeds', 'different', 'times', 'history']\n",
            "    After Lemmatization: ['common', 'solution', 'given', 'audience', 'young', 'earth', 'convention', 'light', 'different', 'speed', 'different', 'time', 'history']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9965\n",
            "    After Stopword Removal: ['though', 'cannot', 'proven', 'lab', ',', 'known', 'fact', 'expensive', 'golf', 'ball', ',', 'greater', 'attraction', 'water', '.']\n",
            "    After Regex: ['though', 'cannot', 'proven', 'lab', 'known', 'fact', 'expensive', 'golf', 'ball', 'greater', 'attraction', 'water']\n",
            "    After Lemmatization: ['though', 'cannot', 'proven', 'lab', 'known', 'fact', 'expensive', 'golf', 'ball', 'greater', 'attraction', 'water']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9966\n",
            "    After Stopword Removal: ['france', 'shown', 'willingness', 'backstop', 'banks', ',', 'even', 'though', 'sizeable', 'exposure', 'places', 'like', 'greece', '.']\n",
            "    After Regex: ['france', 'shown', 'willingness', 'backstop', 'banks', 'even', 'though', 'sizeable', 'exposure', 'places', 'like', 'greece']\n",
            "    After Lemmatization: ['france', 'shown', 'willingness', 'backstop', 'bank', 'even', 'though', 'sizeable', 'exposure', 'place', 'like', 'greece']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9967\n",
            "    After Stopword Removal: ['tshering', \"'\", 'interrogation', 'revealed', 'animal', 'parts', 'first', 'sent', 'nepal', 'using', 'land', 'routes', 'smuggled', 'china', 'tibet', 'pharmaceuticals', 'companies', 'used', 'tiger', 'bones', 'manufacturing', 'aphrodisiac', 'medicines', '.']\n",
            "    After Regex: ['tshering', 'interrogation', 'revealed', 'animal', 'parts', 'first', 'sent', 'nepal', 'using', 'land', 'routes', 'smuggled', 'china', 'tibet', 'pharmaceuticals', 'companies', 'used', 'tiger', 'bones', 'manufacturing', 'aphrodisiac', 'medicines']\n",
            "    After Lemmatization: ['tshering', 'interrogation', 'revealed', 'animal', 'part', 'first', 'sent', 'nepal', 'using', 'land', 'route', 'smuggled', 'china', 'tibet', 'pharmaceutical', 'company', 'used', 'tiger', 'bone', 'manufacturing', 'aphrodisiac', 'medicine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9968\n",
            "    After Stopword Removal: ['government', '?']\n",
            "    After Regex: ['government']\n",
            "    After Lemmatization: ['government']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9969\n",
            "    After Stopword Removal: ['says', 'völuspá', ':', 'know', ',', 'odin', ',|', 'eye', 'third', 'root', 'ash', 'stands', 'heaven', ';', 'root', 'well', 'holy', ',', 'called', 'well', 'urdr', ';', 'gods', 'hold', 'tribunal', '.']\n",
            "    After Regex: ['says', 'vlusp', 'know', 'odin', 'eye', 'third', 'root', 'ash', 'stands', 'heaven', 'root', 'well', 'holy', 'called', 'well', 'urdr', 'gods', 'hold', 'tribunal']\n",
            "    After Lemmatization: ['say', 'vlusp', 'know', 'odin', 'eye', 'third', 'root', 'ash', 'stand', 'heaven', 'root', 'well', 'holy', 'called', 'well', 'urdr', 'god', 'hold', 'tribunal']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9970\n",
            "    After Stopword Removal: ['measuring', ':', 'save', 'measuring', 'spoons', 'syringes', 'children', \"'\", 'medicines', '.']\n",
            "    After Regex: ['measuring', 'save', 'measuring', 'spoons', 'syringes', 'children', 'medicines']\n",
            "    After Lemmatization: ['measuring', 'save', 'measuring', 'spoon', 'syrinx', 'child', 'medicine']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9971\n",
            "    After Stopword Removal: ['names', 'part', '2', 'story', 'come', '?']\n",
            "    After Regex: ['names', 'part', 'story', 'come']\n",
            "    After Lemmatization: ['name', 'part', 'story', 'come']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9972\n",
            "    After Stopword Removal: ['traditional', 'paper', '-', 'plane', 'design', 'simple', 'paper', 'craft', 'kids', 'lead', 'another', 'fun', 'activity', 'kids', ':', 'flying', 'paper', 'aeroplane', '.']\n",
            "    After Regex: ['traditional', 'paper', 'plane', 'design', 'simple', 'paper', 'craft', 'kids', 'lead', 'another', 'fun', 'activity', 'kids', 'flying', 'paper', 'aeroplane']\n",
            "    After Lemmatization: ['traditional', 'paper', 'plane', 'design', 'simple', 'paper', 'craft', 'kid', 'lead', 'another', 'fun', 'activity', 'kid', 'flying', 'paper', 'aeroplane']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9973\n",
            "    After Stopword Removal: ['especially', 'politics', ',', 'courage', 'essential', '.']\n",
            "    After Regex: ['especially', 'politics', 'courage', 'essential']\n",
            "    After Lemmatization: ['especially', 'politics', 'courage', 'essential']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9974\n",
            "    After Stopword Removal: ['diseases', 'manifest', 'misshapen', 'hemoglobin', ',', 'causing', 'anemia', ',', 'severe', ',', 'well', 'symptoms', ',', 'range', 'minor', 'life', '-', 'threatening', '.']\n",
            "    After Regex: ['diseases', 'manifest', 'misshapen', 'hemoglobin', 'causing', 'anemia', 'severe', 'well', 'symptoms', 'range', 'minor', 'life', 'threatening']\n",
            "    After Lemmatization: ['disease', 'manifest', 'misshapen', 'hemoglobin', 'causing', 'anemia', 'severe', 'well', 'symptom', 'range', 'minor', 'life', 'threatening']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9975\n",
            "    After Stopword Removal: ['museum', 'art', 'history', 'home', 'wonderful', 'collection', 'post', '-', 'war', 'modern', 'art', 'addition', 'regional', 'collection', 'pieces', 'celebrating', 'history', 'antelope', 'valley', '.']\n",
            "    After Regex: ['museum', 'art', 'history', 'home', 'wonderful', 'collection', 'post', 'war', 'modern', 'art', 'addition', 'regional', 'collection', 'pieces', 'celebrating', 'history', 'antelope', 'valley']\n",
            "    After Lemmatization: ['museum', 'art', 'history', 'home', 'wonderful', 'collection', 'post', 'war', 'modern', 'art', 'addition', 'regional', 'collection', 'piece', 'celebrating', 'history', 'antelope', 'valley']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9976\n",
            "    After Stopword Removal: ['chaos', ',', 'crisis', 'war', 'increase', 'around', 'us', ',', 'never', 'need', 'good', 'shepherd', '.']\n",
            "    After Regex: ['chaos', 'crisis', 'war', 'increase', 'around', 'us', 'never', 'need', 'good', 'shepherd']\n",
            "    After Lemmatization: ['chaos', 'crisis', 'war', 'increase', 'around', 'u', 'never', 'need', 'good', 'shepherd']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9977\n",
            "    After Stopword Removal: ['essays', 'rated', 'two', 'areas', ':', 'scientific', 'quality', 'significance', ',', 'clarity', 'style', 'writing', '.']\n",
            "    After Regex: ['essays', 'rated', 'two', 'areas', 'scientific', 'quality', 'significance', 'clarity', 'style', 'writing']\n",
            "    After Lemmatization: ['essay', 'rated', 'two', 'area', 'scientific', 'quality', 'significance', 'clarity', 'style', 'writing']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9978\n",
            "    After Stopword Removal: ['often', ',', 'symptoms', \"'\", 'recognised', 'therefore', 'young', 'people', \"'\", 'get', 'help', \"'\", 'needed', '.']\n",
            "    After Regex: ['often', 'symptoms', 'recognised', 'therefore', 'young', 'people', 'get', 'help', 'needed']\n",
            "    After Lemmatization: ['often', 'symptom', 'recognised', 'therefore', 'young', 'people', 'get', 'help', 'needed']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9979\n",
            "    After Stopword Removal: ['donations', 'used', 'purchase', 'light', 'shields', ',', 'replace', 'non', '-', 'shielded', 'fixtures', ',', 'develop', 'informational', 'materials', 'activities', 'public', '.']\n",
            "    After Regex: ['donations', 'used', 'purchase', 'light', 'shields', 'replace', 'non', 'shielded', 'fixtures', 'develop', 'informational', 'materials', 'activities', 'public']\n",
            "    After Lemmatization: ['donation', 'used', 'purchase', 'light', 'shield', 'replace', 'non', 'shielded', 'fixture', 'develop', 'informational', 'material', 'activity', 'public']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9980\n",
            "    After Stopword Removal: ['survey', 'given', 'bishops', 'active', 'adult', 'members', 'church', 'united', 'states', '.']\n",
            "    After Regex: ['survey', 'given', 'bishops', 'active', 'adult', 'members', 'church', 'united', 'states']\n",
            "    After Lemmatization: ['survey', 'given', 'bishop', 'active', 'adult', 'member', 'church', 'united', 'state']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9981\n",
            "    After Stopword Removal: ['natural', 'insect', 'tormentor', 'management', 'home', ':', 'safe', ',', 'non', '-', 'toxic', 'ways', 'dominant', 'ants', ',', 'cockroaches', ',', 'fleas', ',', 'dust', 'mites', ',', 'wasps', ',', 'flies', ',', 'mosquitoes', 'etc', '.']\n",
            "    After Regex: ['natural', 'insect', 'tormentor', 'management', 'home', 'safe', 'non', 'toxic', 'ways', 'dominant', 'ants', 'cockroaches', 'fleas', 'dust', 'mites', 'wasps', 'flies', 'mosquitoes', 'etc']\n",
            "    After Lemmatization: ['natural', 'insect', 'tormentor', 'management', 'home', 'safe', 'non', 'toxic', 'way', 'dominant', 'ant', 'cockroach', 'flea', 'dust', 'mite', 'wasp', 'fly', 'mosquito', 'etc']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9982\n",
            "    After Stopword Removal: ['cells', 'could', 'potentially', 'manipulated', 'within', 'critical', 'time', 'period', 'offering', 'innovative', 'approach', 'repair', 'sci', '.']\n",
            "    After Regex: ['cells', 'could', 'potentially', 'manipulated', 'within', 'critical', 'time', 'period', 'offering', 'innovative', 'approach', 'repair', 'sci']\n",
            "    After Lemmatization: ['cell', 'could', 'potentially', 'manipulated', 'within', 'critical', 'time', 'period', 'offering', 'innovative', 'approach', 'repair', 'sci']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9983\n",
            "    After Stopword Removal: ['soon', 'enough', ',', 'seurat', \"'\", 'geometrical', 'marks', 'would', 'brought', 'end', 'phase', 'pictorial', 'spontaneity', 'experiment', ',', 'moving', 'potential', 'expression', 'towards', 'optical', 'rules', '.']\n",
            "    After Regex: ['soon', 'enough', 'seurat', 'geometrical', 'marks', 'would', 'brought', 'end', 'phase', 'pictorial', 'spontaneity', 'experiment', 'moving', 'potential', 'expression', 'towards', 'optical', 'rules']\n",
            "    After Lemmatization: ['soon', 'enough', 'seurat', 'geometrical', 'mark', 'would', 'brought', 'end', 'phase', 'pictorial', 'spontaneity', 'experiment', 'moving', 'potential', 'expression', 'towards', 'optical', 'rule']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9984\n",
            "    After Stopword Removal: ['feature', 'story', ':', 'coastal', 'communities', 'solving', 'age', '-', 'old', 'erosion', 'problems', 'innovative', 'living', 'shoreline', 'solutions', '.']\n",
            "    After Regex: ['feature', 'story', 'coastal', 'communities', 'solving', 'age', 'old', 'erosion', 'problems', 'innovative', 'living', 'shoreline', 'solutions']\n",
            "    After Lemmatization: ['feature', 'story', 'coastal', 'community', 'solving', 'age', 'old', 'erosion', 'problem', 'innovative', 'living', 'shoreline', 'solution']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9985\n",
            "    After Stopword Removal: ['say', 'hello', 'ancient', 'relative', ',', 'pikaia', 'earliest', 'known', 'member', 'phylum', ',', 'chordates', '.']\n",
            "    After Regex: ['say', 'hello', 'ancient', 'relative', 'pikaia', 'earliest', 'known', 'member', 'phylum', 'chordates']\n",
            "    After Lemmatization: ['say', 'hello', 'ancient', 'relative', 'pikaia', 'earliest', 'known', 'member', 'phylum', 'chordate']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9986\n",
            "    After Stopword Removal: ['order', 'avoid', 'vagueness', 'characterize', 'specificity', 'particular', 'types', 'dialog', 'science', ',', '.', 'e', '.']\n",
            "    After Regex: ['order', 'avoid', 'vagueness', 'characterize', 'specificity', 'particular', 'types', 'dialog', 'science', 'e']\n",
            "    After Lemmatization: ['order', 'avoid', 'vagueness', 'characterize', 'specificity', 'particular', 'type', 'dialog', 'science', 'e']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9987\n",
            "    After Stopword Removal: ['folks', 'typically', 'examine', 'roof', 'skylights', 'leaks', '.']\n",
            "    After Regex: ['folks', 'typically', 'examine', 'roof', 'skylights', 'leaks']\n",
            "    After Lemmatization: ['folk', 'typically', 'examine', 'roof', 'skylight', 'leak']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9988\n",
            "    After Stopword Removal: ['boiling', 'alter', 'protein', 'peanuts', '—', 'roasting', '—', 'usually', 'protein', 'foods', 'acts', 'allergen', '.']\n",
            "    After Regex: ['boiling', 'alter', 'protein', 'peanuts', 'roasting', 'usually', 'protein', 'foods', 'acts', 'allergen']\n",
            "    After Lemmatization: ['boiling', 'alter', 'protein', 'peanut', 'roasting', 'usually', 'protein', 'food', 'act', 'allergen']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9989\n",
            "    After Stopword Removal: ['united', 'states', 'interested', 'eliminating', 'threat', 'posed', 'weapons', 'mass', 'destruction', ',', 'promoting', 'membership', 'nato', \"'\", 'partnership', 'peace', 'program', ',', 'helping', 'regional', 'peacekeeping', 'efforts', 'fostering', 'greater', 'regional', 'cooperation', ',', 'crouch', 'said', '.']\n",
            "    After Regex: ['united', 'states', 'interested', 'eliminating', 'threat', 'posed', 'weapons', 'mass', 'destruction', 'promoting', 'membership', 'nato', 'partnership', 'peace', 'program', 'helping', 'regional', 'peacekeeping', 'efforts', 'fostering', 'greater', 'regional', 'cooperation', 'crouch', 'said']\n",
            "    After Lemmatization: ['united', 'state', 'interested', 'eliminating', 'threat', 'posed', 'weapon', 'mass', 'destruction', 'promoting', 'membership', 'nato', 'partnership', 'peace', 'program', 'helping', 'regional', 'peacekeeping', 'effort', 'fostering', 'greater', 'regional', 'cooperation', 'crouch', 'said']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9990\n",
            "    After Stopword Removal: ['individuals', 'low', '-', 'income', 'age', '65,', 'blind', ',', 'disability', 'eligible', 'assistance', '.']\n",
            "    After Regex: ['individuals', 'low', 'income', 'age', 'blind', 'disability', 'eligible', 'assistance']\n",
            "    After Lemmatization: ['individual', 'low', 'income', 'age', 'blind', 'disability', 'eligible', 'assistance']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9991\n",
            "    After Stopword Removal: ['want', 'attract', 'minority', 'support', ',', 'republicans', 'take', 'account', 'voters', 'believe', 'range', 'issues', ',', 'proper', 'relationship', 'government', 'individual', 'proper', 'role', 'united', 'states', 'rapidly', 'changing', 'world', '.']\n",
            "    After Regex: ['want', 'attract', 'minority', 'support', 'republicans', 'take', 'account', 'voters', 'believe', 'range', 'issues', 'proper', 'relationship', 'government', 'individual', 'proper', 'role', 'united', 'states', 'rapidly', 'changing', 'world']\n",
            "    After Lemmatization: ['want', 'attract', 'minority', 'support', 'republican', 'take', 'account', 'voter', 'believe', 'range', 'issue', 'proper', 'relationship', 'government', 'individual', 'proper', 'role', 'united', 'state', 'rapidly', 'changing', 'world']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9992\n",
            "    After Stopword Removal: ['suggestions', 'biomechanical', ',', 'strength', 'hormonally', 'related', '.']\n",
            "    After Regex: ['suggestions', 'biomechanical', 'strength', 'hormonally', 'related']\n",
            "    After Lemmatization: ['suggestion', 'biomechanical', 'strength', 'hormonally', 'related']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9993\n",
            "    After Stopword Removal: ['using', 'high', 'quality', 'locks', 'major', 'deterrent', 'burglars', \"'\", 'want', 'waste', 'time', 'trying', 'get', 'door', 'open', '.']\n",
            "    After Regex: ['using', 'high', 'quality', 'locks', 'major', 'deterrent', 'burglars', 'want', 'waste', 'time', 'trying', 'get', 'door', 'open']\n",
            "    After Lemmatization: ['using', 'high', 'quality', 'lock', 'major', 'deterrent', 'burglar', 'want', 'waste', 'time', 'trying', 'get', 'door', 'open']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9994\n",
            "    After Stopword Removal: ['understand', 'molecular', 'determinants', 'characteristic', 'differences', ',', 'structure', '-', 'function', 'relationship', 'studied', 'using', 'site', '-', 'directed', 'mutagenesis', '.']\n",
            "    After Regex: ['understand', 'molecular', 'determinants', 'characteristic', 'differences', 'structure', 'function', 'relationship', 'studied', 'using', 'site', 'directed', 'mutagenesis']\n",
            "    After Lemmatization: ['understand', 'molecular', 'determinant', 'characteristic', 'difference', 'structure', 'function', 'relationship', 'studied', 'using', 'site', 'directed', 'mutagenesis']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9995\n",
            "    After Stopword Removal: ['new', 'device', 'may', 'reduce', 'repeat', 'breast', 'cancer', 'surgeries', 'tuesday', ',', 'sept', '.11(', 'healthday', 'news', ')--', 'new', 'device', 'meant', 'help', 'surgeons', 'determine', 'operating', 'room', 'removed', 'cancerous', 'breast', 'cancer', 'tissue', 'may', 'help', 'reduce', 'repeat', 'surgeries', 'lumpectomy', 'without', 'compromising', 'cosmetic', 'effects', ',', 'according', 'new', 'study', '.']\n",
            "    After Regex: ['new', 'device', 'may', 'reduce', 'repeat', 'breast', 'cancer', 'surgeries', 'tuesday', 'sept', 'healthday', 'news', 'new', 'device', 'meant', 'help', 'surgeons', 'determine', 'operating', 'room', 'removed', 'cancerous', 'breast', 'cancer', 'tissue', 'may', 'help', 'reduce', 'repeat', 'surgeries', 'lumpectomy', 'without', 'compromising', 'cosmetic', 'effects', 'according', 'new', 'study']\n",
            "    After Lemmatization: ['new', 'device', 'may', 'reduce', 'repeat', 'breast', 'cancer', 'surgery', 'tuesday', 'sept', 'healthday', 'news', 'new', 'device', 'meant', 'help', 'surgeon', 'determine', 'operating', 'room', 'removed', 'cancerous', 'breast', 'cancer', 'tissue', 'may', 'help', 'reduce', 'repeat', 'surgery', 'lumpectomy', 'without', 'compromising', 'cosmetic', 'effect', 'according', 'new', 'study']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9996\n",
            "    After Stopword Removal: ['-', 'represented', 'political', 'economic', 'decision', '-', 'making', 'processes', '.']\n",
            "    After Regex: ['represented', 'political', 'economic', 'decision', 'making', 'processes']\n",
            "    After Lemmatization: ['represented', 'political', 'economic', 'decision', 'making', 'process']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9997\n",
            "    After Stopword Removal: ['confluence', 'better', '-', '-', 'expected', 'news', 'economic', 'activity', 'inflation', 'suggested', 'somewhat', 'smaller', 'downside', 'risks', 'economic', 'growth', 'well', 'improved', 'prospects', 'core', 'inflation', '.']\n",
            "    After Regex: ['confluence', 'better', 'expected', 'news', 'economic', 'activity', 'inflation', 'suggested', 'somewhat', 'smaller', 'downside', 'risks', 'economic', 'growth', 'well', 'improved', 'prospects', 'core', 'inflation']\n",
            "    After Lemmatization: ['confluence', 'better', 'expected', 'news', 'economic', 'activity', 'inflation', 'suggested', 'somewhat', 'smaller', 'downside', 'risk', 'economic', 'growth', 'well', 'improved', 'prospect', 'core', 'inflation']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9998\n",
            "    After Stopword Removal: ['keyboard', 'commands', 'inserted', 'standard', 'pda', 'input', 'queue', ',', 'simulating', 'input', 'keystroke', '.']\n",
            "    After Regex: ['keyboard', 'commands', 'inserted', 'standard', 'pda', 'input', 'queue', 'simulating', 'input', 'keystroke']\n",
            "    After Lemmatization: ['keyboard', 'command', 'inserted', 'standard', 'pda', 'input', 'queue', 'simulating', 'input', 'keystroke']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 9999\n",
            "    After Stopword Removal: ['book', '2002', 'erminie', 'wheeler', '-', 'voegelin', 'award', ',', 'sponsored', 'american', 'society', 'ethnohistory', '.']\n",
            "    After Regex: ['book', 'erminie', 'wheeler', 'voegelin', 'award', 'sponsored', 'american', 'society', 'ethnohistory']\n",
            "    After Lemmatization: ['book', 'erminie', 'wheeler', 'voegelin', 'award', 'sponsored', 'american', 'society', 'ethnohistory']\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Sentence 10000\n",
            "    After Stopword Removal: ['parents', 'place', 'child', 'adoption', 'plan', 'evident', 'loss', 'lives', '.']\n",
            "    After Regex: ['parents', 'place', 'child', 'adoption', 'plan', 'evident', 'loss', 'lives']\n",
            "    After Lemmatization: ['parent', 'place', 'child', 'adoption', 'plan', 'evident', 'loss', 'life']\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBOW(BAG OF WORDS)**"
      ],
      "metadata": {
        "id": "SlpyEwhv-Rn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Prepare the processed text as input for Word2Vec\n",
        "tokenized_sentences = [process[\"After Lemmatization\"] for process in processed_sentences]\n",
        "# print(tokenized_sentences[:5])\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_sentences,  # Input sentences\n",
        "    vector_size=100,  # Each word will be represented as a n-dimensional vector\n",
        "    window=3,  # Context window size\n",
        "    min_count=2,  # Ignore words with frequency less than 2\n",
        "    sg=0,  # Use CBOW (better for rare words)\n",
        "    workers=4,  # Use 4 CPU cores\n",
        "    epochs=20,  # Train for 10 epochs\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Example: Get vector representation of a word\n",
        "word = \"social\"\n",
        "if word in word2vec_model.wv:\n",
        "    print(f\"Vector representation of '{word}':\\n{word2vec_model.wv[word]}\")\n",
        "    print(len(word2vec_model.wv[word]))\n",
        "else:\n",
        "    print(f\"'{word}' not found in vocabulary\")\n",
        "\n",
        "# Example: Find similar words\n",
        "if word in word2vec_model.wv:\n",
        "    similar_words = word2vec_model.wv.most_similar(word)\n",
        "    print(f\"\\nTop 5 words similar to '{word}':\")\n",
        "    for sim_word, similarity in similar_words:\n",
        "        print(f\"  {sim_word}: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "catw3h1Y9p9_",
        "outputId": "f9dc0242-bdf7-4d7d-f113-2ffad6d80d87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'social':\n",
            "[ 1.01278436e+00 -6.42162979e-01 -1.23449743e+00 -5.20182788e-01\n",
            "  6.55271411e-01 -9.83836353e-01  7.25462556e-01  3.68366167e-02\n",
            "  3.01225662e-01 -1.09758735e+00  4.02019292e-01 -5.52700162e-01\n",
            " -8.40459645e-01 -4.55762371e-02  4.17709976e-01  8.15138042e-01\n",
            "  5.60612500e-01 -9.44923341e-01  1.42891929e-01 -7.71663368e-01\n",
            "  9.71354783e-01 -1.18228650e+00  3.40897143e-01 -6.41768500e-02\n",
            " -2.24597979e+00  8.72681856e-01  1.38370112e-01 -7.15166271e-01\n",
            "  3.89574856e-01 -1.67819035e+00  4.62954164e-01  4.43229109e-01\n",
            " -9.28812996e-02  9.86635268e-01  1.12305152e+00  3.24206173e-01\n",
            " -2.03651071e-01 -8.13216925e-01  1.24686770e-01 -2.12188697e+00\n",
            "  4.01429743e-01  3.51380855e-01  1.01603866e+00 -4.47205484e-01\n",
            "  6.31710231e-01 -5.02178192e-01 -1.03598404e+00 -1.24448931e+00\n",
            "  1.50407463e-01 -5.58010638e-01 -8.60867739e-01 -4.84465845e-02\n",
            "  1.32222652e+00 -7.59393036e-01  6.11274600e-01  1.21168137e+00\n",
            "  8.22969437e-01 -2.31122136e-01  1.33600786e-01 -9.57772255e-01\n",
            " -1.82946131e-01 -1.89993337e-01  3.90122414e-01  8.53213012e-01\n",
            " -1.54001594e+00 -1.05092049e+00  9.36474323e-01 -1.59674966e+00\n",
            " -1.55322754e+00  1.27795184e+00 -9.63205099e-02  5.13376892e-02\n",
            " -2.85190910e-01  8.04547250e-01  3.91029984e-01  1.62452698e+00\n",
            "  8.79956245e-01 -9.53531265e-01 -3.84166569e-01 -1.17927775e-04\n",
            "  5.45914650e-01  5.04632235e-01  1.43019119e-02  1.57466364e+00\n",
            " -1.12611318e+00  8.00942719e-01  6.04917347e-01  4.04505223e-01\n",
            "  9.99567583e-02  7.18036652e-01 -2.80242443e-01  1.33800328e+00\n",
            " -6.13544047e-01  4.80330080e-01 -1.97382182e-01  1.62109292e+00\n",
            "  1.23710059e-01 -2.80871451e-01  5.86603284e-01 -3.10060382e-01]\n",
            "100\n",
            "\n",
            "Top 5 words similar to 'social':\n",
            "  impact: 0.5997\n",
            "  organization: 0.5935\n",
            "  environmental: 0.5881\n",
            "  cultural: 0.5844\n",
            "  infrastructure: 0.5802\n",
            "  influence: 0.5743\n",
            "  collaborative: 0.5695\n",
            "  responsibility: 0.5636\n",
            "  sustainable: 0.5589\n",
            "  principle: 0.5555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SKIP-GRAM**"
      ],
      "metadata": {
        "id": "g8jOex6u-cmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Prepare the processed text as input for Word2Vec\n",
        "tokenized_sentences = [process[\"After Lemmatization\"] for process in processed_sentences]\n",
        "# print(tokenized_sentences[:5])\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_sentences,  # Input sentences\n",
        "    vector_size=100,  # Each word will be represented as a n-dimensional vector\n",
        "    window=3,  # Context window size\n",
        "    min_count=2,  # Ignore words with frequency less than 2\n",
        "    sg=1,  # Use Skip-gram (better for rare words)\n",
        "    workers=4,  # Use 4 CPU cores\n",
        "    epochs=20,  # Train for 10 epochs\n",
        "    hs=1\n",
        "\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Example: Get vector representation of a word\n",
        "word = \"social\"\n",
        "if word in word2vec_model.wv:\n",
        "    print(f\"Vector representation of '{word}':\\n{word2vec_model.wv[word]}\")\n",
        "    print(len(word2vec_model.wv[word]))\n",
        "else:\n",
        "    print(f\"'{word}' not found in vocabulary\")\n",
        "\n",
        "# Example: Find similar words\n",
        "if word in word2vec_model.wv:\n",
        "    similar_words = word2vec_model.wv.most_similar(word)\n",
        "    print(f\"\\nTop 5 words similar to '{word}':\")\n",
        "    for sim_word, similarity in similar_words:\n",
        "        print(f\"  {sim_word}: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmoHHElG9qA_",
        "outputId": "842626cb-7a12-408f-e46b-9508a4a3a1f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'social':\n",
            "[ 0.64958525  0.5112998  -0.4573714  -0.28526485 -0.10216914 -0.67095363\n",
            " -0.14552854  0.04600862 -0.47797123 -0.15101807  0.07563026 -0.6836453\n",
            " -0.67107224 -0.52859724 -0.36733413 -0.33429387  0.04230069 -0.40996194\n",
            " -0.12791686 -0.5226963  -0.05869799 -0.27390465  0.1392044  -0.25652578\n",
            " -0.53674364  0.24603082  0.52525115  0.05088064  0.19028185 -0.46839535\n",
            "  0.15131186  0.31586507 -0.30870238  1.0558351  -0.17683643 -0.00480261\n",
            "  0.7669628  -0.45728698  0.54096055 -1.1030493  -0.57941395 -0.02206804\n",
            "  0.42199555 -0.41206542  0.6897164  -0.06488128 -0.52425784 -0.06905688\n",
            "  0.26800227 -0.16805112 -0.54853874  0.06217616  0.63021487  0.32168454\n",
            " -0.30168545  0.5366433   0.32375062 -0.07221498  0.04280208  0.27604666\n",
            " -0.39921156 -0.04079762 -0.02129481 -0.2465361   0.04827606 -0.11124411\n",
            "  0.63659024 -0.4180326  -0.410827    0.45609152 -0.05449094 -0.16474184\n",
            "  0.08199838  0.19256836  0.39482927  0.7310616   0.10757067 -0.46182972\n",
            "  0.01271802  0.06640949  0.2174418   0.37811095 -0.28301823  0.15150106\n",
            " -0.39358795 -0.06068123  0.04318912  0.11964427 -0.283511    0.14629\n",
            "  0.03862851  0.0108675  -0.64379466 -0.04566636 -0.09815548  0.55392236\n",
            " -0.44191924 -0.00835192  0.11636946  0.11288774]\n",
            "100\n",
            "\n",
            "Top 5 words similar to 'social':\n",
            "  ssa: 0.5870\n",
            "  mobilize: 0.5777\n",
            "  socio: 0.5640\n",
            "  norm: 0.5616\n",
            "  cyber: 0.5590\n",
            "  economical: 0.5578\n",
            "  economics: 0.5571\n",
            "  surround: 0.5570\n",
            "  analyzes: 0.5496\n",
            "  challenge: 0.5489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HetlN_d9qDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-bLfb2E9qGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHKUBC-e9qJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxFrs40Y9qMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPTlxAUS8Fx9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}